{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Model for HAI Security Dataset Anomaly Detection\n",
    "\n",
    "This notebook implements a deep autoencoder for anomaly detection on the HAI security dataset. Autoencoders are effective for anomaly detection as they learn to compress and reconstruct normal data, with anomalies typically having higher reconstruction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Check for GPU availability\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(\"GPU is available for training\")\n",
    "    # Set memory growth to avoid memory allocation errors\n",
    "    for gpu in tf.config.list_physical_devices('GPU'):\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"No GPU available, using CPU for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data\n",
    "\n",
    "First, let's load the preprocessed data created in the preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_processed_data(file_path):\n",
    "    \"\"\"\n",
    "    Load processed data from NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the NPZ file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Loaded data\n",
    "    \"\"\"\n",
    "    # Load NPZ file\n",
    "    npz_data = np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(npz_data['data'], columns=npz_data['columns'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load preprocessor\n",
    "preprocessor_path = './models/hai_hai_20_07_standard_preprocessor.joblib'\n",
    "preprocessor_dict = joblib.load(preprocessor_path)\n",
    "\n",
    "# Extract important information\n",
    "feature_columns = preprocessor_dict['feature_columns']\n",
    "attack_columns = preprocessor_dict['attack_columns']\n",
    "timestamp_col = preprocessor_dict['timestamp_col']\n",
    "\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "print(f\"Attack columns: {attack_columns}\")\n",
    "print(f\"Timestamp column: {timestamp_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get list of processed data files\n",
    "train_data_dir = './processed_data/hai-20.07/train'\n",
    "test_data_dir = './processed_data/hai-20.07/test'\n",
    "\n",
    "train_files = sorted(glob.glob(f'{train_data_dir}/*.npz'))\n",
    "test_files = sorted(glob.glob(f'{test_data_dir}/*.npz'))\n",
    "\n",
    "print(f\"Training files: {[os.path.basename(f) for f in train_files]}\")\n",
    "print(f\"Test files: {[os.path.basename(f) for f in test_files]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Autoencoder\n",
    "\n",
    "Like Random Forest, the autoencoder doesn't require sequence data. We'll use individual time points as samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_and_prepare_tabular_data(file_paths, feature_cols, target_col=None, max_files=None, sample_fraction=None):\n",
    "    \"\"\"\n",
    "    Load and prepare tabular data from multiple files.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of file paths\n",
    "        feature_cols: List of feature column names\n",
    "        target_col: Target column name (None for unsupervised learning)\n",
    "        max_files: Maximum number of files to load (None for all files)\n",
    "        sample_fraction: Fraction of data to sample (None for all data)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y) - Features and targets\n",
    "    \"\"\"\n",
    "    all_X = []\n",
    "    all_y = [] if target_col is not None else None\n",
    "    \n",
    "    # Limit the number of files if specified\n",
    "    if max_files is not None:\n",
    "        file_paths = file_paths[:max_files]\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        \n",
    "        # Load data\n",
    "        df = load_processed_data(file_path)\n",
    "        \n",
    "        # Sample data if specified\n",
    "        if sample_fraction is not None and sample_fraction < 1.0:\n",
    "            df = df.sample(frac=sample_fraction, random_state=42)\n",
    "        \n",
    "        # Extract features\n",
    "        X = df[feature_cols]\n",
    "        all_X.append(X)\n",
    "        \n",
    "        # Extract target if provided\n",
    "        if target_col is not None and target_col in df.columns:\n",
    "            y = df[target_col]\n",
    "            all_y.append(y)\n",
    "    \n",
    "    # Combine data from all files\n",
    "    combined_X = pd.concat(all_X) if all_X else pd.DataFrame()\n",
    "    combined_y = pd.concat(all_y) if all_y else None\n",
    "    \n",
    "    return combined_X, combined_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set parameters\n",
    "target_col = 'attack' if attack_columns else None  # Target column\n",
    "sample_fraction = 0.1  # Sample 10% of data to reduce memory usage\n",
    "\n",
    "# Load and prepare training data\n",
    "print(\"Loading and preparing training data...\")\n",
    "X_train, _ = load_and_prepare_tabular_data(train_files, feature_columns, target_col=None, \n",
    "                                          max_files=2, sample_fraction=sample_fraction)\n",
    "\n",
    "# Load and prepare test data\n",
    "print(\"\\nLoading and preparing test data...\")\n",
    "X_test, y_test = load_and_prepare_tabular_data(test_files, feature_columns, target_col=target_col, \n",
    "                                              max_files=2, sample_fraction=sample_fraction)\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
    "if y_test is not None:\n",
    "    print(f\"Test data shape: {X_test.shape}, Test labels shape: {y_test.shape}\")\n",
    "else:\n",
    "    print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Autoencoder Model\n",
    "\n",
    "Now we'll build a deep autoencoder model for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def build_autoencoder(input_dim, encoding_dim=16, hidden_layers=[128, 64, 32]):\n",
    "    \"\"\"\n",
    "    Build a deep autoencoder model.\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Dimension of input data\n",
    "        encoding_dim: Dimension of the encoded representation\n",
    "        hidden_layers: List of hidden layer dimensions\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, encoder, decoder) - Full model, encoder part, and decoder part\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    encoder_input = Input(shape=(input_dim,), name='encoder_input')\n",
    "    \n",
    "    # Build encoder layers\n",
    "    x = encoder_input\n",
    "    for i, units in enumerate(hidden_layers):\n",
    "        x = Dense(units, activation='relu', name=f'encoder_dense_{i+1}')(x)\n",
    "        x = BatchNormalization(name=f'encoder_bn_{i+1}')(x)\n",
    "        x = Dropout(0.2, name=f'encoder_dropout_{i+1}')(x)\n",
    "    \n",
    "    # Bottleneck layer\n",
    "    encoder_output = Dense(encoding_dim, activation='relu', name='encoder_output')(x)\n",
    "    \n",
    "    # Create encoder model\n",
    "    encoder = Model(encoder_input, encoder_output, name='encoder')\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_input = Input(shape=(encoding_dim,), name='decoder_input')\n",
    "    \n",
    "    # Build decoder layers (reverse of encoder)\n",
    "    x = decoder_input\n",
    "    for i, units in enumerate(reversed(hidden_layers)):\n",
    "        x = Dense(units, activation='relu', name=f'decoder_dense_{i+1}')(x)\n",
    "        x = BatchNormalization(name=f'decoder_bn_{i+1}')(x)\n",
    "        x = Dropout(0.2, name=f'decoder_dropout_{i+1}')(x)\n",
    "    \n",
    "    # Output layer\n",
    "    decoder_output = Dense(input_dim, activation='linear', name='decoder_output')(x)\n",
    "    \n",
    "    # Create decoder model\n",
    "    decoder = Model(decoder_input, decoder_output, name='decoder')\n",
    "    \n",
    "    # Create autoencoder model\n",
    "    autoencoder_input = Input(shape=(input_dim,), name='autoencoder_input')\n",
    "    encoded = encoder(autoencoder_input)\n",
    "    decoded = decoder(encoded)\n",
    "    autoencoder = Model(autoencoder_input, decoded, name='autoencoder')\n",
    "    \n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert data to numpy arrays and ensure float32 type\n",
    "X_train_array = X_train.values.astype('float32')\n",
    "X_test_array = X_test.values.astype('float32')\n",
    "\n",
    "# Set model parameters\n",
    "input_dim = X_train.shape[1]  # Number of features\n",
    "encoding_dim = 16  # Dimension of the encoded representation\n",
    "hidden_layers = [128, 64, 32]  # Hidden layer dimensions\n",
    "\n",
    "# Build model\n",
    "autoencoder, encoder, decoder = build_autoencoder(input_dim, encoding_dim, hidden_layers)\n",
    "\n",
    "# Compile model\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Print model summary\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Autoencoder Model\n",
    "\n",
    "Now we'll train the autoencoder on normal data to learn the normal behavior patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set training parameters\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "validation_split = 0.1\n",
    "\n",
    "# Create model checkpoint callback\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "checkpoint_path = './models/autoencoder_hai_20_07.h5'\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "# Create early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                              patience=10, \n",
    "                              verbose=1, \n",
    "                              mode='min', \n",
    "                              restore_best_weights=True)\n",
    "\n",
    "# Create TensorBoard callback\n",
    "log_dir = './logs/autoencoder_' + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard = TensorBoard(log_dir=log_dir, \n",
    "                         histogram_freq=1, \n",
    "                         write_graph=True, \n",
    "                         write_images=True)\n",
    "\n",
    "# Combine callbacks\n",
    "callbacks = [checkpoint, early_stopping, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train model\n",
    "start_time = time.time()\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    X_train_array, X_train_array,  # Input and output are the same for autoencoder\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Plot zoomed-in view of the last epochs\n",
    "plt.subplot(1, 2, 2)\n",
    "last_epochs = min(20, len(history.history['loss']))  # Last 20 epochs or all if less\n",
    "plt.plot(history.history['loss'][-last_epochs:])\n",
    "plt.plot(history.history['val_loss'][-last_epochs:])\n",
    "plt.title(f'Model Loss (Last {last_epochs} Epochs)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model and Detect Anomalies\n",
    "\n",
    "Now we'll use the trained autoencoder to detect anomalies in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the best model\n",
    "best_model = load_model(checkpoint_path)\n",
    "\n",
    "# Predict on test data\n",
    "X_test_pred = best_model.predict(X_test_array)\n",
    "\n",
    "# Calculate reconstruction error (MSE for each sample)\n",
    "mse = np.mean(np.square(X_test_array - X_test_pred), axis=1)\n",
    "\n",
    "print(f\"Reconstruction error statistics:\")\n",
    "print(f\"Min: {np.min(mse):.6f}\")\n",
    "print(f\"Max: {np.max(mse):.6f}\")\n",
    "print(f\"Mean: {np.mean(mse):.6f}\")\n",
    "print(f\"Std: {np.std(mse):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot reconstruction error distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(mse, bins=50)\n",
    "plt.title('Reconstruction Error Distribution')\n",
    "plt.xlabel('Reconstruction Error (MSE)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(mse, bins=50, log=True)\n",
    "plt.title('Reconstruction Error Distribution (Log Scale)')\n",
    "plt.xlabel('Reconstruction Error (MSE)')\n",
    "plt.ylabel('Frequency (Log Scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Determine Anomaly Threshold\n",
    "\n",
    "We need to determine a threshold for the reconstruction error to classify data points as normal or anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def find_optimal_threshold(mse, y_true):\n",
    "    \"\"\"\n",
    "    Find the optimal threshold for anomaly detection using ROC curve.\n",
    "    \n",
    "    Args:\n",
    "        mse: Reconstruction error values\n",
    "        y_true: True labels (0 for normal, 1 for anomaly)\n",
    "        \n",
    "    Returns:\n",
    "        float: Optimal threshold value\n",
    "    \"\"\"\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, mse)\n",
    "    \n",
    "    # Calculate the geometric mean of sensitivity and specificity\n",
    "    gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "    \n",
    "    # Find the optimal threshold\n",
    "    ix = np.argmax(gmeans)\n",
    "    optimal_threshold = thresholds[ix]\n",
    "    \n",
    "    print(f\"Optimal threshold: {optimal_threshold:.6f}\")\n",
    "    print(f\"At this threshold - TPR: {tpr[ix]:.4f}, FPR: {fpr[ix]:.4f}, G-mean: {gmeans[ix]:.4f}\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.scatter(fpr[ix], tpr[ix], marker='o', color='red', label=f'Optimal (Threshold = {optimal_threshold:.6f})')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate AUC\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Find optimal threshold if labels are available\n",
    "if y_test is not None:\n",
    "    threshold = find_optimal_threshold(mse, y_test)\n",
    "else:\n",
    "    # If no labels, use a statistical approach\n",
    "    threshold = np.mean(mse) + 3 * np.std(mse)  # Mean + 3 standard deviations\n",
    "    print(f\"Using statistical threshold: {threshold:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Classify as anomaly if reconstruction error > threshold\n",
    "y_pred = (mse > threshold).astype(int)\n",
    "\n",
    "# Evaluate if labels are available\n",
    "if y_test is not None:\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualize Reconstructions\n",
    "\n",
    "Let's visualize some examples of normal and anomalous data points with their reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_feature_reconstructions(X, X_pred, feature_names, indices, mse, title):\n",
    "    \"\"\"\n",
    "    Plot original vs. reconstructed values for selected features.\n",
    "    \n",
    "    Args:\n",
    "        X: Original data\n",
    "        X_pred: Reconstructed data\n",
    "        feature_names: List of feature names\n",
    "        indices: Indices of samples to plot\n",
    "        mse: Reconstruction errors\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    # Select top features with highest reconstruction error\n",
    "    feature_mse = np.mean(np.square(X[indices] - X_pred[indices]), axis=0)\n",
    "    top_features_idx = np.argsort(feature_mse)[-5:]  # Top 5 features\n",
    "    \n",
    "    # Plot each sample\n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.suptitle(f\"{title} - Sample {i+1} (MSE: {mse[idx]:.6f})\")\n",
    "        \n",
    "        for j, feature_idx in enumerate(top_features_idx):\n",
    "            plt.subplot(1, 5, j+1)\n",
    "            \n",
    "            # Plot original and reconstructed values\n",
    "            plt.bar([0, 1], [X[idx, feature_idx], X_pred[idx, feature_idx]], \n",
    "                   color=['blue', 'orange'])\n",
    "            \n",
    "            plt.title(f\"{feature_names[feature_idx]}\")\n",
    "            plt.xticks([0, 1], ['Original', 'Reconstructed'])\n",
    "            plt.ylabel('Value')\n",
    "            \n",
    "            # Add reconstruction error\n",
    "            error = np.abs(X[idx, feature_idx] - X_pred[idx, feature_idx])\n",
    "            plt.annotate(f\"Error: {error:.4f}\", xy=(0.5, 0.9), xycoords='axes fraction', \n",
    "                        ha='center', va='center', bbox=dict(boxstyle='round', fc='yellow', alpha=0.3))\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get indices of normal and anomalous samples\n",
    "normal_indices = np.where(y_pred == 0)[0]\n",
    "anomaly_indices = np.where(y_pred == 1)[0]\n",
    "\n",
    "# Select random samples\n",
    "num_samples = 3\n",
    "selected_normal = np.random.choice(normal_indices, size=min(num_samples, len(normal_indices)), replace=False)\n",
    "selected_anomaly = np.random.choice(anomaly_indices, size=min(num_samples, len(anomaly_indices)), replace=False)\n",
    "\n",
    "# Plot normal samples\n",
    "if len(selected_normal) > 0:\n",
    "    plot_feature_reconstructions(X_test_array, X_test_pred, X_test.columns, selected_normal, mse, \"Normal Sample\")\n",
    "\n",
    "# Plot anomalous samples\n",
    "if len(selected_anomaly) > 0:\n",
    "    plot_feature_reconstructions(X_test_array, X_test_pred, X_test.columns, selected_anomaly, mse, \"Anomalous Sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Analyze Feature Reconstruction Errors\n",
    "\n",
    "Let's analyze which features have the highest reconstruction errors in anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get indices of anomalies\n",
    "anomaly_indices = np.where(y_pred == 1)[0]\n",
    "\n",
    "if len(anomaly_indices) > 0:\n",
    "    # Calculate feature-wise reconstruction error for anomalies\n",
    "    anomaly_feature_mse = np.mean(np.square(X_test_array[anomaly_indices] - X_test_pred[anomaly_indices]), axis=0)\n",
    "    \n",
    "    # Create a DataFrame for easier visualization\n",
    "    feature_error_df = pd.DataFrame({\n",
    "        'Feature': X_test.columns,\n",
    "        'Reconstruction_Error': anomaly_feature_mse\n",
    "    }).sort_values('Reconstruction_Error', ascending=False)\n",
    "    \n",
    "    # Plot top 15 features with highest reconstruction error\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Reconstruction_Error', y='Feature', data=feature_error_df.head(15))\n",
    "    plt.title('Top 15 Features with Highest Reconstruction Error in Anomalies')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 10 features with highest reconstruction error in anomalies:\")\n",
    "    for i, (feature, error) in enumerate(zip(feature_error_df['Feature'].head(10), \n",
    "                                           feature_error_df['Reconstruction_Error'].head(10))):\n",
    "        print(f\"{i+1}. {feature}: {error:.6f}\")\n",
    "else:\n",
    "    print(\"No anomalies detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Encoded Representations\n",
    "\n",
    "Let's visualize the encoded representations to see if anomalies form distinct clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get encoder model\n",
    "encoder_model = Model(inputs=best_model.input, outputs=best_model.layers[len(best_model.layers)//2-1].output)\n",
    "\n",
    "# Get encoded representations\n",
    "encoded_data = encoder_model.predict(X_test_array)\n",
    "\n",
    "print(f\"Encoded data shape: {encoded_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize encoded representations using PCA if dimension > 2\n",
    "if encoded_data.shape[1] > 2:\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    # Apply PCA to reduce to 2 dimensions\n",
    "    pca = PCA(n_components=2)\n",
    "    encoded_2d = pca.fit_transform(encoded_data)\n",
    "    \n",
    "    print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "else:\n",
    "    encoded_2d = encoded_data\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "plot_df = pd.DataFrame({\n",
    "    'x': encoded_2d[:, 0],\n",
    "    'y': encoded_2d[:, 1],\n",
    "    'anomaly': y_pred,\n",
    "    'mse': mse\n",
    "})\n",
    "\n",
    "# Plot encoded representations\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot normal and anomalous points\n",
    "plt.scatter(plot_df[plot_df['anomaly'] == 0]['x'], plot_df[plot_df['anomaly'] == 0]['y'], \n",
    "           c='blue', label='Normal', alpha=0.5)\n",
    "plt.scatter(plot_df[plot_df['anomaly'] == 1]['x'], plot_df[plot_df['anomaly'] == 1]['y'], \n",
    "           c='red', label='Anomaly', alpha=0.7)\n",
    "\n",
    "plt.title('Encoded Representations (2D Projection)')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot encoded representations with reconstruction error as color\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "scatter = plt.scatter(plot_df['x'], plot_df['y'], c=plot_df['mse'], \n",
    "                     cmap='viridis', alpha=0.7, s=30)\n",
    "\n",
    "plt.colorbar(scatter, label='Reconstruction Error (MSE)')\n",
    "plt.title('Encoded Representations with Reconstruction Error')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model and Results\n",
    "\n",
    "Finally, let's save the model and results for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save encoder model\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "encoder_model.save('./models/autoencoder_encoder_hai_20_07.h5')\n",
    "\n",
    "# Save threshold and other metadata\n",
    "model_metadata = {\n",
    "    'threshold': threshold,\n",
    "    'encoding_dim': encoding_dim,\n",
    "    'hidden_layers': hidden_layers,\n",
    "    'feature_columns': feature_columns,\n",
    "    'training_history': {\n",
    "        'loss': history.history['loss'],\n",
    "        'val_loss': history.history['val_loss']\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump(model_metadata, './models/autoencoder_metadata_hai_20_07.joblib')\n",
    "print(\"Model and metadata saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we've implemented a deep autoencoder for anomaly detection on the HAI security dataset. The autoencoder learns to compress and reconstruct normal behavior patterns and identifies anomalies based on reconstruction error. Key steps included:\n",
    "\n",
    "1. Loading and preparing preprocessed data\n",
    "2. Building a deep autoencoder model\n",
    "3. Training the model on normal data\n",
    "4. Detecting anomalies using reconstruction error\n",
    "5. Evaluating the model's performance\n",
    "6. Analyzing feature reconstruction errors\n",
    "7. Visualizing encoded representations\n",
    "\n",
    "The autoencoder provides an effective approach for detecting anomalies in industrial control system data, with the added benefit of dimensionality reduction and feature learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}