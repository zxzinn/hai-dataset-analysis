{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI Dataset Analysis and ResBiLSTM Model Template\n",
    "\n",
    "This notebook provides a template for analyzing any HAI dataset and implementing a Residual Bidirectional LSTM model for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import custom utility module\n",
    "from hai_utils import *\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Set the dataset parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dataset configuration\n",
    "DATASET_NAME = 'hai-20.07'  # Change to the dataset you want to analyze\n",
    "PARQUET_DIR = 'parquet_data'\n",
    "TRAIN_FILE = 'train2'  # File containing training data with attack labels\n",
    "TEST_FILE = 'test2'    # File containing test data with attack labels\n",
    "TIME_STEPS = 100       # Number of time steps for sequence creation\n",
    "STEP_SIZE = 1          # Step size for sequence creation\n",
    "N_FEATURES = 30        # Number of features to select\n",
    "FEATURE_SELECTION = True  # Whether to perform feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading Data\n",
    "\n",
    "Load the dataset from Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "data_dict = load_dataset(PARQUET_DIR, DATASET_NAME)\n",
    "\n",
    "# Display dataset information\n",
    "for name, df in data_dict.items():\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"Columns: {df.columns.tolist()[:5]}...\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get train and test dataframes\n",
    "train_df = data_dict[TRAIN_FILE]\n",
    "test_df = data_dict[TEST_FILE]\n",
    "\n",
    "# Check if attack column exists\n",
    "if 'attack' not in train_df.columns:\n",
    "    print(\"Warning: 'attack' column not found in training data!\")\n",
    "    print(f\"Available columns: {train_df.columns.tolist()}\")\n",
    "else:\n",
    "    print(f\"Attack column found in training data with {train_df['attack'].sum()} attack samples\")\n",
    "    \n",
    "if 'attack' not in test_df.columns:\n",
    "    print(\"Warning: 'attack' column not found in test data!\")\n",
    "    print(f\"Available columns: {test_df.columns.tolist()}\")\n",
    "else:\n",
    "    print(f\"Attack column found in test data with {test_df['attack'].sum()} attack samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration\n",
    "\n",
    "Explore the basic characteristics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check attack distribution\n",
    "if 'attack' in train_df.columns:\n",
    "    plot_attack_distribution(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Detect time column\n",
    "time_col = None\n",
    "for col in train_df.columns:\n",
    "    if col.lower() in ['time', 'timestamp']:\n",
    "        time_col = col\n",
    "        break\n",
    "\n",
    "print(f\"Time column: {time_col}\")\n",
    "if time_col:\n",
    "    print(f\"Time range: {train_df[time_col].min()} to {train_df[time_col].max()}\")\n",
    "    print(f\"Total duration: {train_df[time_col].max() - train_df[time_col].min()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check basic statistics\n",
    "train_df.describe().T.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for missing values\n",
    "missing_values = train_df.isnull().sum()\n",
    "if missing_values.sum() > 0:\n",
    "    print(\"Columns with missing values:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"No missing values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize time series for important features\n",
    "# Select first 10 non-time, non-target features\n",
    "feature_cols = [col for col in train_df.columns if col != time_col and col != 'attack'][:10]\n",
    "plot_time_series(train_df, feature_cols, time_col=time_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize feature correlation matrix\n",
    "plot_correlation_matrix(train_df, n_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare normal vs attack samples\n",
    "if 'attack' in train_df.columns:\n",
    "    # Select a few features to compare\n",
    "    features_to_compare = feature_cols[:5]\n",
    "    \n",
    "    fig, axes = plt.subplots(len(features_to_compare), 1, figsize=(15, 4*len(features_to_compare)))\n",
    "    \n",
    "    for i, feature in enumerate(features_to_compare):\n",
    "        normal_data = train_df[train_df['attack'] == 0][feature]\n",
    "        attack_data = train_df[train_df['attack'] == 1][feature]\n",
    "        \n",
    "        sns.kdeplot(normal_data, label='Normal', ax=axes[i])\n",
    "        sns.kdeplot(attack_data, label='Attack', ax=axes[i])\n",
    "        \n",
    "        axes[i].set_title(f'Distribution of {feature}')\n",
    "        axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "Prepare the data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preprocess data\n",
    "X_train, X_test, y_train, y_test, feature_names, scaler = preprocess_data(\n",
    "    train_df, test_df, \n",
    "    target_col='attack', \n",
    "    time_col=time_col,\n",
    "    feature_selection=FEATURE_SELECTION, \n",
    "    n_features=N_FEATURES,\n",
    "    scaler_type='standard'\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"Selected features: {feature_names[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create sequences for time series modeling\n",
    "X_train_seq, y_train_seq = create_sequences(X_train, y_train, time_steps=TIME_STEPS, step=STEP_SIZE)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test, y_test, time_steps=TIME_STEPS, step=STEP_SIZE)\n",
    "\n",
    "print(f\"X_train_seq shape: {X_train_seq.shape}\")\n",
    "print(f\"X_test_seq shape: {X_test_seq.shape}\")\n",
    "print(f\"y_train_seq shape: {y_train_seq.shape}\")\n",
    "print(f\"y_test_seq shape: {y_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Split training data into training and validation sets\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train_seq, y_train_seq, test_size=0.2, random_state=42, stratify=y_train_seq\n",
    ")\n",
    "\n",
    "print(f\"X_train_final shape: {X_train_final.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"Attack ratio in training set: {np.mean(y_train_final):.4f}\")\n",
    "print(f\"Attack ratio in validation set: {np.mean(y_val):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ResBiLSTM Model\n",
    "\n",
    "Create and train the Residual Bidirectional LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model configuration\n",
    "LSTM_UNITS = 64\n",
    "DENSE_UNITS = 32\n",
    "DROPOUT_RATE = 0.3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "PATIENCE = 10\n",
    "MODEL_PATH = f'best_resbilstm_{DATASET_NAME.replace(\"-\", \"_\")}.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create model\n",
    "input_shape = (TIME_STEPS, X_train_final.shape[2])\n",
    "model = create_residual_bilstm_model(\n",
    "    input_shape=input_shape,\n",
    "    lstm_units=LSTM_UNITS,\n",
    "    dense_units=DENSE_UNITS,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train model\n",
    "history, model = train_model(\n",
    "    model=model,\n",
    "    X_train=X_train_final,\n",
    "    y_train=y_train_final,\n",
    "    X_val=X_val,\n",
    "    y_val=y_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    model_path=MODEL_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Evaluate the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate model on test set\n",
    "results = evaluate_model(model, X_test_seq, y_test_seq)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {results['accuracy']:.4f}\")\n",
    "print(f\"Precision: {results['precision']:.4f}\")\n",
    "print(f\"Recall: {results['recall']:.4f}\")\n",
    "print(f\"F1 Score: {results['f1']:.4f}\")\n",
    "print(f\"AUC: {results['auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot evaluation results\n",
    "plot_evaluation_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot feature importance\n",
    "plot_feature_importance(model, feature_names, n_top=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Predictions\n",
    "\n",
    "Visualize the model's predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get predictions\n",
    "y_pred = results['y_pred']\n",
    "y_pred_proba = results['y_pred_proba']\n",
    "\n",
    "# Create a DataFrame with actual and predicted values\n",
    "pred_df = pd.DataFrame({\n",
    "    'Actual': y_test_seq,\n",
    "    'Predicted': y_pred.flatten(),\n",
    "    'Probability': y_pred_proba.flatten()\n",
    "})\n",
    "\n",
    "# Plot actual vs predicted values\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(pred_df.index, pred_df['Actual'], label='Actual', marker='o', markersize=3, linestyle='-', alpha=0.7)\n",
    "plt.plot(pred_df.index, pred_df['Probability'], label='Predicted Probability', marker=None, linestyle='-', alpha=0.7)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.3, label='Threshold (0.5)')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot prediction errors\n",
    "errors = pred_df[pred_df['Actual'] != pred_df['Predicted']]\n",
    "print(f\"Number of errors: {len(errors)} out of {len(pred_df)} samples ({len(errors)/len(pred_df)*100:.2f}%)\")\n",
    "\n",
    "# Plot false positives and false negatives\n",
    "false_positives = pred_df[(pred_df['Actual'] == 0) & (pred_df['Predicted'] == 1)]\n",
    "false_negatives = pred_df[(pred_df['Actual'] == 1) & (pred_df['Predicted'] == 0)]\n",
    "\n",
    "print(f\"False positives: {len(false_positives)} ({len(false_positives)/len(pred_df)*100:.2f}%)\")\n",
    "print(f\"False negatives: {len(false_negatives)} ({len(false_negatives)/len(pred_df)*100:.2f}%)\")\n",
    "\n",
    "# Plot probability distribution for errors\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(false_positives['Probability'], color='red', label='False Positives', alpha=0.5, bins=20)\n",
    "sns.histplot(false_negatives['Probability'], color='blue', label='False Negatives', alpha=0.5, bins=20)\n",
    "plt.axvline(x=0.5, color='black', linestyle='--', alpha=0.7, label='Threshold (0.5)')\n",
    "plt.title('Probability Distribution for Errors')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model and Results\n",
    "\n",
    "Save the model, scaler, and results for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create results directory if it doesn't exist\n",
    "results_dir = f'results_{DATASET_NAME.replace(\"-\", \"_\")}'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Save model if not already saved during training\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    model.save(MODEL_PATH)\n",
    "\n",
    "# Save scaler\n",
    "import joblib\n",
    "scaler_path = os.path.join(results_dir, 'scaler.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to {scaler_path}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names_path = os.path.join(results_dir, 'feature_names.txt')\n",
    "with open(feature_names_path, 'w') as f:\n",
    "    for feature in feature_names:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "print(f\"Feature names saved to {feature_names_path}\")\n",
    "\n",
    "# Save model configuration\n",
    "config = {\n",
    "    'dataset_name': DATASET_NAME,\n",
    "    'train_file': TRAIN_FILE,\n",
    "    'test_file': TEST_FILE,\n",
    "    'time_steps': TIME_STEPS,\n",
    "    'step_size': STEP_SIZE,\n",
    "    'n_features': N_FEATURES,\n",
    "    'feature_selection': FEATURE_SELECTION,\n",
    "    'lstm_units': LSTM_UNITS,\n",
    "    'dense_units': DENSE_UNITS,\n",
    "    'dropout_rate': DROPOUT_RATE,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': EPOCHS,\n",
    "    'patience': PATIENCE,\n",
    "    'model_path': MODEL_PATH,\n",
    "    'accuracy': results['accuracy'],\n",
    "    'precision': results['precision'],\n",
    "    'recall': results['recall'],\n",
    "    'f1': results['f1'],\n",
    "    'auc': results['auc']\n",
    "}\n",
    "\n",
    "config_path = os.path.join(results_dir, 'config.json')\n",
    "import json\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "print(f\"Configuration saved to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "In this notebook, we analyzed the HAI dataset and built a Residual Bidirectional LSTM model for anomaly detection. The model achieved good performance in detecting attacks, as evidenced by the evaluation metrics.\n",
    "\n",
    "Key steps in the analysis:\n",
    "1. Data loading and exploration\n",
    "2. Feature engineering and sequence creation\n",
    "3. Model creation and training\n",
    "4. Model evaluation and visualization\n",
    "5. Saving model and results for future use\n",
    "\n",
    "This template can be adapted for any HAI dataset by changing the configuration parameters at the beginning of the notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}