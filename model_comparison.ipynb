{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI Security Dataset: Model Comparison\n",
    "\n",
    "This notebook compares the performance of different anomaly detection models trained on the HAI security dataset. We'll evaluate LSTM, Random Forest, and Autoencoder models to determine their strengths and weaknesses for industrial control system anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, roc_curve, auc, f1_score, precision_score, recall_score, accuracy_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Test Data\n",
    "\n",
    "First, let's load the preprocessed test data to evaluate all models on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_processed_data(file_path):\n",
    "    \"\"\"\n",
    "    Load processed data from NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the NPZ file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Loaded data\n",
    "    \"\"\"\n",
    "    # Load NPZ file\n",
    "    npz_data = np.load(file_path)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(npz_data['data'], columns=npz_data['columns'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load preprocessor\n",
    "preprocessor_path = './models/hai_hai_20_07_standard_preprocessor.joblib'\n",
    "preprocessor_dict = joblib.load(preprocessor_path)\n",
    "\n",
    "# Extract important information\n",
    "feature_columns = preprocessor_dict['feature_columns']\n",
    "attack_columns = preprocessor_dict['attack_columns']\n",
    "timestamp_col = preprocessor_dict['timestamp_col']\n",
    "\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "print(f\"Attack columns: {attack_columns}\")\n",
    "print(f\"Timestamp column: {timestamp_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get list of test data files\n",
    "test_data_dir = './processed_data/hai-20.07/test'\n",
    "test_files = sorted(glob.glob(f'{test_data_dir}/*.npz'))\n",
    "\n",
    "print(f\"Test files: {[os.path.basename(f) for f in test_files]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_and_prepare_tabular_data(file_paths, feature_cols, target_col=None, max_files=None, sample_fraction=None):\n",
    "    \"\"\"\n",
    "    Load and prepare tabular data from multiple files.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of file paths\n",
    "        feature_cols: List of feature column names\n",
    "        target_col: Target column name (None for unsupervised learning)\n",
    "        max_files: Maximum number of files to load (None for all files)\n",
    "        sample_fraction: Fraction of data to sample (None for all data)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y) - Features and targets\n",
    "    \"\"\"\n",
    "    all_X = []\n",
    "    all_y = [] if target_col is not None else None\n",
    "    \n",
    "    # Limit the number of files if specified\n",
    "    if max_files is not None:\n",
    "        file_paths = file_paths[:max_files]\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        \n",
    "        # Load data\n",
    "        df = load_processed_data(file_path)\n",
    "        \n",
    "        # Sample data if specified\n",
    "        if sample_fraction is not None and sample_fraction < 1.0:\n",
    "            df = df.sample(frac=sample_fraction, random_state=42)\n",
    "        \n",
    "        # Extract features\n",
    "        X = df[feature_cols]\n",
    "        all_X.append(X)\n",
    "        \n",
    "        # Extract target if provided\n",
    "        if target_col is not None and target_col in df.columns:\n",
    "            y = df[target_col]\n",
    "            all_y.append(y)\n",
    "    \n",
    "    # Combine data from all files\n",
    "    combined_X = pd.concat(all_X) if all_X else pd.DataFrame()\n",
    "    combined_y = pd.concat(all_y) if all_y else None\n",
    "    \n",
    "    return combined_X, combined_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_sequences(data, feature_cols, target_col=None, seq_length=100):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM input.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame containing the data\n",
    "        feature_cols: List of feature column names\n",
    "        target_col: Target column name (None for unsupervised learning)\n",
    "        seq_length: Length of each sequence\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y) - Sequences and targets (if target_col is provided)\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = [] if target_col is not None else None\n",
    "    \n",
    "    # Extract features\n",
    "    features = data[feature_cols].values\n",
    "    \n",
    "    # Extract target if provided\n",
    "    targets = data[target_col].values if target_col is not None else None\n",
    "    \n",
    "    # Create sequences\n",
    "    for i in range(len(features) - seq_length):\n",
    "        X.append(features[i:i+seq_length])\n",
    "        if target_col is not None:\n",
    "            # Use the label of the last timestep in the sequence\n",
    "            y.append(targets[i+seq_length])\n",
    "    \n",
    "    return np.array(X), np.array(y) if target_col is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_and_prepare_sequence_data(file_paths, feature_cols, target_col=None, seq_length=100, max_files=None):\n",
    "    \"\"\"\n",
    "    Load and prepare sequence data from multiple files.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of file paths\n",
    "        feature_cols: List of feature column names\n",
    "        target_col: Target column name (None for unsupervised learning)\n",
    "        seq_length: Length of each sequence\n",
    "        max_files: Maximum number of files to load (None for all files)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y) - Combined sequences and targets\n",
    "    \"\"\"\n",
    "    all_X = []\n",
    "    all_y = [] if target_col is not None else None\n",
    "    \n",
    "    # Limit the number of files if specified\n",
    "    if max_files is not None:\n",
    "        file_paths = file_paths[:max_files]\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        \n",
    "        # Load data\n",
    "        df = load_processed_data(file_path)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = create_sequences(df, feature_cols, target_col, seq_length)\n",
    "        \n",
    "        all_X.append(X)\n",
    "        if target_col is not None:\n",
    "            all_y.append(y)\n",
    "    \n",
    "    # Combine data from all files\n",
    "    combined_X = np.vstack(all_X) if all_X else np.array([])\n",
    "    combined_y = np.concatenate(all_y) if all_y else None\n",
    "    \n",
    "    return combined_X, combined_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set parameters\n",
    "target_col = 'attack' if attack_columns else None  # Target column\n",
    "sample_fraction = 0.1  # Sample 10% of data to reduce memory usage\n",
    "seq_length = 100  # Sequence length for LSTM\n",
    "\n",
    "# Load tabular test data (for Random Forest and Autoencoder)\n",
    "print(\"Loading tabular test data...\")\n",
    "X_test_tabular, y_test_tabular = load_and_prepare_tabular_data(test_files, feature_columns, \n",
    "                                                              target_col=target_col, \n",
    "                                                              max_files=2, \n",
    "                                                              sample_fraction=sample_fraction)\n",
    "\n",
    "# Load sequence test data (for LSTM)\n",
    "print(\"\\nLoading sequence test data...\")\n",
    "X_test_sequence, y_test_sequence = load_and_prepare_sequence_data(test_files, feature_columns, \n",
    "                                                                 target_col=target_col, \n",
    "                                                                 seq_length=seq_length, \n",
    "                                                                 max_files=2)\n",
    "\n",
    "print(f\"\\nTabular test data shape: {X_test_tabular.shape}, Labels shape: {y_test_tabular.shape if y_test_tabular is not None else None}\")\n",
    "print(f\"Sequence test data shape: {X_test_sequence.shape}, Labels shape: {y_test_sequence.shape if y_test_sequence is not None else None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Trained Models\n",
    "\n",
    "Now let's load the trained models for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if models exist\n",
    "model_files = {\n",
    "    'lstm': './models/lstm_autoencoder_hai_20_07.h5',\n",
    "    'random_forest': './models/random_forest_hai_20_07.joblib',\n",
    "    'autoencoder': './models/autoencoder_hai_20_07.h5'\n",
    "}\n",
    "\n",
    "metadata_files = {\n",
    "    'lstm': './models/lstm_model_metadata_hai_20_07.joblib',\n",
    "    'random_forest': './models/random_forest_metadata_hai_20_07.joblib',\n",
    "    'autoencoder': './models/autoencoder_metadata_hai_20_07.joblib'\n",
    "}\n",
    "\n",
    "# Check which models are available\n",
    "available_models = {}\n",
    "available_metadata = {}\n",
    "\n",
    "for model_name, model_path in model_files.items():\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"{model_name.capitalize()} model found at {model_path}\")\n",
    "        available_models[model_name] = model_path\n",
    "    else:\n",
    "        print(f\"{model_name.capitalize()} model not found at {model_path}\")\n",
    "\n",
    "for model_name, metadata_path in metadata_files.items():\n",
    "    if os.path.exists(metadata_path):\n",
    "        print(f\"{model_name.capitalize()} metadata found at {metadata_path}\")\n",
    "        available_metadata[model_name] = metadata_path\n",
    "    else:\n",
    "        print(f\"{model_name.capitalize()} metadata not found at {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load models and metadata\n",
    "models = {}\n",
    "metadata = {}\n",
    "\n",
    "for model_name, model_path in available_models.items():\n",
    "    try:\n",
    "        if model_name == 'random_forest':\n",
    "            # Load scikit-learn model\n",
    "            models[model_name] = joblib.load(model_path)\n",
    "        else:\n",
    "            # Load Keras model\n",
    "            models[model_name] = load_model(model_path)\n",
    "        print(f\"Loaded {model_name} model successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_name} model: {e}\")\n",
    "\n",
    "for model_name, metadata_path in available_metadata.items():\n",
    "    try:\n",
    "        metadata[model_name] = joblib.load(metadata_path)\n",
    "        print(f\"Loaded {model_name} metadata successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_name} metadata: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate Models\n",
    "\n",
    "Now let's evaluate each model on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_model(model_name, model, X_test, y_test, threshold=None):\n",
    "    \"\"\"\n",
    "    Evaluate a model on test data.\n",
    "    \n",
    "    Args:\n",
    "        model_name: Name of the model\n",
    "        model: Trained model\n",
    "        X_test: Test features\n",
    "        y_test: Test labels\n",
    "        threshold: Anomaly threshold (None to use default)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    print(f\"Evaluating {model_name} model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Get predictions\n",
    "    if model_name == 'lstm':\n",
    "        # LSTM autoencoder reconstruction\n",
    "        X_pred = model.predict(X_test)\n",
    "        # Calculate reconstruction error (MSE)\n",
    "        anomaly_scores = np.mean(np.square(X_test - X_pred), axis=(1, 2))\n",
    "    elif model_name == 'random_forest':\n",
    "        # Random Forest probability of normal class\n",
    "        y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "        # Convert to anomaly score (higher = more anomalous)\n",
    "        anomaly_scores = 1 - y_pred_proba\n",
    "    elif model_name == 'autoencoder':\n",
    "        # Autoencoder reconstruction\n",
    "        X_pred = model.predict(X_test)\n",
    "        # Calculate reconstruction error (MSE)\n",
    "        anomaly_scores = np.mean(np.square(X_test - X_pred), axis=1)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {model_name}\")\n",
    "    \n",
    "    # Use threshold from metadata if not provided\n",
    "    if threshold is None and model_name in metadata:\n",
    "        threshold = metadata[model_name]['threshold']\n",
    "        print(f\"Using threshold from metadata: {threshold}\")\n",
    "    elif threshold is None:\n",
    "        # Use statistical threshold if no metadata\n",
    "        threshold = np.mean(anomaly_scores) + 3 * np.std(anomaly_scores)\n",
    "        print(f\"Using statistical threshold: {threshold}\")\n",
    "    \n",
    "    # Classify as anomaly if score > threshold\n",
    "    y_pred = (anomaly_scores > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred),\n",
    "        'f1': f1_score(y_test, y_pred),\n",
    "        'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "        'anomaly_scores': anomaly_scores,\n",
    "        'threshold': threshold,\n",
    "        'prediction_time': time.time() - start_time\n",
    "    }\n",
    "    \n",
    "    # Calculate ROC AUC\n",
    "    fpr, tpr, _ = roc_curve(y_test, anomaly_scores)\n",
    "    metrics['roc_auc'] = auc(fpr, tpr)\n",
    "    metrics['fpr'] = fpr\n",
    "    metrics['tpr'] = tpr\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"ROC AUC: {metrics['roc_auc']:.4f}\")\n",
    "    print(f\"Prediction time: {metrics['prediction_time']:.2f} seconds\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate models\n",
    "evaluation_results = {}\n",
    "\n",
    "# Evaluate LSTM model\n",
    "if 'lstm' in models:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Evaluating LSTM Autoencoder\")\n",
    "    print(\"=\" * 50)\n",
    "    evaluation_results['lstm'] = evaluate_model('lstm', models['lstm'], X_test_sequence, y_test_sequence)\n",
    "\n",
    "# Evaluate Random Forest model\n",
    "if 'random_forest' in models:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Evaluating Random Forest\")\n",
    "    print(\"=\" * 50)\n",
    "    evaluation_results['random_forest'] = evaluate_model('random_forest', models['random_forest'], X_test_tabular, y_test_tabular)\n",
    "\n",
    "# Evaluate Autoencoder model\n",
    "if 'autoencoder' in models:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Evaluating Autoencoder\")\n",
    "    print(\"=\" * 50)\n",
    "    evaluation_results['autoencoder'] = evaluate_model('autoencoder', models['autoencoder'], X_test_tabular.values, y_test_tabular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Model Performance\n",
    "\n",
    "Let's compare the performance of the different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a comparison table\n",
    "if evaluation_results:\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc', 'prediction_time']\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, results in evaluation_results.items():\n",
    "        row = {'Model': model_name.capitalize()}\n",
    "        for metric in metrics:\n",
    "            if metric in results:\n",
    "                row[metric] = results[metric]\n",
    "        comparison_data.append(row)\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Format the table\n",
    "    pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "    display(comparison_df)\n",
    "    \n",
    "    # Reset display format\n",
    "    pd.reset_option('display.float_format')\n",
    "else:\n",
    "    print(\"No evaluation results available for comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot comparison of key metrics\n",
    "if len(evaluation_results) > 1:\n",
    "    metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    plot_data = []\n",
    "    for model_name, results in evaluation_results.items():\n",
    "        for metric in metrics_to_plot:\n",
    "            if metric in results:\n",
    "                plot_data.append({\n",
    "                    'Model': model_name.capitalize(),\n",
    "                    'Metric': metric.capitalize(),\n",
    "                    'Value': results[metric]\n",
    "                })\n",
    "    \n",
    "    plot_df = pd.DataFrame(plot_data)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.barplot(x='Metric', y='Value', hue='Model', data=plot_df)\n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.ylim(0, 1.05)  # Metrics are between 0 and 1\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Need at least two models for comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot ROC curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name, results in evaluation_results.items():\n",
    "    if 'fpr' in results and 'tpr' in results:\n",
    "        plt.plot(results['fpr'], results['tpr'], \n",
    "                label=f'{model_name.capitalize()} (AUC = {results[\"roc_auc\"]:.4f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for All Models')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot confusion matrices for all models\n",
    "if evaluation_results:\n",
    "    n_models = len(evaluation_results)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 5))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = [axes]  # Make axes iterable if only one subplot\n",
    "    \n",
    "    for i, (model_name, results) in enumerate(evaluation_results.items()):\n",
    "        if 'confusion_matrix' in results:\n",
    "            cm = results['confusion_matrix']\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
    "            axes[i].set_title(f'{model_name.capitalize()} Confusion Matrix')\n",
    "            axes[i].set_ylabel('True Label')\n",
    "            axes[i].set_xlabel('Predicted Label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Anomaly Score Distributions\n",
    "\n",
    "Let's compare the anomaly score distributions for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot anomaly score distributions\n",
    "if evaluation_results:\n",
    "    n_models = len(evaluation_results)\n",
    "    fig, axes = plt.subplots(n_models, 1, figsize=(12, 5*n_models))\n",
    "    \n",
    "    if n_models == 1:\n",
    "        axes = [axes]  # Make axes iterable if only one subplot\n",
    "    \n",
    "    for i, (model_name, results) in enumerate(evaluation_results.items()):\n",
    "        if 'anomaly_scores' in results and 'threshold' in results:\n",
    "            scores = results['anomaly_scores']\n",
    "            threshold = results['threshold']\n",
    "            \n",
    "            # Get true labels\n",
    "            y_true = y_test_sequence if model_name == 'lstm' else y_test_tabular\n",
    "            \n",
    "            # Create DataFrame for easier plotting\n",
    "            score_df = pd.DataFrame({\n",
    "                'Anomaly Score': scores,\n",
    "                'True Label': y_true\n",
    "            })\n",
    "            \n",
    "            # Plot distributions\n",
    "            sns.histplot(data=score_df, x='Anomaly Score', hue='True Label', \n",
    "                        bins=50, kde=True, ax=axes[i])\n",
    "            \n",
    "            # Add threshold line\n",
    "            axes[i].axvline(x=threshold, color='r', linestyle='--', \n",
    "                           label=f'Threshold = {threshold:.6f}')\n",
    "            \n",
    "            axes[i].set_title(f'{model_name.capitalize()} Anomaly Score Distribution')\n",
    "            axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Strengths and Weaknesses\n",
    "\n",
    "Let's analyze the strengths and weaknesses of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a table of model strengths and weaknesses\n",
    "model_analysis = {\n",
    "    'lstm': {\n",
    "        'strengths': [\n",
    "            'Captures temporal dependencies in the data',\n",
    "            'Effective for detecting anomalies that manifest over time',\n",
    "            'Can handle complex patterns in time series data'\n",
    "        ],\n",
    "        'weaknesses': [\n",
    "            'Computationally intensive to train',\n",
    "            'Requires sequence data, which increases memory usage',\n",
    "            'Less interpretable than tree-based models'\n",
    "        ],\n",
    "        'best_for': 'Detecting anomalies that develop over time, such as gradual sensor drift or sequential attack patterns'\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'strengths': [\n",
    "            'Highly interpretable with feature importance',\n",
    "            'Fast prediction time',\n",
    "            'Handles high-dimensional data well',\n",
    "            'Robust to outliers and noise'\n",
    "        ],\n",
    "        'weaknesses': [\n",
    "            'Does not capture temporal dependencies',\n",
    "            'May overfit on small datasets',\n",
    "            'Limited in capturing complex non-linear relationships'\n",
    "        ],\n",
    "        'best_for': 'Detecting anomalies based on feature values at a single time point, with good interpretability of which features contribute to anomalies'\n",
    "    },\n",
    "    'autoencoder': {\n",
    "        'strengths': [\n",
    "            'Learns complex non-linear relationships',\n",
    "            'Effective for high-dimensional data reduction',\n",
    "            'Can capture subtle patterns that simpler models might miss'\n",
    "        ],\n",
    "        'weaknesses': [\n",
    "            'Less interpretable than tree-based models',\n",
    "            'Requires careful tuning of architecture',\n",
    "            'May struggle with very sparse anomalies'\n",
    "        ],\n",
    "        'best_for': 'Detecting complex anomalies that involve non-linear relationships between features'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display analysis for available models\n",
    "for model_name in evaluation_results.keys():\n",
    "    if model_name in model_analysis:\n",
    "        print(f\"\\n{model_name.upper()} ANALYSIS\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"Strengths:\")\n",
    "        for strength in model_analysis[model_name]['strengths']:\n",
    "            print(f\"- {strength}\")\n",
    "        print(\"\\nWeaknesses:\")\n",
    "        for weakness in model_analysis[model_name]['weaknesses']:\n",
    "            print(f\"- {weakness}\")\n",
    "        print(f\"\\nBest for: {model_analysis[model_name]['best_for']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ensemble Approach\n",
    "\n",
    "Let's explore an ensemble approach that combines the predictions of multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create ensemble predictions if we have multiple models\n",
    "if len(evaluation_results) > 1:\n",
    "    print(\"Creating ensemble predictions...\")\n",
    "    \n",
    "    # Get predictions from each model\n",
    "    predictions = {}\n",
    "    for model_name, results in evaluation_results.items():\n",
    "        if 'anomaly_scores' in results and 'threshold' in results:\n",
    "            # Normalize scores to [0, 1] range for fair comparison\n",
    "            scores = results['anomaly_scores']\n",
    "            min_score = np.min(scores)\n",
    "            max_score = np.max(scores)\n",
    "            normalized_scores = (scores - min_score) / (max_score - min_score)\n",
    "            \n",
    "            predictions[model_name] = {\n",
    "                'scores': normalized_scores,\n",
    "                'binary': (scores > results['threshold']).astype(int)\n",
    "            }\n",
    "    \n",
    "    # Create ensemble predictions using different strategies\n",
    "    \n",
    "    # 1. Majority voting (for binary predictions)\n",
    "    if len(predictions) >= 2:\n",
    "        binary_preds = np.column_stack([predictions[model]['binary'] for model in predictions])\n",
    "        majority_vote = np.sum(binary_preds, axis=1) >= (len(predictions) / 2)\n",
    "        \n",
    "        # Evaluate majority voting\n",
    "        y_true = y_test_tabular  # Use tabular labels for evaluation\n",
    "        \n",
    "        accuracy = accuracy_score(y_true, majority_vote)\n",
    "        precision = precision_score(y_true, majority_vote)\n",
    "        recall = recall_score(y_true, majority_vote)\n",
    "        f1 = f1_score(y_true, majority_vote)\n",
    "        \n",
    "        print(\"\\nEnsemble (Majority Voting) Results:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        cm = confusion_matrix(y_true, majority_vote)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Ensemble (Majority Voting) Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "        \n",
    "    # 2. Average of normalized scores\n",
    "    if len(predictions) >= 2:\n",
    "        score_matrix = np.column_stack([predictions[model]['scores'] for model in predictions])\n",
    "        avg_scores = np.mean(score_matrix, axis=1)\n",
    "        \n",
    "        # Find optimal threshold\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, avg_scores)\n",
    "        gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "        ix = np.argmax(gmeans)\n",
    "        optimal_threshold = thresholds[ix]\n",
    "        \n",
    "        # Make predictions\n",
    "        avg_preds = (avg_scores > optimal_threshold).astype(int)\n",
    "        \n",
    "        # Evaluate\n",
    "        accuracy = accuracy_score(y_true, avg_preds)\n",
    "        precision = precision_score(y_true, avg_preds)\n",
    "        recall = recall_score(y_true, avg_preds)\n",
    "        f1 = f1_score(y_true, avg_preds)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        print(\"\\nEnsemble (Average Scores) Results:\")\n",
    "        print(f\"Optimal threshold: {optimal_threshold:.6f}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.plot(fpr, tpr, marker='.')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.scatter(fpr[ix], tpr[ix], marker='o', color='red', \n",
    "                   label=f'Optimal (Threshold = {optimal_threshold:.6f})')\n",
    "        plt.title('Ensemble (Average Scores) ROC Curve')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        cm = confusion_matrix(y_true, avg_preds)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Ensemble (Average Scores) Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Need at least two models for ensemble approach.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Recommendations\n",
    "\n",
    "Based on our analysis, we can draw the following conclusions and make recommendations for anomaly detection in industrial control systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Summary of Model Performance\n",
    "\n",
    "Let's summarize the performance of each model and the ensemble approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a summary table with all models including ensemble if available\n",
    "summary_data = []\n",
    "\n",
    "# Add individual models\n",
    "for model_name, results in evaluation_results.items():\n",
    "    summary_data.append({\n",
    "        'Model': model_name.capitalize(),\n",
    "        'Accuracy': results.get('accuracy', np.nan),\n",
    "        'Precision': results.get('precision', np.nan),\n",
    "        'Recall': results.get('recall', np.nan),\n",
    "        'F1 Score': results.get('f1', np.nan),\n",
    "        'ROC AUC': results.get('roc_auc', np.nan)\n",
    "    })\n",
    "\n",
    "# Add ensemble if available\n",
    "if 'accuracy' in locals() and 'precision' in locals() and 'recall' in locals() and 'f1' in locals() and 'roc_auc' in locals():\n",
    "    summary_data.append({\n",
    "        'Model': 'Ensemble (Avg)',\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'ROC AUC': roc_auc\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Format the table\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "display(summary_df)\n",
    "\n",
    "# Reset display format\n",
    "pd.reset_option('display.float_format')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Recommendations\n",
    "\n",
    "Based on our analysis, here are our recommendations for anomaly detection in industrial control systems:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Best Overall Model**: [Will be determined based on actual results]\n",
    "\n",
    "2. **For Real-time Detection**: Random Forest offers the fastest prediction time while maintaining good accuracy, making it suitable for real-time anomaly detection in industrial control systems.\n",
    "\n",
    "3. **For Complex Temporal Patterns**: LSTM is recommended for detecting anomalies that manifest over time, such as gradual sensor drift or sequential attack patterns.\n",
    "\n",
    "4. **For Interpretability**: Random Forest provides feature importance, making it easier to understand which sensors or components are contributing to anomalies.\n",
    "\n",
    "5. **For Complex Non-linear Relationships**: Autoencoder is effective at capturing complex non-linear relationships between features.\n",
    "\n",
    "6. **Ensemble Approach**: For critical systems where false negatives must be minimized, an ensemble approach combining multiple models can provide more robust detection.\n",
    "\n",
    "7. **Deployment Strategy**: \n",
    "   - For edge devices with limited computational resources: Random Forest\n",
    "   - For centralized monitoring systems with GPU capabilities: LSTM or Autoencoder\n",
    "   - For critical infrastructure: Ensemble approach\n",
    "\n",
    "8. **Future Work**:\n",
    "   - Explore more sophisticated ensemble methods that weight models based on their performance in specific types of anomalies\n",
    "   - Investigate online learning approaches to adapt to evolving normal behavior\n",
    "   - Develop explainable AI techniques to better understand the nature of detected anomalies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}