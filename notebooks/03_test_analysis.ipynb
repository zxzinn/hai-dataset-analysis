{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI Dataset Test Analysis and Anomaly Detection\n",
    "\n",
    "This notebook implements anomaly detection models and analyzes their performance on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import joblib\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define paths\n",
    "PROCESSED_DIR = Path('../data/processed')\n",
    "MODELS_DIR = Path('../models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load processed data\n",
    "train_files = list(PROCESSED_DIR.glob('train_*.parquet'))\n",
    "test_files = list(PROCESSED_DIR.glob('test_*.parquet'))\n",
    "\n",
    "train_dfs = [pl.read_parquet(f) for f in train_files]\n",
    "test_dfs = [pl.read_parquet(f) for f in test_files]\n",
    "\n",
    "train_df = pl.concat(train_dfs)\n",
    "test_df = pl.concat(test_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# LSTM Autoencoder Model\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=input_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        encoded, _ = self.encoder(x)\n",
    "        \n",
    "        # Decode\n",
    "        decoded, _ = self.decoder(encoded)\n",
    "        \n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TCN Model\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                              stride=stride, padding=padding, dilation=dilation)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.dropout(self.relu(self.conv1(x)))\n",
    "        return out\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation = 2 ** i\n",
    "            in_channels = input_size if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers.append(\n",
    "                TemporalBlock(in_channels, out_channels, kernel_size,\n",
    "                             stride=1, dilation=dilation,\n",
    "                             padding=(kernel_size-1) * dilation)\n",
    "            )\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        x = self.linear(x.transpose(1,2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare data for models\n",
    "def prepare_sequences(df, sequence_length=60):\n",
    "    # Select features\n",
    "    feature_cols = [\n",
    "        col for col in df.columns \n",
    "        if any(x in col for x in ['_error', '_PV_mean_', '_PV_std_'])\n",
    "    ]\n",
    "    \n",
    "    # Convert to numpy and scale\n",
    "    data = df.select(feature_cols).to_numpy()\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data)\n",
    "    \n",
    "    # Create sequences\n",
    "    sequences = []\n",
    "    for i in range(len(scaled_data) - sequence_length + 1):\n",
    "        sequences.append(scaled_data[i:i+sequence_length])\n",
    "    \n",
    "    return np.array(sequences), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train models\n",
    "# 1. Isolation Forest\n",
    "isolation_forest = IsolationForest(random_state=42, contamination=0.1)\n",
    "isolation_forest.fit(train_df.select(pl.all()).to_numpy())\n",
    "joblib.dump(isolation_forest, MODELS_DIR / 'isolation_forest/model.joblib')\n",
    "\n",
    "# 2. LSTM Autoencoder\n",
    "sequences, scaler = prepare_sequences(train_df)\n",
    "input_size = sequences.shape[2]\n",
    "hidden_size = 32\n",
    "\n",
    "lstm_autoencoder = LSTMAutoencoder(input_size, hidden_size)\n",
    "lstm_autoencoder.train()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm_autoencoder.parameters())\n",
    "\n",
    "# Convert to torch tensors\n",
    "train_sequences = torch.FloatTensor(sequences)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(train_sequences), batch_size):\n",
    "        batch = train_sequences[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_autoencoder(batch)\n",
    "        loss = criterion(outputs, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "torch.save(lstm_autoencoder.state_dict(), MODELS_DIR / 'lstm_autoencoder/model.pth')\n",
    "joblib.dump(scaler, MODELS_DIR / 'lstm_autoencoder/scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate models on test data\n",
    "def get_anomaly_scores(model, data):\n",
    "    if isinstance(model, IsolationForest):\n",
    "        return -model.score_samples(data)\n",
    "    elif isinstance(model, LSTMAutoencoder):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            reconstructed = model(torch.FloatTensor(data))\n",
    "            reconstruction_error = torch.mean((torch.FloatTensor(data) - reconstructed) ** 2, dim=(1,2))\n",
    "            return reconstruction_error.numpy()\n",
    "\n",
    "# Get anomaly scores\n",
    "if_scores = get_anomaly_scores(isolation_forest, test_df.select(pl.all()).to_numpy())\n",
    "test_sequences, _ = prepare_sequences(test_df)\n",
    "lstm_scores = get_anomaly_scores(lstm_autoencoder, test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot results\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot Isolation Forest scores\n",
    "axes[0].plot(if_scores)\n",
    "axes[0].set_title('Isolation Forest Anomaly Scores')\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Anomaly Score')\n",
    "\n",
    "# Plot LSTM Autoencoder scores\n",
    "axes[1].plot(lstm_scores)\n",
    "axes[1].set_title('LSTM Autoencoder Reconstruction Error')\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Reconstruction Error')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze attack detection performance\n",
    "def plot_attack_detection(scores, attack_labels, title):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot scores\n",
    "    plt.plot(scores, label='Anomaly Score', alpha=0.7)\n",
    "    \n",
    "    # Highlight attack periods\n",
    "    attack_regions = np.where(attack_labels == 1)[0]\n",
    "    plt.fill_between(range(len(scores)), \n",
    "                     min(scores), max(scores),\n",
    "                     where=attack_labels == 1,\n",
    "                     color='red', alpha=0.3,\n",
    "                     label='Attack Period')\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Anomaly Score')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Get attack labels\n",
    "attack_labels = test_df.select('attack').to_numpy().flatten()\n",
    "\n",
    "# Plot detection results\n",
    "plot_attack_detection(if_scores, attack_labels, 'Isolation Forest Attack Detection')\n",
    "plot_attack_detection(lstm_scores, attack_labels, 'LSTM Autoencoder Attack Detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analyze detection performance by control loop\n",
    "control_loops = ['P1-PC', 'P1-LC', 'P1-FC', 'P1-TC']\n",
    "\n",
    "for loop in control_loops:\n",
    "    # Get attack periods for this loop\n",
    "    loop_attacks = test_df.select(f'{loop.lower()}_attack').to_numpy().flatten()\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot anomaly scores during attacks\n",
    "    plt.scatter(range(len(if_scores)), if_scores, \n",
    "               c=loop_attacks, cmap='coolwarm',\n",
    "               alpha=0.6)\n",
    "    \n",
    "    plt.title(f'Anomaly Detection Performance for {loop}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Anomaly Score')\n",
    "    plt.colorbar(label='Attack')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
