{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI-20.07 Dataset Anomaly Detection - Isolation Forest Model\n",
    "\n",
    "This notebook uses the Isolation Forest algorithm for anomaly detection on the HAI-20.07 dataset.\n",
    "\n",
    "The HAI dataset contains data from industrial control systems (ICS), where training data does not include attack labels, while test data includes attack labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, auc, roc_curve, f1_score, precision_score, recall_score\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data path\n",
    "data_path = \"../../hai-security-dataset/hai-20.07/\"\n",
    "\n",
    "# Load training data\n",
    "train_files = [f for f in os.listdir(data_path) if f.startswith('train')]\n",
    "train_dfs = []\n",
    "\n",
    "for file in train_files:\n",
    "    print(f\"Loading training file: {file}\")\n",
    "    df = pd.read_csv(f\"{data_path}{file}\", sep=\";\")\n",
    "    train_dfs.append(df)\n",
    "    \n",
    "# Combine training data\n",
    "train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "print(f\"Training data shape: {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_files = [f for f in os.listdir(data_path) if f.startswith('test')]\n",
    "test_dfs = []\n",
    "\n",
    "for file in test_files:\n",
    "    print(f\"Loading test file: {file}\")\n",
    "    df = pd.read_csv(f\"{data_path}{file}\", sep=\";\")\n",
    "    test_dfs.append(df)\n",
    "    \n",
    "# Combine test data\n",
    "test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "print(f\"Test data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic information of the dataset\n",
    "print(\"Column names in training dataset:\")\n",
    "print(train_df.columns.tolist())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in training dataset:\")\n",
    "print(train_df.isnull().sum().sum())\n",
    "\n",
    "print(\"\\nMissing values in test dataset:\")\n",
    "print(test_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "train_df['time'] = pd.to_datetime(train_df['time'])\n",
    "test_df['time'] = pd.to_datetime(test_df['time'])\n",
    "\n",
    "# Extract attack labels from test data\n",
    "attack_columns = [col for col in test_df.columns if 'attack' in col.lower()]\n",
    "print(f\"Attack label columns: {attack_columns}\")\n",
    "\n",
    "# Create a combined attack label (if multiple attack columns exist)\n",
    "if len(attack_columns) > 1:\n",
    "    test_df['attack_combined'] = test_df[attack_columns].max(axis=1)\n",
    "    y_test = test_df['attack_combined']\n",
    "else:\n",
    "    y_test = test_df[attack_columns[0]]\n",
    "\n",
    "# Print attack distribution\n",
    "print(f\"\\nAttack distribution in test data:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training and testing\n",
    "# Exclude timestamp and attack labels\n",
    "feature_columns = [col for col in train_df.columns if col not in ['time'] + attack_columns]\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "\n",
    "# Prepare training and testing data\n",
    "X_train = train_df[feature_columns].values\n",
    "X_test = test_df[feature_columns].values\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training data shape after preprocessing: {X_train_scaled.shape}\")\n",
    "print(f\"Testing data shape after preprocessing: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Isolation Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "n_estimators = 100\n",
    "max_samples = 'auto'\n",
    "contamination = 'auto'\n",
    "random_state = 42\n",
    "\n",
    "# Create and train the model\n",
    "print(\"Training Isolation Forest model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "model = IsolationForest(\n",
    "    n_estimators=n_estimators,\n",
    "    max_samples=max_samples,\n",
    "    contamination=contamination,\n",
    "    random_state=random_state,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save the model if it doesn't exist\n",
    "model_dir = \"./\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "model_filename = os.path.join(model_dir, \"isolation_forest_model.pkl\")\n",
    "scaler_filename = os.path.join(model_dir, \"scaler.pkl\")\n",
    "\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "    \n",
    "with open(scaler_filename, 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "    \n",
    "print(f\"Model saved to {model_filename}\")\n",
    "print(f\"Scaler saved to {scaler_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict anomalies on test data\n",
    "# Isolation Forest returns -1 for anomalies and 1 for normal data\n",
    "print(\"Predicting anomalies on test data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "y_pred_raw = model.predict(X_test_scaled)\n",
    "# Convert to binary format (0 for normal, 1 for anomaly)\n",
    "y_pred = np.where(y_pred_raw == -1, 1, 0)\n",
    "\n",
    "prediction_time = time.time() - start_time\n",
    "print(f\"Prediction completed in {prediction_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate anomaly scores\n",
    "# Lower scores (more negative) indicate higher anomaly probability\n",
    "anomaly_scores = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Plot anomaly score distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(anomaly_scores, bins=50, alpha=0.7)\n",
    "plt.title('Anomaly Score Distribution')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(\"Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Normal', 'Anomaly'],\n",
    "            yticklabels=['Normal', 'Anomaly'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calculate metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "# For Isolation Forest, we need to use negative of anomaly scores as the decision function\n",
    "# since lower scores indicate higher anomaly probability\n",
    "fpr, tpr, thresholds = roc_curve(y_test, -anomaly_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, -anomaly_scores)\n",
    "pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(recall_curve, precision_curve, color='blue', lw=2, label=f'PR curve (area = {pr_auc:.4f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Anomalies Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with timestamps, actual labels, and predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'timestamp': test_df['time'],\n",
    "    'actual': y_test,\n",
    "    'predicted': y_pred,\n",
    "    'anomaly_score': anomaly_scores\n",
    "})\n",
    "\n",
    "# Plot actual vs predicted anomalies over time\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Sample data for better visualization if dataset is large\n",
    "sample_size = min(10000, len(results_df))\n",
    "sample_indices = np.linspace(0, len(results_df)-1, sample_size, dtype=int)\n",
    "sample_df = results_df.iloc[sample_indices]\n",
    "\n",
    "plt.plot(sample_df['timestamp'], sample_df['actual'], 'b-', alpha=0.5, label='Actual')\n",
    "plt.plot(sample_df['timestamp'], sample_df['predicted'], 'r-', alpha=0.5, label='Predicted')\n",
    "plt.title('Actual vs Predicted Anomalies Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Anomaly (1) / Normal (0)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot anomaly scores over time with actual labels\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Create a colormap based on actual labels\n",
    "colors = np.where(sample_df['actual'] == 1, 'red', 'blue')\n",
    "\n",
    "plt.scatter(sample_df['timestamp'], sample_df['anomaly_score'], c=colors, alpha=0.5, s=10)\n",
    "plt.title('Anomaly Scores Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Anomaly Score')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add a horizontal line at the threshold (typically 0 for Isolation Forest)\n",
    "plt.axhline(y=0, color='g', linestyle='--')\n",
    "\n",
    "# Add a legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Actual Anomaly'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Normal'),\n",
    "    Line2D([0], [0], color='g', linestyle='--', label='Threshold')\n",
    "]\n",
    "plt.legend(handles=legend_elements)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different thresholds for anomaly detection\n",
    "thresholds = np.linspace(min(anomaly_scores), max(anomaly_scores), 100)\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = np.where(anomaly_scores <= threshold, 1, 0)\n",
    "    f1_scores.append(f1_score(y_test, y_pred_threshold))\n",
    "    precision_scores.append(precision_score(y_test, y_pred_threshold))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_threshold))\n",
    "\n",
    "# Find the threshold that maximizes F1 score\n",
    "best_threshold_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_idx]\n",
    "best_f1 = f1_scores[best_threshold_idx]\n",
    "best_precision = precision_scores[best_threshold_idx]\n",
    "best_recall = recall_scores[best_threshold_idx]\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.4f}\")\n",
    "print(f\"Best F1 score: {best_f1:.4f}\")\n",
    "print(f\"Precision at best threshold: {best_precision:.4f}\")\n",
    "print(f\"Recall at best threshold: {best_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1, precision, and recall scores for different thresholds\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(thresholds, f1_scores, 'b-', label='F1 Score')\n",
    "plt.plot(thresholds, precision_scores, 'g-', label='Precision')\n",
    "plt.plot(thresholds, recall_scores, 'r-', label='Recall')\n",
    "plt.axvline(x=best_threshold, color='k', linestyle='--', label=f'Best Threshold: {best_threshold:.4f}')\n",
    "plt.title('Performance Metrics vs. Threshold')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the optimized threshold\n",
    "y_pred_optimized = np.where(anomaly_scores <= best_threshold, 1, 0)\n",
    "\n",
    "# Evaluate with optimized threshold\n",
    "print(\"Confusion Matrix with Optimized Threshold:\")\n",
    "cm_optimized = confusion_matrix(y_test, y_pred_optimized)\n",
    "print(cm_optimized)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_optimized, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Normal', 'Anomaly'],\n",
    "            yticklabels=['Normal', 'Anomaly'])\n",
    "plt.title('Confusion Matrix with Optimized Threshold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report with Optimized Threshold:\")\n",
    "print(classification_report(y_test, y_pred_optimized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save the Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized threshold\n",
    "threshold_filename = os.path.join(model_dir, \"optimized_threshold.pkl\")\n",
    "with open(threshold_filename, 'wb') as file:\n",
    "    pickle.dump(best_threshold, file)\n",
    "    \n",
    "print(f\"Optimized threshold saved to {threshold_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Loaded and preprocessed the HAI-20.07 dataset\n",
    "2. Trained an Isolation Forest model for anomaly detection\n",
    "3. Evaluated the model's performance using various metrics\n",
    "4. Optimized the anomaly detection threshold to maximize F1 score\n",
    "5. Saved the model, scaler, and optimized threshold for future use\n",
    "\n",
    "The Isolation Forest algorithm has demonstrated its ability to detect anomalies in the industrial control system data without requiring labeled training data, making it suitable for real-world scenarios where attack data may not be available during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}