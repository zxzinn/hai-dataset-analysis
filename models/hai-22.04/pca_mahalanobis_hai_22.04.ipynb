{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI-22.04 Dataset Anomaly Detection - PCA and Mahalanobis Distance\n",
    "\n",
    "This notebook uses Principal Component Analysis (PCA) and Mahalanobis Distance for anomaly detection on the HAI-22.04 dataset.\n",
    "\n",
    "The HAI dataset contains data from industrial control systems (ICS), where training data does not include attack labels, while test data includes attack labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, auc, roc_curve, f1_score, precision_score, recall_score\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data path\n",
    "data_path = \"../../hai-security-dataset/hai-22.04/\"\n",
    "\n",
    "# Load training data\n",
    "train_files = [f for f in os.listdir(data_path) if f.startswith('train')]\n",
    "train_dfs = []\n",
    "\n",
    "for file in train_files:\n",
    "    print(f\"Loading training file: {file}\")\n",
    "    df = pd.read_csv(f\"{data_path}{file}\")\n",
    "    train_dfs.append(df)\n",
    "    \n",
    "# Combine training data\n",
    "train_df = pd.concat(train_dfs, ignore_index=True)\n",
    "print(f\"Training data shape: {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_files = [f for f in os.listdir(data_path) if f.startswith('test')]\n",
    "test_dfs = []\n",
    "\n",
    "for file in test_files:\n",
    "    print(f\"Loading test file: {file}\")\n",
    "    df = pd.read_csv(f\"{data_path}{file}\")\n",
    "    test_dfs.append(df)\n",
    "    \n",
    "# Combine test data\n",
    "test_df = pd.concat(test_dfs, ignore_index=True)\n",
    "print(f\"Test data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check basic information of the dataset\n",
    "print(\"Column names in training dataset:\")\n",
    "print(train_df.columns.tolist())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values in training dataset:\")\n",
    "print(train_df.isnull().sum().sum())\n",
    "\n",
    "print(\"\\nMissing values in test dataset:\")\n",
    "print(test_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
    "test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n",
    "\n",
    "# Extract attack labels from test data\n",
    "attack_columns = [col for col in test_df.columns if 'attack' in col.lower()]\n",
    "print(f\"Attack label columns: {attack_columns}\")\n",
    "\n",
    "# Create a combined attack label (if multiple attack columns exist)\n",
    "if len(attack_columns) > 1:\n",
    "    test_df['attack_combined'] = test_df[attack_columns].max(axis=1)\n",
    "    y_test = test_df['attack_combined']\n",
    "else:\n",
    "    y_test = test_df[attack_columns[0]]\n",
    "\n",
    "# Print attack distribution\n",
    "print(f\"\\nAttack distribution in test data:\\n{y_test.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training and testing\n",
    "# Exclude timestamp and attack labels\n",
    "feature_columns = [col for col in train_df.columns if col not in ['timestamp'] + attack_columns]\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "\n",
    "# Prepare training and testing data\n",
    "X_train = train_df[feature_columns].values\n",
    "X_test = test_df[feature_columns].values\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training data shape after preprocessing: {X_train_scaled.shape}\")\n",
    "print(f\"Testing data shape after preprocessing: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of components to retain\n",
    "# We'll use a variance threshold approach\n",
    "\n",
    "# First, fit PCA with all components\n",
    "pca_all = PCA()\n",
    "pca_all.fit(X_train_scaled)\n",
    "\n",
    "# Plot explained variance ratio\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(np.cumsum(pca_all.explained_variance_ratio_), marker='o')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')\n",
    "plt.axhline(y=0.99, color='g', linestyle='--', label='99% Explained Variance')\n",
    "plt.title('Cumulative Explained Variance Ratio')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Find number of components for 95% and 99% variance\n",
    "n_components_95 = np.argmax(np.cumsum(pca_all.explained_variance_ratio_) >= 0.95) + 1\n",
    "n_components_99 = np.argmax(np.cumsum(pca_all.explained_variance_ratio_) >= 0.99) + 1\n",
    "\n",
    "print(f\"Number of components for 95% variance: {n_components_95}\")\n",
    "print(f\"Number of components for 99% variance: {n_components_99}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the number of components for 95% variance\n",
    "n_components = n_components_95\n",
    "\n",
    "# Apply PCA with the selected number of components\n",
    "print(f\"Applying PCA with {n_components} components...\")\n",
    "start_time = time.time()\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "pca_time = time.time() - start_time\n",
    "print(f\"PCA completed in {pca_time:.2f} seconds\")\n",
    "\n",
    "print(f\"PCA transformed training data shape: {X_train_pca.shape}\")\n",
    "print(f\"PCA transformed testing data shape: {X_test_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate Mahalanobis Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the covariance matrix from the PCA-transformed training data\n",
    "print(\"Estimating covariance matrix...\")\n",
    "start_time = time.time()\n",
    "\n",
    "cov = EmpiricalCovariance().fit(X_train_pca)\n",
    "\n",
    "cov_time = time.time() - start_time\n",
    "print(f\"Covariance estimation completed in {cov_time:.2f} seconds\")\n",
    "\n",
    "# Calculate the mean of the PCA-transformed training data\n",
    "mean_vec = np.mean(X_train_pca, axis=0)\n",
    "\n",
    "# Function to calculate Mahalanobis distance\n",
    "def mahalanobis_distance(x, mean, cov):\n",
    "    inv_cov = np.linalg.inv(cov.covariance_)\n",
    "    x_minus_mean = x - mean\n",
    "    left = np.dot(x_minus_mean, inv_cov)\n",
    "    mahal = np.dot(left, x_minus_mean.T)\n",
    "    return np.sqrt(mahal.diagonal())\n",
    "\n",
    "# Calculate Mahalanobis distance for training and test data\n",
    "print(\"Calculating Mahalanobis distances...\")\n",
    "start_time = time.time()\n",
    "\n",
    "train_mahal = mahalanobis_distance(X_train_pca, mean_vec, cov)\n",
    "test_mahal = mahalanobis_distance(X_test_pca, mean_vec, cov)\n",
    "\n",
    "mahal_time = time.time() - start_time\n",
    "print(f\"Mahalanobis distance calculation completed in {mahal_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mahalanobis distance distribution for training data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(train_mahal, bins=50, alpha=0.7, label='Training Data')\n",
    "plt.title('Mahalanobis Distance Distribution (Training Data)')\n",
    "plt.xlabel('Mahalanobis Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mahalanobis distance distribution for test data by class\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(test_mahal[y_test == 0], bins=50, alpha=0.7, label='Normal', color='blue')\n",
    "plt.hist(test_mahal[y_test == 1], bins=50, alpha=0.7, label='Anomaly', color='red')\n",
    "plt.title('Mahalanobis Distance by Class (Test Data)')\n",
    "plt.xlabel('Mahalanobis Distance')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save the model if it doesn't exist\n",
    "model_dir = \"./\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the model components\n",
    "scaler_filename = os.path.join(model_dir, \"scaler.pkl\")\n",
    "pca_filename = os.path.join(model_dir, \"pca_model.pkl\")\n",
    "cov_filename = os.path.join(model_dir, \"covariance_model.pkl\")\n",
    "mean_filename = os.path.join(model_dir, \"mean_vector.pkl\")\n",
    "\n",
    "with open(scaler_filename, 'wb') as file:\n",
    "    pickle.dump(scaler, file)\n",
    "    \n",
    "with open(pca_filename, 'wb') as file:\n",
    "    pickle.dump(pca, file)\n",
    "    \n",
    "with open(cov_filename, 'wb') as file:\n",
    "    pickle.dump(cov, file)\n",
    "    \n",
    "with open(mean_filename, 'wb') as file:\n",
    "    pickle.dump(mean_vec, file)\n",
    "    \n",
    "print(f\"Scaler saved to {scaler_filename}\")\n",
    "print(f\"PCA model saved to {pca_filename}\")\n",
    "print(f\"Covariance model saved to {cov_filename}\")\n",
    "print(f\"Mean vector saved to {mean_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different thresholds for anomaly detection\n",
    "thresholds = np.linspace(min(test_mahal), max(test_mahal), 100)\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = np.where(test_mahal >= threshold, 1, 0)\n",
    "    f1_scores.append(f1_score(y_test, y_pred_threshold))\n",
    "    precision_scores.append(precision_score(y_test, y_pred_threshold))\n",
    "    recall_scores.append(recall_score(y_test, y_pred_threshold))\n",
    "\n",
    "# Find the threshold that maximizes F1 score\n",
    "best_threshold_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_idx]\n",
    "best_f1 = f1_scores[best_threshold_idx]\n",
    "best_precision = precision_scores[best_threshold_idx]\n",
    "best_recall = recall_scores[best_threshold_idx]\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.4f}\")\n",
    "print(f\"Best F1 score: {best_f1:.4f}\")\n",
    "print(f\"Precision at best threshold: {best_precision:.4f}\")\n",
    "print(f\"Recall at best threshold: {best_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1, precision, and recall scores for different thresholds\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(thresholds, f1_scores, 'b-', label='F1 Score')\n",
    "plt.plot(thresholds, precision_scores, 'g-', label='Precision')\n",
    "plt.plot(thresholds, recall_scores, 'r-', label='Recall')\n",
    "plt.axvline(x=best_threshold, color='k', linestyle='--', label=f'Best Threshold: {best_threshold:.4f}')\n",
    "plt.title('Performance Metrics vs. Threshold')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with theoretical threshold based on chi-square distribution\n",
    "# For Mahalanobis distance squared, the threshold can be derived from chi-square distribution\n",
    "# with degrees of freedom equal to the number of components\n",
    "\n",
    "# Calculate theoretical thresholds for different confidence levels\n",
    "confidence_levels = [0.90, 0.95, 0.99, 0.999]\n",
    "theoretical_thresholds = [np.sqrt(chi2.ppf(cl, n_components)) for cl in confidence_levels]\n",
    "\n",
    "print(\"Theoretical thresholds based on chi-square distribution:\")\n",
    "for cl, th in zip(confidence_levels, theoretical_thresholds):\n",
    "    print(f\"Confidence level {cl*100:.1f}%: {th:.4f}\")\n",
    "\n",
    "print(f\"\\nEmpirical best threshold: {best_threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized threshold\n",
    "threshold_filename = os.path.join(model_dir, \"optimized_threshold.pkl\")\n",
    "with open(threshold_filename, 'wb') as file:\n",
    "    pickle.dump(best_threshold, file)\n",
    "    \n",
    "print(f\"Optimized threshold saved to {threshold_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the optimized threshold\n",
    "y_pred_optimized = np.where(test_mahal >= best_threshold, 1, 0)\n",
    "\n",
    "# Evaluate with optimized threshold\n",
    "print(\"Confusion Matrix with Optimized Threshold:\")\n",
    "cm_optimized = confusion_matrix(y_test, y_pred_optimized)\n",
    "print(cm_optimized)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_optimized, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Normal', 'Anomaly'],\n",
    "            yticklabels=['Normal', 'Anomaly'])\n",
    "plt.title('Confusion Matrix with Optimized Threshold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report with Optimized Threshold:\")\n",
    "print(classification_report(y_test, y_pred_optimized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, test_mahal)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test, test_mahal)\n",
    "pr_auc = auc(recall_curve, precision_curve)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(recall_curve, precision_curve, color='blue', lw=2, label=f'PR curve (area = {pr_auc:.4f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyze Anomalies Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with timestamps, actual labels, and predictions\n",
    "results_df = pd.DataFrame({\n",
    "    'timestamp': test_df['timestamp'],\n",
    "    'actual': y_test,\n",
    "    'predicted': y_pred_optimized,\n",
    "    'mahalanobis_distance': test_mahal\n",
    "})\n",
    "\n",
    "# Plot actual vs predicted anomalies over time\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Sample data for better visualization if dataset is large\n",
    "sample_size = min(10000, len(results_df))\n",
    "sample_indices = np.linspace(0, len(results_df)-1, sample_size, dtype=int)\n",
    "sample_df = results_df.iloc[sample_indices]\n",
    "\n",
    "plt.plot(sample_df['timestamp'], sample_df['actual'], 'b-', alpha=0.5, label='Actual')\n",
    "plt.plot(sample_df['timestamp'], sample_df['predicted'], 'r-', alpha=0.5, label='Predicted')\n",
    "plt.title('Actual vs Predicted Anomalies Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Anomaly (1) / Normal (0)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Mahalanobis distance over time with actual labels\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Create a colormap based on actual labels\n",
    "colors = np.where(sample_df['actual'] == 1, 'red', 'blue')\n",
    "\n",
    "plt.scatter(sample_df['timestamp'], sample_df['mahalanobis_distance'], c=colors, alpha=0.5, s=10)\n",
    "plt.title('Mahalanobis Distance Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Mahalanobis Distance')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add a horizontal line at the threshold\n",
    "plt.axhline(y=best_threshold, color='g', linestyle='--')\n",
    "\n",
    "# Add a legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Actual Anomaly'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Normal'),\n",
    "    Line2D([0], [0], color='g', linestyle='--', label='Threshold')\n",
    "]\n",
    "plt.legend(handles=legend_elements)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance based on PCA components\n",
    "# Get the absolute values of the PCA components\n",
    "component_importance = np.abs(pca.components_)\n",
    "\n",
    "# Sum the importance across all components, weighted by explained variance\n",
    "feature_importance = np.sum(component_importance.T * pca.explained_variance_ratio_, axis=1)\n",
    "\n",
    "# Normalize to get relative importance\n",
    "feature_importance = feature_importance / np.sum(feature_importance)\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display the top 20 most important features\n",
    "print(\"Top 20 most important features:\")\n",
    "importance_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plt.figure(figsize=(14, 10))\n",
    "top_n = 20  # Show top 20 features\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(top_n))\n",
    "plt.title(f'Top {top_n} Feature Importance Based on PCA')\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we have:\n",
    "\n",
    "1. Loaded and preprocessed the HAI-22.04 dataset\n",
    "2. Applied Principal Component Analysis (PCA) for dimensionality reduction\n",
    "3. Calculated Mahalanobis distances to detect anomalies\n",
    "4. Optimized the anomaly detection threshold to maximize F1 score\n",
    "5. Evaluated the model's performance using various metrics\n",
    "6. Analyzed feature importance based on PCA components\n",
    "7. Saved the model components and optimized threshold for future use\n",
    "\n",
    "The combination of PCA and Mahalanobis distance has demonstrated its effectiveness for anomaly detection in industrial control system data. This approach has several advantages:\n",
    "\n",
    "- It reduces the dimensionality of the data while preserving most of the variance\n",
    "- It takes into account the correlation between features\n",
    "- It provides a statistical foundation for setting anomaly thresholds\n",
    "- It is computationally efficient for high-dimensional data\n",
    "\n",
    "This method is particularly suitable for industrial control systems where normal operation follows a multivariate Gaussian distribution, and anomalies represent deviations from this distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}