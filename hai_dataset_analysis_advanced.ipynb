{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI-20.07 Dataset Analysis with Advanced Efficient Architectures\n",
    "\n",
    "This notebook analyzes the HAI-20.07 dataset and implements advanced but computationally efficient architectures for attack detection in industrial control systems. The focus is on achieving high performance with low computational resources (\"低消耗高效能\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install required packages\n",
    "    !pip install -q xgboost scikit-learn matplotlib seaborn torch tensorflow tqdm psutil memory_profiler kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Dataset from Kaggle\n",
    "\n",
    "To download the HAI Security Dataset from Kaggle, you need to provide your Kaggle API credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload kaggle.json file\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    import os\n",
    "    import json\n",
    "    \n",
    "    # Create .kaggle directory if it doesn't exist\n",
    "    !mkdir -p ~/.kaggle\n",
    "    \n",
    "    # Upload kaggle.json file\n",
    "    print(\"Please upload your kaggle.json file:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Move the uploaded file to the .kaggle directory\n",
    "    for fn in uploaded.keys():\n",
    "        if fn == 'kaggle.json':\n",
    "            with open(fn, 'r') as f:\n",
    "                # Verify it's a valid JSON file\n",
    "                try:\n",
    "                    json.load(f)\n",
    "                    print(\"Valid kaggle.json file uploaded.\")\n",
    "                except json.JSONDecodeError:\n",
    "                    print(\"Error: Invalid JSON file. Please upload a valid kaggle.json file.\")\n",
    "                    continue\n",
    "            \n",
    "            !cp kaggle.json ~/.kaggle/\n",
    "            !chmod 600 ~/.kaggle/kaggle.json\n",
    "            print(\"Kaggle API credentials configured successfully.\")\n",
    "        else:\n",
    "            print(f\"Warning: Uploaded file '{fn}' is not named 'kaggle.json'. Please upload the correct file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the HAI Security Dataset\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    \n",
    "    # Check if dataset already exists\n",
    "    if not os.path.exists('/content/hai-security-dataset'):\n",
    "        print(\"Downloading HAI Security Dataset from Kaggle...\")\n",
    "        !kaggle datasets download icsdataset/hai-security-dataset -p /content/\n",
    "        \n",
    "        print(\"\\nExtracting dataset...\")\n",
    "        !unzip -q /content/hai-security-dataset.zip -d /content/\n",
    "        print(\"Dataset extracted successfully.\")\n",
    "    else:\n",
    "        print(\"HAI Security Dataset already exists in /content/hai-security-dataset\")\n",
    "    \n",
    "    # List the contents of the dataset directory\n",
    "    print(\"\\nContents of the dataset directory:\")\n",
    "    !ls -la /content/hai-security-dataset/hai-20.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Flatten, Input, Dropout, BatchNormalization\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Check for GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to measure memory usage\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "# Define a class to track model performance and efficiency\n",
    "class ModelEvaluator:\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "        \n",
    "    def add_result(self, model_name, accuracy, precision, recall, f1, training_time, inference_time, memory_usage):\n",
    "        self.results.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1,\n",
    "            'Training Time (s)': training_time,\n",
    "            'Inference Time (ms)': inference_time * 1000,  # Convert to milliseconds\n",
    "            'Memory Usage (MB)': memory_usage\n",
    "        })\n",
    "        \n",
    "    def get_results_df(self):\n",
    "        return pd.DataFrame(self.results)\n",
    "    \n",
    "    def plot_comparison(self):\n",
    "        results_df = self.get_results_df()\n",
    "        \n",
    "        # Plot performance metrics\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Performance metrics\n",
    "        plt.subplot(2, 2, 1)\n",
    "        performance_df = results_df.melt(id_vars=['Model'], \n",
    "                                        value_vars=['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "                                        var_name='Metric', value_name='Value')\n",
    "        sns.barplot(x='Model', y='Value', hue='Metric', data=performance_df)\n",
    "        plt.title('Performance Metrics Comparison')\n",
    "        plt.ylim(0.8, 1.0)  # Adjust as needed\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(loc='lower right')\n",
    "        \n",
    "        # Training time\n",
    "        plt.subplot(2, 2, 2)\n",
    "        sns.barplot(x='Model', y='Training Time (s)', data=results_df)\n",
    "        plt.title('Training Time Comparison (seconds)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yscale('log')  # Log scale for better visualization\n",
    "        \n",
    "        # Inference time\n",
    "        plt.subplot(2, 2, 3)\n",
    "        sns.barplot(x='Model', y='Inference Time (ms)', data=results_df)\n",
    "        plt.title('Inference Time Comparison (milliseconds)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.yscale('log')  # Log scale for better visualization\n",
    "        \n",
    "        # Memory usage\n",
    "        plt.subplot(2, 2, 4)\n",
    "        sns.barplot(x='Model', y='Memory Usage (MB)', data=results_df)\n",
    "        plt.title('Memory Usage Comparison (MB)')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "# Initialize the evaluator\n",
    "evaluator = ModelEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths based on environment\n",
    "if IN_COLAB:\n",
    "    # Colab paths after downloading from Kaggle\n",
    "    data_path = '/content/hai-security-dataset/hai-20.07/'\n",
    "else:\n",
    "    # Local paths\n",
    "    data_path = 'hai-security-dataset/hai-20.07/'\n",
    "\n",
    "# Load training datasets\n",
    "train1 = pd.read_csv(f'{data_path}train1.csv', sep=';')\n",
    "train2 = pd.read_csv(f'{data_path}train2.csv', sep=';')\n",
    "\n",
    "# Load testing datasets\n",
    "test1 = pd.read_csv(f'{data_path}test1.csv', sep=';')\n",
    "test2 = pd.read_csv(f'{data_path}test2.csv', sep=';')\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(\"Training Dataset 1 Shape:\", train1.shape)\n",
    "print(\"Training Dataset 2 Shape:\", train2.shape)\n",
    "print(\"Testing Dataset 1 Shape:\", test1.shape)\n",
    "print(\"Testing Dataset 2 Shape:\", test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time column to datetime\n",
    "train1['time'] = pd.to_datetime(train1['time'])\n",
    "train2['time'] = pd.to_datetime(train2['time'])\n",
    "test1['time'] = pd.to_datetime(test1['time'])\n",
    "test2['time'] = pd.to_datetime(test2['time'])\n",
    "\n",
    "# Identify feature columns (excluding time and attack labels)\n",
    "feature_columns = [col for col in train1.columns if col not in ['time', 'attack', 'attack_P1', 'attack_P2', 'attack_P3']]\n",
    "print(f\"Number of feature columns: {len(feature_columns)}\")\n",
    "print(f\"First few feature columns: {feature_columns[:5]}...\")\n",
    "\n",
    "# Check attack distribution\n",
    "print(\"\\nAttack distribution in train1:\")\n",
    "print(train1['attack'].value_counts(normalize=True) * 100)\n",
    "print(\"\\nAttack distribution in test1:\")\n",
    "print(test1['attack'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training datasets\n",
    "train_combined = pd.concat([train1, train2], ignore_index=True)\n",
    "\n",
    "# Combine testing datasets\n",
    "test_combined = pd.concat([test1, test2], ignore_index=True)\n",
    "\n",
    "# Extract features and target\n",
    "X_train = train_combined[feature_columns]\n",
    "y_train = train_combined['attack']\n",
    "\n",
    "X_test = test_combined[feature_columns]\n",
    "y_test = test_combined['attack']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"X_test_scaled shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to add time-based features\n",
    "def add_time_features(df):\n",
    "    # Make a copy to avoid modifying the original dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Extract time-based features\n",
    "    df_copy['hour'] = df_copy['time'].dt.hour\n",
    "    df_copy['minute'] = df_copy['time'].dt.minute\n",
    "    df_copy['second'] = df_copy['time'].dt.second\n",
    "    df_copy['day_of_week'] = df_copy['time'].dt.dayofweek\n",
    "    \n",
    "    # Add cyclical time features (to capture periodicity)\n",
    "    df_copy['hour_sin'] = np.sin(2 * np.pi * df_copy['hour'] / 24)\n",
    "    df_copy['hour_cos'] = np.cos(2 * np.pi * df_copy['hour'] / 24)\n",
    "    df_copy['minute_sin'] = np.sin(2 * np.pi * df_copy['minute'] / 60)\n",
    "    df_copy['minute_cos'] = np.cos(2 * np.pi * df_copy['minute'] / 60)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Create a function to add rolling window statistics\n",
    "def add_rolling_features(df, window_sizes=[5, 10, 20]):\n",
    "    # Make a copy to avoid modifying the original dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Sort by time to ensure correct rolling window calculation\n",
    "    df_copy = df_copy.sort_values('time')\n",
    "    \n",
    "    # Select features for rolling statistics (exclude time and attack columns)\n",
    "    rolling_features = [col for col in df_copy.columns if col not in ['time', 'attack', 'attack_P1', 'attack_P2', 'attack_P3']]\n",
    "    \n",
    "    # Use only the top 5 most important features for rolling statistics to keep it efficient\n",
    "    top_features = rolling_features[:5]  # In a real scenario, you would select based on feature importance\n",
    "    \n",
    "    # Calculate rolling statistics for different window sizes\n",
    "    for window_size in window_sizes:\n",
    "        for feature in top_features:\n",
    "            df_copy[f'{feature}_rolling_mean_{window_size}'] = df_copy[feature].rolling(window=window_size).mean()\n",
    "            df_copy[f'{feature}_rolling_std_{window_size}'] = df_copy[feature].rolling(window=window_size).std()\n",
    "    \n",
    "    # Drop NaN values created by rolling window\n",
    "    df_copy = df_copy.dropna()\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Create a function to add lag features\n",
    "def add_lag_features(df, lag_steps=[1, 2, 3]):\n",
    "    # Make a copy to avoid modifying the original dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Sort by time to ensure correct lag calculation\n",
    "    df_copy = df_copy.sort_values('time')\n",
    "    \n",
    "    # Select features for lag (exclude time and attack columns)\n",
    "    lag_features = [col for col in df_copy.columns if col not in ['time', 'attack', 'attack_P1', 'attack_P2', 'attack_P3']]\n",
    "    \n",
    "    # Use only the top 5 most important features for lag to keep it efficient\n",
    "    top_features = lag_features[:5]  # In a real scenario, you would select based on feature importance\n",
    "    \n",
    "    # Calculate lag features\n",
    "    for lag in lag_steps:\n",
    "        for feature in top_features:\n",
    "            df_copy[f'{feature}_lag_{lag}'] = df_copy[feature].shift(lag)\n",
    "    \n",
    "    # Drop NaN values created by lag\n",
    "    df_copy = df_copy.dropna()\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all feature engineering steps\n",
    "print(\"Applying feature engineering...\")\n",
    "# Add time features\n",
    "train_features = add_time_features(train_combined)\n",
    "test_features = add_time_features(test_combined)\n",
    "\n",
    "# Add rolling features\n",
    "train_features = add_rolling_features(train_features)\n",
    "test_features = add_rolling_features(test_features)\n",
    "\n",
    "# Add lag features\n",
    "train_features = add_lag_features(train_features)\n",
    "test_features = add_lag_features(test_features)\n",
    "\n",
    "# Extract features and target from the enhanced datasets\n",
    "feature_columns_enhanced = [col for col in train_features.columns \n",
    "                           if col not in ['time', 'attack', 'attack_P1', 'attack_P2', 'attack_P3']]\n",
    "\n",
    "X_train_enhanced = train_features[feature_columns_enhanced]\n",
    "y_train_enhanced = train_features['attack']\n",
    "\n",
    "X_test_enhanced = test_features[feature_columns_enhanced]\n",
    "y_test_enhanced = test_features['attack']\n",
    "\n",
    "# Scale the enhanced features\n",
    "scaler_enhanced = StandardScaler()\n",
    "X_train_enhanced_scaled = scaler_enhanced.fit_transform(X_train_enhanced)\n",
    "X_test_enhanced_scaled = scaler_enhanced.transform(X_test_enhanced)\n",
    "\n",
    "print(\"X_train_enhanced_scaled shape:\", X_train_enhanced_scaled.shape)\n",
    "print(\"X_test_enhanced_scaled shape:\", X_test_enhanced_scaled.shape)\n",
    "print(f\"Number of enhanced features: {X_train_enhanced_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for Sequence Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences for time series models\n",
    "def create_sequences(X, y, time_steps=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Create sequences for training and testing\n",
    "TIME_STEPS = 10\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train.values, TIME_STEPS)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test.values, TIME_STEPS)\n",
    "\n",
    "print(\"X_train_seq shape:\", X_train_seq.shape)\n",
    "print(\"y_train_seq shape:\", y_train_seq.shape)\n",
    "print(\"X_test_seq shape:\", X_test_seq.shape)\n",
    "print(\"y_test_seq shape:\", y_test_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model 1: Lightweight Temporal Convolutional Network (TCN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a lightweight TCN model using PyTorch\n",
    "class LightweightTCN(nn.Module):\n",
    "    def __init__(self, input_size, num_channels=[32, 16], kernel_size=3, dropout=0.2):\n",
    "        super(LightweightTCN, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        # Input layer\n",
    "        layers.append(nn.Conv1d(in_channels=input_size, out_channels=num_channels[0], kernel_size=kernel_size, padding=kernel_size//2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm1d(num_channels[0]))\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for i in range(num_levels - 1):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_channels[i]\n",
    "            out_channels = num_channels[i + 1]\n",
    "            layers.append(nn.Conv1d(in_channels=in_channels, out_channels=out_channels, \n",
    "                                   kernel_size=kernel_size, padding=dilation_size, dilation=dilation_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(out_channels))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.linear = nn.Linear(num_channels[-1], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, time_steps, features]\n",
    "        # Convert to [batch, features, time_steps] for Conv1D\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.network(x)\n",
    "        # Global average pooling\n",
    "        x = torch.mean(x, dim=2)\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the TCN model\n",
    "def train_tcn_model(X_train, y_train, X_test, y_test):\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    \n",
    "    # Create DataLoader for batch processing\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    # Initialize model, loss function, and optimizer\n",
    "    input_size = X_train.shape[2]  # Number of features\n",
    "    model = LightweightTCN(input_size).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Measure memory usage before training\n",
    "    memory_before = get_memory_usage()\n",
    "    \n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_dataset)\n",
    "        if (epoch + 1) % 2 == 0 or epoch == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Measure memory usage after training\n",
    "    memory_after = get_memory_usage()\n",
    "    memory_used = memory_after - memory_before\n",
    "    print(f\"Memory used: {memory_used:.2f} MB\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Measure inference time\n",
    "        inference_start = time.time()\n",
    "        X_test_tensor = X_test_tensor.to(device)\n",
    "        y_pred_proba = model(X_test_tensor).cpu().numpy()\n",
    "        inference_time = (time.time() - inference_start) / len(X_test_tensor)\n",
    "        print(f\"Average inference time per sample: {inference_time*1000:.4f} ms\")\n",
    "        \n",
    "        y_pred = (y_pred_proba > 0.5).astype(int).reshape(-1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = classification_report(y_test, y_pred, output_dict=True)['1']['precision']\n",
    "    recall = classification_report(y_test, y_pred, output_dict=True)['1']['recall']\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Add results to evaluator\n",
    "    evaluator.add_result('Lightweight TCN', accuracy, precision, recall, f1, \n",
    "                        training_time, inference_time, memory_used)\n",
    "    \n",
    "    return model, y_pred, y_pred_proba\n",
    "\n",
    "# Train the TCN model\n",
    "print(\"Training Lightweight TCN model...\")\n",
    "tcn_model, y_pred_tcn, y_prob_tcn = train_tcn_model(X_train_seq, y_train_seq, X_test_seq, y_test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model 2: Optimized LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom attention layer for Keras\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='attention_weight', \n",
    "                                 shape=(input_shape[-1], 1), \n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='attention_bias', \n",
    "                                 shape=(input_shape[1], 1), \n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        # Alignment scores. Shape: (batch_size, seq_len, 1)\n",
    "        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
    "        \n",
    "        # Remove the last dimension. Shape: (batch_size, seq_len)\n",
    "        e = tf.keras.backend.squeeze(e, axis=-1)\n",
    "        \n",
    "        # Compute the weights. Shape: (batch_size, seq_len)\n",
    "        alpha = tf.keras.backend.softmax(e)\n",
    "        \n",
    "        # Shape: (batch_size, seq_len, 1)\n",
    "        alpha_expanded = tf.keras.backend.expand_dims(alpha)\n",
    "        \n",
    "        # Compute the context vector. Shape: (batch_size, features)\n",
    "        context = tf.keras.backend.sum(x * alpha_expanded, axis=1)\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimized LSTM model with attention using TensorFlow/Keras\n",
    "def create_lstm_attention_model(input_shape):\n",
    "    # Input layer\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    # LSTM layer with fewer units for efficiency\n",
    "    lstm_out = LSTM(32, return_sequences=True)(inputs)\n",
    "    \n",
    "    # Apply attention\n",
    "    attention_out = AttentionLayer()(lstm_out)\n",
    "    \n",
    "    # Output layer\n",
    "    x = Dense(16, activation='relu')(attention_out)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LSTM model\n",
    "def train_lstm_model(X_train, y_train, X_test, y_test):\n",
    "    # Measure memory usage before training\n",
    "    memory_before = get_memory_usage()\n",
    "    \n",
    "    # Create and train the model\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])  # (time_steps, features)\n",
    "    model = create_lstm_attention_model(input_shape)\n",
    "    \n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        validation_split=0.1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Measure memory usage after training\n",
    "    memory_after = get_memory_usage()\n",
    "    memory_used = memory_after - memory_before\n",
    "    print(f\"Memory used: {memory_used:.2f} MB\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    # Measure inference time\n",
    "    inference_start = time.time()\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    inference_time = (time.time() - inference_start) / len(X_test)\n",
    "    print(f\"Average inference time per sample: {inference_time*1000:.4f} ms\")\n",
    "    \n",
    "    y_pred = (y_pred_proba > 0.5).astype(int).reshape(-1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = classification_report(y_test, y_pred, output_dict=True)['1']['precision']\n",
    "    recall = classification_report(y_test, y_pred, output_dict=True)['1']['recall']\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Add results to evaluator\n",
    "    evaluator.add_result('LSTM with Attention', accuracy, precision, recall, f1, \n",
    "                        training_time, inference_time, memory_used)\n",
    "    \n",
    "    return model, y_pred, y_pred_proba\n",
    "\n",
    "# Train the LSTM model\n",
    "print(\"Training Optimized LSTM with Attention model...\")\n",
    "lstm_model, y_pred_lstm, y_prob_lstm = train_lstm_model(X_train_seq, y_train_seq, X_test_seq, y_test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model 3: Efficient Ensemble Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an efficient ensemble method\n",
    "def train_efficient_ensemble(X_train, y_train, X_test, y_test):\n",
    "    # Measure memory usage before training\n",
    "    memory_before = get_memory_usage()\n",
    "    \n",
    "    # Train XGBoost model with GPU acceleration\n",
    "    start_time = time.time()\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=50,  # Reduced number of estimators for efficiency\n",
    "        max_depth=5,      # Reduced depth for efficiency\n",
    "        learning_rate=0.1,\n",
    "        random_state=42, \n",
    "        use_label_encoder=False, \n",
    "        eval_metric='logloss',\n",
    "        tree_method='gpu_hist' if torch.cuda.is_available() else 'hist'  # Use GPU if available\n",
    "    )\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Train a small Random Forest model\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=50,  # Reduced number of estimators for efficiency\n",
    "        max_depth=5,      # Reduced depth for efficiency\n",
    "        random_state=42,\n",
    "        n_jobs=-1         # Use all available cores\n",
    "    )\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Train a small Gradient Boosting model\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    gb_model = GradientBoostingClassifier(\n",
    "        n_estimators=50,  # Reduced number of estimators for efficiency\n",
    "        max_depth=3,      # Reduced depth for efficiency\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Measure memory usage after training\n",
    "    memory_after = get_memory_usage()\n",
    "    memory_used = memory_after - memory_before\n",
    "    print(f\"Memory used: {memory_used:.2f} MB\")\n",
    "    \n",
    "    # Make predictions\n",
    "    inference_start = time.time()\n",
    "    y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred_proba_gb = gb_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Weighted average of probabilities (can be optimized with a validation set)\n",
    "    y_pred_proba = 0.5 * y_pred_proba_xgb + 0.3 * y_pred_proba_rf + 0.2 * y_pred_proba_gb\n",
    "    inference_time = (time.time() - inference_start) / len(X_test)\n",
    "    print(f\"Average inference time per sample: {inference_time*1000:.4f} ms\")\n",
    "    \n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = classification_report(y_test, y_pred, output_dict=True)['1']['precision']\n",
    "    recall = classification_report(y_test, y_pred, output_dict=True)['1']['recall']\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Add results to evaluator\n",
    "    evaluator.add_result('Efficient Ensemble', accuracy, precision, recall, f1, \n",
    "                        training_time, inference_time, memory_used)\n",
    "    \n",
    "    return (xgb_model, rf_model, gb_model), y_pred, y_pred_proba\n",
    "\n",
    "# Train the ensemble model\n",
    "print(\"Training Efficient Ensemble model...\")\n",
    "ensemble_models, y_pred_ensemble, y_prob_ensemble = train_efficient_ensemble(X_train_enhanced_scaled, y_train_enhanced, X_test_enhanced_scaled, y_test_enhanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model 4: Feature-based Approach with Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a feature-based model with advanced feature engineering\n",
    "def train_feature_based_model(X_train, y_train, X_test, y_test):\n",
    "    # Measure memory usage before training\n",
    "    memory_before = get_memory_usage()\n",
    "    \n",
    "    # Feature selection to reduce dimensionality\n",
    "    from sklearn.feature_selection import SelectFromModel\n",
    "    \n",
    "    # Use a simple model for feature selection\n",
    "    selector = SelectFromModel(xgb.XGBClassifier(n_estimators=50, random_state=42))\n",
    "    selector.fit(X_train, y_train)\n",
    "    \n",
    "    # Transform the data\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    print(f\"Original features: {X_train.shape[1]}, Selected features: {X_train_selected.shape[1]}\")\n",
    "    \n",
    "    # Train XGBoost model with GPU acceleration\n",
    "    start_time = time.time()\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42, \n",
    "        use_label_encoder=False, \n",
    "        eval_metric='logloss',\n",
    "        tree_method='gpu_hist' if torch.cuda.is_available() else 'hist'  # Use GPU if available\n",
    "    )\n",
    "    xgb_model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Measure memory usage after training\n",
    "    memory_after = get_memory_usage()\n",
    "    memory_used = memory_after - memory_before\n",
    "    print(f\"Memory used: {memory_used:.2f} MB\")\n",
    "    \n",
    "    # Make predictions\n",
    "    inference_start = time.time()\n",
    "    y_pred_proba = xgb_model.predict_proba(X_test_selected)[:, 1]\n",
    "    inference_time = (time.time() - inference_start) / len(X_test_selected)\n",
    "    print(f\"Average inference time per sample: {inference_time*1000:.4f} ms\")\n",
    "    \n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = classification_report(y_test, y_pred, output_dict=True)['1']['precision']\n",
    "    recall = classification_report(y_test, y_pred, output_dict=True)['1']['recall']\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Add results to evaluator\n",
    "    evaluator.add_result('Feature-based XGBoost', accuracy, precision, recall, f1, \n",
    "                        training_time, inference_time, memory_used)\n",
    "    \n",
    "    return (selector, xgb_model), y_pred, y_pred_proba\n",
    "\n",
    "# Train the feature-based model\n",
    "print(\"Training Feature-based model with advanced feature engineering...\")\n",
    "feature_models, y_pred_feature, y_prob_feature = train_feature_based_model(X_train_enhanced_scaled, y_train_enhanced, X_test_enhanced_scaled, y_test_enhanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Model Performance and Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results dataframe\n",
    "results_df = evaluator.get_results_df()\n",
    "print(\"Model Performance and Efficiency Comparison:\")\n",
    "print(results_df)\n",
    "\n",
    "# Plot the comparison\n",
    "evaluator.plot_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best model based on a combination of performance and efficiency\n",
    "def calculate_efficiency_score(row):\n",
    "    # Normalize values (higher is better for accuracy, lower is better for time and memory)\n",
    "    accuracy_norm = row['Accuracy'] / results_df['Accuracy'].max()\n",
    "    training_time_norm = results_df['Training Time (s)'].min() / row['Training Time (s)']\n",
    "    inference_time_norm = results_df['Inference Time (ms)'].min() / row['Inference Time (ms)']\n",
    "    memory_norm = results_df['Memory Usage (MB)'].min() / row['Memory Usage (MB)']\n",
    "    \n",
    "    # Calculate weighted score (adjust weights based on priorities)\n",
    "    score = 0.5 * accuracy_norm + 0.2 * training_time_norm + 0.2 * inference_time_norm + 0.1 * memory_norm\n",
    "    return score\n",
    "\n",
    "# Calculate efficiency score for each model\n",
    "results_df['Efficiency Score'] = results_df.apply(calculate_efficiency_score, axis=1)\n",
    "\n",
    "# Sort by efficiency score\n",
    "results_df_sorted = results_df.sort_values('Efficiency Score', ascending=False)\n",
    "print(\"Models ranked by Efficiency Score (balancing accuracy and computational efficiency):\")\n",
    "print(results_df_sorted[['Model', 'Accuracy', 'F1 Score', 'Training Time (s)', 'Inference Time (ms)', 'Memory Usage (MB)', 'Efficiency Score']])\n",
    "\n",
    "# Get the best model\n",
    "best_model_name = results_df_sorted.iloc[0]['Model']\n",
    "print(f\"\\nBest model based on efficiency score: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "import joblib\n",
    "\n",
    "# Define the path to save the model\n",
    "if IN_COLAB:\n",
    "    model_path = '/content/best_model.joblib'\n",
    "    scaler_path = '/content/scaler.joblib'\n",
    "else:\n",
    "    model_path = 'best_model.joblib'\n",
    "    scaler_path = 'scaler.joblib'\n",
    "\n",
    "# Save the appropriate model based on the best model name\n",
    "if best_model_name == 'Lightweight TCN':\n",
    "    # For PyTorch models, save the state dict\n",
    "    torch.save(tcn_model.state_dict(), model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"Saved TCN model to {model_path}\")\n",
    "elif best_model_name == 'LSTM with Attention':\n",
    "    # For TensorFlow models, save the entire model\n",
    "    lstm_model.save(model_path)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"Saved LSTM model to {model_path}\")\n",
    "elif best_model_name == 'Efficient Ensemble':\n",
    "    # For ensemble models, save all component models\n",
    "    joblib.dump(ensemble_models, model_path)\n",
    "    joblib.dump(scaler_enhanced, scaler_path)\n",
    "    print(f\"Saved Ensemble model to {model_path}\")\n",
    "elif best_model_name == 'Feature-based XGBoost':\n",
    "    # For feature-based models, save the selector and model\n",
    "    joblib.dump(feature_models, model_path)\n",
    "    joblib.dump(scaler_enhanced, scaler_path)\n",
    "    print(f\"Saved Feature-based model to {model_path}\")\n",
    "\n",
    "# If in Colab, download the model files\n",
    "if IN_COLAB:\n",
    "    from google.colab import files\n",
    "    files.download(model_path)\n",
    "    files.download(scaler_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
