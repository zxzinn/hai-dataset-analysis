{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI 20.07 Dataset Analysis\n",
    "\n",
    "This notebook analyzes the HAI 20.07 dataset using various anomaly detection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, roc_curve, auc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "\n",
    "# Set up GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory allocated: {torch.cuda.memory_allocated(0) / 1e9} GB\")\n",
    "    print(f\"Memory cached: {torch.cuda.memory_reserved(0) / 1e9} GB\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define paths\n",
    "data_path = 'hai-security-dataset/hai-20.07/'\n",
    "train_files = ['train1.csv', 'train2.csv']\n",
    "test_files = ['test1.csv', 'test2.csv']\n",
    "\n",
    "# Function to load data\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Convert timestamp to datetime\n",
    "    df.iloc[:, 0] = pd.to_datetime(df.iloc[:, 0])\n",
    "    # Set timestamp as index\n",
    "    df.set_index(df.columns[0], inplace=True)\n",
    "    return df\n",
    "\n",
    "# Load training data\n",
    "train_dfs = []\n",
    "for file in train_files:\n",
    "    df = load_data(os.path.join(data_path, file))\n",
    "    train_dfs.append(df)\n",
    "    print(f\"Loaded {file} with shape {df.shape}\")\n",
    "\n",
    "# Load test data\n",
    "test_dfs = []\n",
    "for file in test_files:\n",
    "    df = load_data(os.path.join(data_path, file))\n",
    "    test_dfs.append(df)\n",
    "    print(f\"Loaded {file} with shape {df.shape}\")\n",
    "\n",
    "# Concatenate training data\n",
    "train_df = pd.concat(train_dfs)\n",
    "print(f\"Combined training data shape: {train_df.shape}\")\n",
    "\n",
    "# Display the first few rows of the training data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for missing values\n",
    "missing_values = train_df.isnull().sum()\n",
    "print(\"Columns with missing values:\")\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Get basic statistics\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify feature and label columns\n",
    "# Assuming the last 4 columns are labels\n",
    "feature_cols = train_df.columns[:-4]\n",
    "label_cols = train_df.columns[-4:]\n",
    "\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Label columns: {label_cols.tolist()}\")\n",
    "\n",
    "# Check label distribution in test data\n",
    "for i, test_df in enumerate(test_dfs):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for j, col in enumerate(label_cols):\n",
    "        plt.subplot(2, 2, j+1)\n",
    "        test_df[col].value_counts().plot(kind='bar')\n",
    "        plt.title(f'Label Distribution - {col}')\n",
    "        plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Label Distribution in test{i+1}.csv')\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize the time series data for a few selected features\n",
    "selected_features = feature_cols[:5]  # Select first 5 features for visualization\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(selected_features):\n",
    "    plt.subplot(len(selected_features), 1, i+1)\n",
    "    plt.plot(train_df.index, train_df[feature])\n",
    "    plt.title(feature)\n",
    "    plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize correlations between features\n",
    "plt.figure(figsize=(20, 16))\n",
    "corr_matrix = train_df[feature_cols].corr()\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, cmap='coolwarm', annot=False, square=True)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to preprocess data\n",
    "def preprocess_data(train_df, test_dfs, feature_cols, label_cols, scaler_type='standard'):\n",
    "    # Extract features and labels\n",
    "    X_train = train_df[feature_cols].values\n",
    "    \n",
    "    X_tests = []\n",
    "    y_tests = []\n",
    "    for test_df in test_dfs:\n",
    "        X_test = test_df[feature_cols].values\n",
    "        y_test = test_df[label_cols].values\n",
    "        X_tests.append(X_test)\n",
    "        y_tests.append(y_test)\n",
    "    \n",
    "    # Scale the features\n",
    "    if scaler_type == 'standard':\n",
    "        scaler = StandardScaler()\n",
    "    else:  # minmax\n",
    "        scaler = MinMaxScaler()\n",
    "    \n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_tests_scaled = [scaler.transform(X_test) for X_test in X_tests]\n",
    "    \n",
    "    return X_train_scaled, X_tests_scaled, y_tests, scaler\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_scaled, X_tests_scaled, y_tests, scaler = preprocess_data(train_df, test_dfs, feature_cols, label_cols)\n",
    "\n",
    "print(f\"Training data shape: {X_train_scaled.shape}\")\n",
    "for i, X_test_scaled in enumerate(X_tests_scaled):\n",
    "    print(f\"Test{i+1} data shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to create sequences for time series models\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:(i + seq_length)]\n",
    "        xs.append(x)\n",
    "    return np.array(xs)\n",
    "\n",
    "# Create sequences for training and testing\n",
    "seq_length = 10  # Sequence length for time series models\n",
    "\n",
    "X_train_seq = create_sequences(X_train_scaled, seq_length)\n",
    "X_tests_seq = [create_sequences(X_test_scaled, seq_length) for X_test_scaled in X_tests_scaled]\n",
    "y_tests_seq = [y_test[seq_length:] for y_test in y_tests]\n",
    "\n",
    "print(f\"Training sequences shape: {X_train_seq.shape}\")\n",
    "for i, X_test_seq in enumerate(X_tests_seq):\n",
    "    print(f\"Test{i+1} sequences shape: {X_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_seq).to(device)\n",
    "X_tests_tensor = [torch.FloatTensor(X_test_seq).to(device) for X_test_seq in X_tests_seq]\n",
    "\n",
    "# Create DataLoader for training\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Training tensor shape: {X_train_tensor.shape}\")\n",
    "for i, X_test_tensor in enumerate(X_tests_tensor):\n",
    "    print(f\"Test{i+1} tensor shape: {X_test_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation\n",
    "\n",
    "### 3.1 Traditional Machine Learning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Isolation Forest\n",
    "def train_isolation_forest(X_train, contamination=0.01):\n",
    "    print(\"Training Isolation Forest...\")\n",
    "    start_time = time.time()\n",
    "    model = IsolationForest(n_estimators=100, contamination=contamination, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    return model\n",
    "\n",
    "# One-Class SVM\n",
    "def train_one_class_svm(X_train, nu=0.01):\n",
    "    print(\"Training One-Class SVM...\")\n",
    "    start_time = time.time()\n",
    "    model = OneClassSVM(kernel='rbf', gamma='auto', nu=nu)\n",
    "    model.fit(X_train)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    return model\n",
    "\n",
    "# PCA-based anomaly detection\n",
    "def train_pca(X_train, n_components=0.95):\n",
    "    print(\"Training PCA...\")\n",
    "    start_time = time.time()\n",
    "    model = PCA(n_components=n_components, random_state=42)\n",
    "    model.fit(X_train)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    return model\n",
    "\n",
    "# Function to detect anomalies using PCA reconstruction error\n",
    "def detect_anomalies_pca(model, X, threshold_factor=3):\n",
    "    X_transformed = model.transform(X)\n",
    "    X_reconstructed = model.inverse_transform(X_transformed)\n",
    "    reconstruction_error = np.mean(np.square(X - X_reconstructed), axis=1)\n",
    "    threshold = np.mean(reconstruction_error) + threshold_factor * np.std(reconstruction_error)\n",
    "    return reconstruction_error > threshold, reconstruction_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Deep Learning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# LSTM Autoencoder\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1, dropout=0.2):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=input_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        _, (hidden, _) = self.encoder(x)\n",
    "        # Use the last hidden state as the encoded representation\n",
    "        hidden_repeat = hidden[-1].unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        # Decode\n",
    "        output, _ = self.decoder(hidden_repeat)\n",
    "        return output\n",
    "\n",
    "# CNN Autoencoder\n",
    "class CNNAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, seq_length):\n",
    "        super(CNNAutoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(16, 32, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(32, input_dim, kernel_size=2, stride=2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch, seq_length, input_dim]\n",
    "        # Reshape for Conv1d: [batch, input_dim, seq_length]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # Encode\n",
    "        encoded = self.encoder(x)\n",
    "        # Decode\n",
    "        decoded = self.decoder(encoded)\n",
    "        # Reshape back: [batch, seq_length, input_dim]\n",
    "        decoded = decoded.permute(0, 2, 1)\n",
    "        return decoded\n",
    "\n",
    "# Transformer Autoencoder\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class TransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, nhead=4, num_layers=2, dropout=0.1):\n",
    "        super(TransformerAutoencoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoder = PositionalEncoding(hidden_dim)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=nhead, dim_feedforward=hidden_dim*4, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(hidden_dim, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch, seq_length, input_dim]\n",
    "        # Reshape for Transformer: [seq_length, batch, input_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        # Project input to hidden dimension\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Project back to input dimension\n",
    "        x = self.output_projection(x)\n",
    "        \n",
    "        # Reshape back: [batch, seq_length, input_dim]\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to train deep learning models\n",
    "def train_dl_model(model, train_loader, num_epochs=50, learning_rate=0.001):\n",
    "    print(f\"Training {model.__class__.__name__}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch_idx, (data,) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses)\n",
    "    plt.title(f'{model.__class__.__name__} Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return model, train_losses\n",
    "\n",
    "# Function to detect anomalies using reconstruction error\n",
    "def detect_anomalies_dl(model, X, threshold_factor=3):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X).to(device)\n",
    "        X_reconstructed = model(X_tensor).cpu().numpy()\n",
    "    \n",
    "    reconstruction_error = np.mean(np.square(X - X_reconstructed), axis=(1, 2))\n",
    "    threshold = np.mean(reconstruction_error) + threshold_factor * np.std(reconstruction_error)\n",
    "    return reconstruction_error > threshold, reconstruction_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train traditional ML models\n",
    "# Use non-sequential data for these models\n",
    "isolation_forest = train_isolation_forest(X_train_scaled)\n",
    "one_class_svm = train_one_class_svm(X_train_scaled)\n",
    "pca_model = train_pca(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train deep learning models\n",
    "input_dim = X_train_seq.shape[2]  # Number of features\n",
    "seq_length = X_train_seq.shape[1]  # Sequence length\n",
    "hidden_dim = 64  # Hidden dimension\n",
    "\n",
    "# Initialize models\n",
    "lstm_autoencoder = LSTMAutoencoder(input_dim, hidden_dim, num_layers=2).to(device)\n",
    "cnn_autoencoder = CNNAutoencoder(input_dim, seq_length).to(device)\n",
    "transformer_autoencoder = TransformerAutoencoder(input_dim, hidden_dim).to(device)\n",
    "\n",
    "# Train models\n",
    "lstm_autoencoder, lstm_losses = train_dl_model(lstm_autoencoder, train_loader, num_epochs=50)\n",
    "cnn_autoencoder, cnn_losses = train_dl_model(cnn_autoencoder, train_loader, num_epochs=50)\n",
    "transformer_autoencoder, transformer_losses = train_dl_model(transformer_autoencoder, train_loader, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(model_name, y_true, y_pred):\n",
    "    # Convert to binary (0 for normal, 1 for anomaly)\n",
    "    y_true_binary = (y_true[:, 0] > 0).astype(int)\n",
    "    y_pred_binary = y_pred.astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true_binary, y_pred_binary, average='binary')\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true_binary, y_pred_binary)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"{model_name} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate traditional ML models\n",
    "results = []\n",
    "\n",
    "for test_idx, (X_test_scaled, y_test) in enumerate(zip(X_tests_scaled, y_tests)):\n",
    "    print(f\"\\nEvaluating on Test{test_idx+1}:\")\n",
    "    \n",
    "    # Isolation Forest\n",
    "    y_pred_if = isolation_forest.predict(X_test_scaled)\n",
    "    # Convert from -1/1 to 0/1 (1 for anomaly)\n",
    "    y_pred_if = (y_pred_if == -1).astype(int)\n",
    "    precision_if, recall_if, f1_if = evaluate_model(\"Isolation Forest\", y_test, y_pred_if)\n",
    "    \n",
    "    # One-Class SVM\n",
    "    y_pred_svm = one_class_svm.predict(X_test_scaled)\n",
    "    # Convert from -1/1 to 0/1 (1 for anomaly)\n",
    "    y_pred_svm = (y_pred_svm == -1).astype(int)\n",
    "    precision_svm, recall_svm, f1_svm = evaluate_model(\"One-Class SVM\", y_test, y_pred_svm)\n",
    "    \n",
    "    # PCA\n",
    "    y_pred_pca, _ = detect_anomalies_pca(pca_model, X_test_scaled)\n",
    "    precision_pca, recall_pca, f1_pca = evaluate_model(\"PCA\", y_test, y_pred_pca)\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'test_idx': test_idx + 1,\n",
    "        'isolation_forest': {'precision': precision_if, 'recall': recall_if, 'f1': f1_if},\n",
    "        'one_class_svm': {'precision': precision_svm, 'recall': recall_svm, 'f1': f1_svm},\n",
    "        'pca': {'precision': precision_pca, 'recall': recall_pca, 'f1': f1_pca}\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate deep learning models\n",
    "for test_idx, (X_test_seq, y_test_seq) in enumerate(zip(X_tests_seq, y_tests_seq)):\n",
    "    print(f\"\\nEvaluating deep learning models on Test{test_idx+1}:\")\n",
    "    \n",
    "    # LSTM Autoencoder\n",
    "    y_pred_lstm, _ = detect_anomalies_dl(lstm_autoencoder, X_test_seq)\n",
    "    precision_lstm, recall_lstm, f1_lstm = evaluate_model(\"LSTM Autoencoder\", y_test_seq, y_pred_lstm)\n",
    "    \n",
    "    # CNN Autoencoder\n",
    "    y_pred_cnn, _ = detect_anomalies_dl(cnn_autoencoder, X_test_seq)\n",
    "    precision_cnn, recall_cnn, f1_cnn = evaluate_model(\"CNN Autoencoder\", y_test_seq, y_pred_cnn)\n",
    "    \n",
    "    # Transformer Autoencoder\n",
    "    y_pred_transformer, _ = detect_anomalies_dl(transformer_autoencoder, X_test_seq)\n",
    "    precision_transformer, recall_transformer, f1_transformer = evaluate_model(\"Transformer Autoencoder\", y_test_seq, y_pred_transformer)\n",
    "    \n",
    "    # Update results\n",
    "    results[test_idx].update({\n",
    "        'lstm_autoencoder': {'precision': precision_lstm, 'recall': recall_lstm, 'f1': f1_lstm},\n",
    "        'cnn_autoencoder': {'precision': precision_cnn, 'recall': recall_cnn, 'f1': f1_cnn},\n",
    "        'transformer_autoencoder': {'precision': precision_transformer, 'recall': recall_transformer, 'f1': f1_transformer}\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Visualization and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize F1 scores for all models\n",
    "model_names = ['Isolation Forest', 'One-Class SVM', 'PCA', 'LSTM Autoencoder', 'CNN Autoencoder', 'Transformer Autoencoder']\n",
    "model_keys = ['isolation_forest', 'one_class_svm', 'pca', 'lstm_autoencoder', 'cnn_autoencoder', 'transformer_autoencoder']\n",
    "\n",
    "for test_idx, result in enumerate(results):\n",
    "    f1_scores = [result[key]['f1'] for key in model_keys]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(model_names, f1_scores)\n",
    "    plt.title(f'F1 Scores on Test{test_idx+1}')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom', rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize precision-recall trade-off\n",
    "for test_idx, result in enumerate(results):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for i, key in enumerate(model_keys):\n",
    "        plt.scatter(result[key]['recall'], result[key]['precision'], s=100, label=model_names[i])\n",
    "    \n",
    "    plt.title(f'Precision-Recall Trade-off on Test{test_idx+1}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize anomaly detection over time for the best performing model\n",
    "# Find the best model based on average F1 score\n",
    "avg_f1_scores = {}\n",
    "for key in model_keys:\n",
    "    avg_f1_scores[key] = np.mean([result[key]['f1'] for result in results])\n",
    "    \n",
    "best_model_key = max(avg_f1_scores, key=avg_f1_scores.get)\n",
    "best_model_name = model_names[model_keys.index(best_model_key)]\n",
    "print(f\"Best performing model: {best_model_name} with average F1 score: {avg_f1_scores[best_model_key]:.4f}\")\n",
    "\n",
    "# Visualize anomaly detection over time for the best model\n",
    "for test_idx, (X_test_scaled, y_test) in enumerate(zip(X_tests_scaled, y_tests)):\n",
    "    # Get predictions from the best model\n",
    "    if best_model_key == 'isolation_forest':\n",
    "        y_pred = (isolation_forest.predict(X_test_scaled) == -1).astype(int)\n",
    "    elif best_model_key == 'one_class_svm':\n",
    "        y_pred = (one_class_svm.predict(X_test_scaled) == -1).astype(int)\n",
    "    elif best_model_key == 'pca':\n",
    "        y_pred, _ = detect_anomalies_pca(pca_model, X_test_scaled)\n",
    "    else:\n",
    "        # For deep learning models, use the sequence data\n",
    "        X_test_seq = X_tests_seq[test_idx]\n",
    "        y_test = y_tests_seq[test_idx]\n",
    "        if best_model_key == 'lstm_autoencoder':\n",
    "            y_pred, _ = detect_anomalies_dl(lstm_autoencoder, X_test_seq)\n",
    "        elif best_model_key == 'cnn_autoencoder':\n",
    "            y_pred, _ = detect_anomalies_dl(cnn_autoencoder, X_test_seq)\n",
    "        elif best_model_key == 'transformer_autoencoder':\n",
    "            y_pred, _ = detect_anomalies_dl(transformer_autoencoder, X_test_seq)\n",
    "    \n",
    "    # Convert true labels to binary\n",
    "    y_true_binary = (y_test[:, 0] > 0).astype(int)\n",
    "    \n",
    "    # Plot true vs predicted anomalies over time\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(y_true_binary, label='True Anomalies', color='blue', alpha=0.7)\n",
    "    plt.plot(y_pred, label='Predicted Anomalies', color='red', alpha=0.7)\n",
    "    plt.title(f'{best_model_name} - Anomaly Detection Over Time (Test{test_idx+1})')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Anomaly (1) / Normal (0)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Ensemble method: Majority voting\n",
    "def ensemble_majority_voting(predictions):\n",
    "    # Sum predictions and check if majority voted for anomaly\n",
    "    return np.sum(predictions, axis=0) > (len(predictions) / 2)\n",
    "\n",
    "# Evaluate ensemble method\n",
    "for test_idx, (X_test_scaled, X_test_seq, y_test, y_test_seq) in enumerate(zip(X_tests_scaled, X_tests_seq, y_tests, y_tests_seq)):\n",
    "    print(f\"\\nEvaluating ensemble method on Test{test_idx+1}:\")\n",
    "    \n",
    "    # Get predictions from all models\n",
    "    y_pred_if = (isolation_forest.predict(X_test_scaled) == -1).astype(int)\n",
    "    y_pred_svm = (one_class_svm.predict(X_test_scaled) == -1).astype(int)\n",
    "    y_pred_pca, _ = detect_anomalies_pca(pca_model, X_test_scaled)\n",
    "    \n",
    "    # For deep learning models, use the sequence data\n",
    "    y_pred_lstm, _ = detect_anomalies_dl(lstm_autoencoder, X_test_seq)\n",
    "    y_pred_cnn, _ = detect_anomalies_dl(cnn_autoencoder, X_test_seq)\n",
    "    y_pred_transformer, _ = detect_anomalies_dl(transformer_autoencoder, X_test_seq)\n",
    "    \n",
    "    # Ensure all predictions have the same length\n",
    "    min_length = min(len(y_pred_if), len(y_pred_lstm))\n",
    "    y_pred_if = y_pred_if[:min_length]\n",
    "    y_pred_svm = y_pred_svm[:min_length]\n",
    "    y_pred_pca = y_pred_pca[:min_length]\n",
    "    y_pred_lstm = y_pred_lstm[:min_length]\n",
    "    y_pred_cnn = y_pred_cnn[:min_length]\n",
    "    y_pred_transformer = y_pred_transformer[:min_length]\n",
    "    y_true = y_test[:min_length] if len(y_test) > len(y_test_seq) else y_test_seq[:min_length]\n",
    "    \n",
    "    # Ensemble predictions using majority voting\n",
    "    predictions = [y_pred_if, y_pred_svm, y_pred_pca, y_pred_lstm, y_pred_cnn, y_pred_transformer]\n",
    "    y_pred_ensemble = ensemble_majority_voting(predictions)\n",
    "    \n",
    "    # Evaluate ensemble method\n",
    "    precision_ensemble, recall_ensemble, f1_ensemble = evaluate_model(\"Ensemble (Majority Voting)\", y_true, y_pred_ensemble)\n",
    "    \n",
    "    # Update results\n",
    "    results[test_idx]['ensemble'] = {'precision': precision_ensemble, 'recall': recall_ensemble, 'f1': f1_ensemble}\n",
    "    \n",
    "    # Compare ensemble with individual models\n",
    "    model_names_with_ensemble = model_names + ['Ensemble (Majority Voting)']\n",
    "    model_keys_with_ensemble = model_keys + ['ensemble']\n",
    "    \n",
    "    f1_scores = [results[test_idx][key]['f1'] for key in model_keys_with_ensemble]\n",
    "    \n",
    "    plt.figure(figsize=(14, 6))\n",
    "    bars = plt.bar(model_names_with_ensemble, f1_scores)\n",
    "    plt.title(f'F1 Scores with Ensemble on Test{test_idx+1}')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{height:.3f}', ha='center', va='bottom', rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}