{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI-20.07 Dataset Analysis: Optimized LightGBM Model\n",
    "\n",
    "This notebook implements an optimized LightGBM model for attack detection in industrial control systems using the HAI-20.07 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import psutil\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed tabular data\n",
    "with open('preprocessed_data/tabular_data.pkl', 'rb') as f:\n",
    "    tabular_data = pickle.load(f)\n",
    "\n",
    "X_train_enhanced_scaled = tabular_data['X_train_enhanced_scaled']\n",
    "y_train_enhanced = tabular_data['y_train_enhanced']\n",
    "X_test_enhanced_scaled = tabular_data['X_test_enhanced_scaled']\n",
    "y_test_enhanced = tabular_data['y_test_enhanced']\n",
    "X_train_enhanced_balanced = tabular_data['X_train_enhanced_balanced']\n",
    "y_train_enhanced_balanced = tabular_data['y_train_enhanced_balanced']\n",
    "feature_columns_enhanced = tabular_data['feature_columns_enhanced']\n",
    "\n",
    "print(\"X_train_enhanced_balanced shape:\", X_train_enhanced_balanced.shape)\n",
    "print(\"y_train_enhanced_balanced shape:\", y_train_enhanced_balanced.shape)\n",
    "print(\"X_test_enhanced_scaled shape:\", X_test_enhanced_scaled.shape)\n",
    "print(\"y_test_enhanced shape:\", y_test_enhanced.shape)\n",
    "print(\"Number of features:\", len(feature_columns_enhanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to measure memory usage accurately\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    # Force garbage collection before measuring memory\n",
    "    gc.collect()\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    return memory_info.rss / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "# Function to calculate model size in MB\n",
    "def get_lgb_model_size(model):\n",
    "    with open('temp_model.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    size_bytes = os.path.getsize('temp_model.pkl')\n",
    "    os.remove('temp_model.pkl')\n",
    "    return size_bytes / (1024 * 1024)  # Convert to MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Optimization with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameters to optimize\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 15, 63),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 0.0, 1.0),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 0.0, 1.0),\n",
    "        'verbose': -1,\n",
    "        'random_state': RANDOM_SEED\n",
    "    }\n",
    "    \n",
    "    # Create dataset for LightGBM\n",
    "    train_data = lgb.Dataset(X_train_enhanced_balanced, label=y_train_enhanced_balanced)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = lgb.cv(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=100,\n",
    "        nfold=5,\n",
    "        stratified=True,\n",
    "        early_stopping_rounds=20,\n",
    "        seed=RANDOM_SEED,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Return the best AUC score\n",
    "    return cv_results['auc-mean'][-1]\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "def optimize_hyperparameters(n_trials=50):\n",
    "    print(\"Optimizing LightGBM hyperparameters...\")\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    \n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"  Value: {trial.value:.4f}\")\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "    \n",
    "    return trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter optimization with a small number of trials for demonstration\n",
    "best_params = optimize_hyperparameters(n_trials=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and Evaluate LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an optimized LightGBM model\n",
    "def train_lightgbm_model(X_train, y_train, X_test, y_test, params=None):\n",
    "    # Measure memory usage before training\n",
    "    memory_before = get_memory_usage()\n",
    "    \n",
    "    # Define LightGBM parameters if not provided\n",
    "    if params is None:\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': -1,\n",
    "            'num_threads': 4,\n",
    "            'scale_pos_weight': sum(y_train == 0) / sum(y_train == 1)  # Handle class imbalance\n",
    "        }\n",
    "    \n",
    "    # Train model\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create dataset for LightGBM\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    \n",
    "    # Train with early stopping\n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[train_data],\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(stopping_rounds=50, verbose=True),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Measure memory usage after training\n",
    "    memory_after = get_memory_usage()\n",
    "    memory_used = memory_after - memory_before\n",
    "    print(f\"Memory used: {memory_used:.2f} MB\")\n",
    "    \n",
    "    # Calculate model size\n",
    "    model_size = get_lgb_model_size(model)\n",
    "    print(f\"Model size: {model_size:.2f} MB\")\n",
    "    \n",
    "    # Make predictions\n",
    "    inference_start = time.time()\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    inference_time = (time.time() - inference_start) / len(X_test)\n",
    "    print(f\"Average inference time per sample: {inference_time*1000:.4f} ms\")\n",
    "    \n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc_score:.4f}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix - Optimized LightGBM')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'model_name': 'Optimized LightGBM',\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc_score,\n",
    "        'training_time': training_time,\n",
    "        'inference_time': inference_time,\n",
    "        'memory_used': memory_used,\n",
    "        'model_size': model_size,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba,\n",
    "        'params': params\n",
    "    }\n",
    "    \n",
    "    # Create directory for results if it doesn't exist\n",
    "    if not os.path.exists('model_results'):\n",
    "        os.makedirs('model_results')\n",
    "    \n",
    "    # Save results\n",
    "    with open('model_results/lightgbm_results.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    # Save model\n",
    "    model.save_model('model_results/lightgbm_model.txt')\n",
    "    \n",
    "    return model, y_pred, y_pred_proba, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LightGBM model with optimized hyperparameters\n",
    "print(\"Training Optimized LightGBM model...\")\n",
    "lgbm_model, y_pred_lgbm, y_prob_lgbm, lgbm_results = train_lightgbm_model(\n",
    "    X_train_enhanced_balanced, y_train_enhanced_balanced, \n",
    "    X_test_enhanced_scaled, y_test_enhanced,\n",
    "    params=best_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = lgbm_model.feature_importance(importance_type='gain')\n",
    "\n",
    "# Create a DataFrame for feature importance\n",
    "feature_names = [f'Feature_{i}' for i in range(len(feature_importance))]\n",
    "if len(feature_columns_enhanced) == len(feature_importance):\n",
    "    feature_names = feature_columns_enhanced\n",
    "    \n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Display top 20 features\n",
    "print(\"Top 20 most important features:\")\n",
    "print(importance_df.head(20))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "plt.title('Top 20 Feature Importance (Gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test_enhanced, y_prob_lgbm)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Optimized LightGBM')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot precision-recall curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test_enhanced, y_prob_lgbm)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (area = {pr_auc:.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve - Optimized LightGBM')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize threshold for better F1 score\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_prob_lgbm > threshold).astype(int)\n",
    "    f1 = f1_score(y_test_enhanced, y_pred_threshold)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Find the best threshold\n",
    "best_threshold_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_idx]\n",
    "best_f1 = f1_scores[best_threshold_idx]\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.2f} with F1 score: {best_f1:.4f}\")\n",
    "\n",
    "# Plot F1 scores for different thresholds\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, f1_scores, marker='o')\n",
    "plt.axvline(x=best_threshold, color='r', linestyle='--', label=f'Best threshold: {best_threshold:.2f}')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Recalculate metrics with the optimized threshold\n",
    "y_pred_optimized = (y_prob_lgbm > best_threshold).astype(int)\n",
    "accuracy_optimized = accuracy_score(y_test_enhanced, y_pred_optimized)\n",
    "precision_optimized = precision_score(y_test_enhanced, y_pred_optimized, zero_division=0)\n",
    "recall_optimized = recall_score(y_test_enhanced, y_pred_optimized, zero_division=0)\n",
    "f1_optimized = f1_score(y_test_enhanced, y_pred_optimized, zero_division=0)\n",
    "\n",
    "print(f\"Optimized Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_optimized:.4f}\")\n",
    "print(f\"Precision: {precision_optimized:.4f}\")\n",
    "print(f\"Recall: {recall_optimized:.4f}\")\n",
    "print(f\"F1 Score: {f1_optimized:.4f}\")\n",
    "\n",
    "# Plot confusion matrix with optimized threshold\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm_optimized = confusion_matrix(y_test_enhanced, y_pred_optimized)\n",
    "sns.heatmap(cm_optimized, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Optimized LightGBM (Optimized Threshold)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}