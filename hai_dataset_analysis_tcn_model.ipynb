{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI-20.07 Dataset Analysis: Optimized TCN Model\n",
    "\n",
    "This notebook implements an optimized Temporal Convolutional Network (TCN) model for attack detection in industrial control systems using the HAI-20.07 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import psutil\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Check for GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device name: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed sequence data\n",
    "with open('preprocessed_data/sequence_data.pkl', 'rb') as f:\n",
    "    sequence_data = pickle.load(f)\n",
    "\n",
    "X_train_seq = sequence_data['X_train_seq']\n",
    "y_train_seq = sequence_data['y_train_seq']\n",
    "X_test_seq = sequence_data['X_test_seq']\n",
    "y_test_seq = sequence_data['y_test_seq']\n",
    "X_train_seq_balanced = sequence_data['X_train_seq_balanced']\n",
    "y_train_seq_balanced = sequence_data['y_train_seq_balanced']\n",
    "TIME_STEPS = sequence_data['TIME_STEPS']\n",
    "STRIDE = sequence_data['STRIDE']\n",
    "\n",
    "print(\"X_train_seq_balanced shape:\", X_train_seq_balanced.shape)\n",
    "print(\"y_train_seq_balanced shape:\", y_train_seq_balanced.shape)\n",
    "print(\"X_test_seq shape:\", X_test_seq.shape)\n",
    "print(\"y_test_seq shape:\", y_test_seq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to measure memory usage accurately\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    # Force garbage collection before measuring memory\n",
    "    gc.collect()\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    return memory_info.rss / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "# Function to calculate model size in MB\n",
    "def get_model_size(model):\n",
    "    torch.save(model.state_dict(), \"temp_model.pt\")\n",
    "    size_bytes = os.path.getsize(\"temp_model.pt\")\n",
    "    os.remove(\"temp_model.pt\")\n",
    "    return size_bytes / (1024 * 1024)  # Convert to MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Optimized TCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an optimized lightweight TCN model using PyTorch\n",
    "class OptimizedTCN(nn.Module):\n",
    "    def __init__(self, input_size, num_channels=[16, 8], kernel_size=3, dropout=0.1):\n",
    "        super(OptimizedTCN, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        \n",
    "        # Input layer with fewer channels\n",
    "        layers.append(nn.Conv1d(in_channels=input_size, out_channels=num_channels[0], kernel_size=kernel_size, padding=kernel_size//2))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.BatchNorm1d(num_channels[0]))\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        # Hidden layers with dilated convolutions\n",
    "        for i in range(num_levels - 1):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_channels[i]\n",
    "            out_channels = num_channels[i + 1]\n",
    "            layers.append(nn.Conv1d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                   kernel_size=kernel_size, padding=dilation_size, dilation=dilation_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(out_channels))\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self.linear = nn.Linear(num_channels[-1], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, time_steps, features]\n",
    "        # Convert to [batch, features, time_steps] for Conv1D\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.network(x)\n",
    "        # Global average pooling\n",
    "        x = torch.mean(x, dim=2)\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and Evaluate TCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the optimized TCN model\n",
    "def train_tcn_model(X_train, y_train, X_test, y_test):\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1)\n",
    "    X_test_tensor = torch.FloatTensor(X_test)\n",
    "    \n",
    "    # Create weighted sampler for imbalanced data\n",
    "    class_counts = np.bincount(y_train.astype(int))\n",
    "    class_weights = 1. / torch.tensor(class_counts, dtype=torch.float)\n",
    "    sample_weights = class_weights[torch.tensor(y_train.astype(int))]\n",
    "    sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "    \n",
    "    # Create DataLoader for batch processing\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, sampler=sampler)\n",
    "    \n",
    "    # Initialize model, loss function, and optimizer\n",
    "    input_size = X_train.shape[2]  # Number of features\n",
    "    model = OptimizedTCN(input_size).to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # Added weight decay for regularization\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "    # Measure memory usage before training\n",
    "    memory_before = get_memory_usage()\n",
    "    \n",
    "    # Train the model\n",
    "    start_time = time.time()\n",
    "    num_epochs = 10\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Create a small validation set\n",
    "    val_size = int(0.1 * len(X_train_tensor))\n",
    "    train_indices = list(range(len(X_train_tensor) - val_size))\n",
    "    val_indices = list(range(len(X_train_tensor) - val_size, len(X_train_tensor)))\n",
    "    \n",
    "    X_val_tensor = X_train_tensor[val_indices].to(device)\n",
    "    y_val_tensor = y_train_tensor[val_indices].to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Validate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor)\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        if (epoch + 1) % 2 == 0 or epoch == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training time: {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Measure memory usage after training\n",
    "    memory_after = get_memory_usage()\n",
    "    memory_used = memory_after - memory_before\n",
    "    print(f\"Memory used: {memory_used:.2f} MB\")\n",
    "    \n",
    "    # Calculate model size\n",
    "    model_size = get_model_size(model)\n",
    "    print(f\"Model size: {model_size:.2f} MB\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Measure inference time\n",
    "        inference_start = time.time()\n",
    "        X_test_tensor = X_test_tensor.to(device)\n",
    "        y_pred_proba = model(X_test_tensor).cpu().numpy()\n",
    "        inference_time = (time.time() - inference_start) / len(X_test_tensor)\n",
    "        print(f\"Average inference time per sample: {inference_time*1000:.4f} ms\")\n",
    "        \n",
    "        y_pred = (y_pred_proba > 0.5).astype(int).reshape(-1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba.reshape(-1))\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc_score:.4f}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix - Optimized TCN')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'model_name': 'Optimized TCN',\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc_score,\n",
    "        'training_time': training_time,\n",
    "        'inference_time': inference_time,\n",
    "        'memory_used': memory_used,\n",
    "        'model_size': model_size,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba.reshape(-1)\n",
    "    }\n",
    "    \n",
    "    # Create directory for results if it doesn't exist\n",
    "    if not os.path.exists('model_results'):\n",
    "        os.makedirs('model_results')\n",
    "    \n",
    "    # Save results\n",
    "    with open('model_results/tcn_results.pkl', 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'model_results/tcn_model.pt')\n",
    "    \n",
    "    return model, y_pred, y_pred_proba.reshape(-1), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the TCN model\n",
    "print(\"Training Optimized TCN model...\")\n",
    "tcn_model, y_pred_tcn, y_prob_tcn, tcn_results = train_tcn_model(X_train_seq_balanced, y_train_seq_balanced, X_test_seq, y_test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance using permutation importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Define a function to get predictions from the PyTorch model\n",
    "def get_predictions(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X).to(device)\n",
    "        y_pred = model(X_tensor).cpu().numpy().reshape(-1)\n",
    "    return y_pred\n",
    "\n",
    "# Calculate permutation importance\n",
    "result = permutation_importance(\n",
    "    estimator=lambda X: get_predictions(tcn_model, X),\n",
    "    X=X_test_seq.reshape(X_test_seq.shape[0], -1),  # Flatten the sequence data\n",
    "    y=y_test_seq,\n",
    "    n_repeats=5,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Get feature importances\n",
    "importances = result.importances_mean\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(importances)), importances)\n",
    "plt.yticks(range(len(importances)), [f'Feature {i}' for i in range(len(importances))])\n",
    "plt.title('Feature Importance (Permutation Importance)')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test_seq, y_prob_tcn)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Optimized TCN')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot precision-recall curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test_seq, y_prob_tcn)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (area = {pr_auc:.3f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve - Optimized TCN')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize threshold for better F1 score\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "f1_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_prob_tcn > threshold).astype(int)\n",
    "    f1 = f1_score(y_test_seq, y_pred_threshold)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "# Find the best threshold\n",
    "best_threshold_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_threshold_idx]\n",
    "best_f1 = f1_scores[best_threshold_idx]\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.2f} with F1 score: {best_f1:.4f}\")\n",
    "\n",
    "# Plot F1 scores for different thresholds\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, f1_scores, marker='o')\n",
    "plt.axvline(x=best_threshold, color='r', linestyle='--', label=f'Best threshold: {best_threshold:.2f}')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Recalculate metrics with the optimized threshold\n",
    "y_pred_optimized = (y_prob_tcn > best_threshold).astype(int)\n",
    "accuracy_optimized = accuracy_score(y_test_seq, y_pred_optimized)\n",
    "precision_optimized = precision_score(y_test_seq, y_pred_optimized, zero_division=0)\n",
    "recall_optimized = recall_score(y_test_seq, y_pred_optimized, zero_division=0)\n",
    "f1_optimized = f1_score(y_test_seq, y_pred_optimized, zero_division=0)\n",
    "\n",
    "print(f\"Optimized Metrics:\")\n",
    "print(f\"Accuracy: {accuracy_optimized:.4f}\")\n",
    "print(f\"Precision: {precision_optimized:.4f}\")\n",
    "print(f\"Recall: {recall_optimized:.4f}\")\n",
    "print(f\"F1 Score: {f1_optimized:.4f}\")\n",
    "\n",
    "# Plot confusion matrix with optimized threshold\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm_optimized = confusion_matrix(y_test_seq, y_pred_optimized)\n",
    "sns.heatmap(cm_optimized, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix - Optimized TCN (Optimized Threshold)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}