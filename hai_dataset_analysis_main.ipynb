{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI-20.07 Dataset Analysis with Optimized Efficient Architectures\n",
    "\n",
    "This notebook analyzes the HAI-20.07 dataset and implements optimized, computationally efficient architectures for attack detection in industrial control systems. The focus is on achieving high performance with low computational resources (\"低消耗高效能\").\n",
    "\n",
    "Improvements over the previous analysis include:\n",
    "1. Better handling of class imbalance\n",
    "2. Accurate memory usage tracking\n",
    "3. More efficient model architectures\n",
    "4. Improved evaluation metrics and visualization\n",
    "5. Proper dataset loading and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Colab\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "print(f\"Running in Google Colab: {IN_COLAB}\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install required packages\n",
    "    !pip install -q xgboost scikit-learn matplotlib seaborn torch tensorflow tqdm psutil memory_profiler imbalanced-learn lightgbm optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Dataset from Kaggle (if in Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the HAI Security Dataset\n",
    "if IN_COLAB:\n",
    "    import os\n",
    "    \n",
    "    # Mount Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Check if dataset already exists\n",
    "    if not os.path.exists('/content/hai-security-dataset'):\n",
    "        print(\"Downloading HAI Security Dataset from Kaggle...\")\n",
    "        \n",
    "        # Install Kaggle API if not already installed\n",
    "        !pip install -q kaggle\n",
    "        \n",
    "        # Create Kaggle API configuration directory\n",
    "        !mkdir -p ~/.kaggle\n",
    "        \n",
    "        # Request user to upload kaggle.json file\n",
    "        from google.colab import files\n",
    "        print(\"Please upload your kaggle.json file (can be obtained from Kaggle > Account > API > Create New API Token)\")\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        # Check if the correct file was uploaded\n",
    "        if 'kaggle.json' in uploaded:\n",
    "            # Copy to Kaggle's configuration directory\n",
    "            !cp kaggle.json ~/.kaggle/kaggle.json\n",
    "            # Set correct permissions\n",
    "            !chmod 600 ~/.kaggle/kaggle.json\n",
    "            print(\"Credentials successfully set\")\n",
    "            \n",
    "            # Download the dataset\n",
    "            !kaggle datasets download icsdataset/hai-security-dataset -p /content/\n",
    "            \n",
    "            # Extract the dataset\n",
    "            print(\"\\nExtracting dataset...\")\n",
    "            !unzip -q /content/hai-security-dataset.zip -d /content/\n",
    "            print(\"Dataset extracted successfully.\")\n",
    "        else:\n",
    "            print(\"kaggle.json file not found, please ensure you upload the correct file\")\n",
    "    else:\n",
    "        print(\"HAI Security Dataset already exists in /content/hai-security-dataset\")\n",
    "    \n",
    "    # List the contents of the dataset directory\n",
    "    print(\"\\nContents of the dataset directory:\")\n",
    "    !ls -la /content/hai-security-dataset/hai-20.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, precision_recall_curve, auc\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to measure memory usage accurately\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    # Force garbage collection before measuring memory\n",
    "    gc.collect()\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    return memory_info.rss / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "# Define a class to track model performance and efficiency\n",
    "class ModelEvaluator:\n",
    "    def __init__(self):\n",
    "        self.results = []\n",
    "        \n",
    "    def add_result(self, model_name, accuracy, precision, recall, f1, auc_score, \n",
    "                   training_time, inference_time, memory_usage, model_size=None):\n",
    "        self.results.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1 Score': f1,\n",
    "            'AUC': auc_score,\n",
    "            'Training Time (s)': training_time,\n",
    "            'Inference Time (ms)': inference_time * 1000,  # Convert to milliseconds\n",
    "            'Memory Usage (MB)': memory_usage,\n",
    "            'Model Size (MB)': model_size if model_size is not None else 0\n",
    "        })\n",
    "        \n",
    "    def get_results_df(self):\n",
    "        return pd.DataFrame(self.results)\n",
    "    \n",
    "    def plot_comparison(self):\n",
    "        results_df = self.get_results_df()\n",
    "        \n",
    "        # Create a figure with 6 subplots\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "        \n",
    "        # Performance metrics\n",
    "        performance_df = results_df.melt(id_vars=['Model'],\n",
    "                                        value_vars=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC'],\n",
    "                                        var_name='Metric', value_name='Value')\n",
    "        sns.barplot(x='Model', y='Value', hue='Metric', data=performance_df, ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Performance Metrics Comparison')\n",
    "        axes[0, 0].set_ylim(0.5, 1.0)  # Adjust as needed\n",
    "        axes[0, 0].set_xticklabels(axes[0, 0].get_xticklabels(), rotation=45, ha='right')\n",
    "        axes[0, 0].legend(loc='lower right')\n",
    "        \n",
    "        # Training time\n",
    "        sns.barplot(x='Model', y='Training Time (s)', data=results_df, ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Training Time Comparison (seconds)')\n",
    "        axes[0, 1].set_xticklabels(axes[0, 1].get_xticklabels(), rotation=45, ha='right')\n",
    "        axes[0, 1].set_yscale('log')  # Log scale for better visualization\n",
    "        \n",
    "        # Inference time\n",
    "        sns.barplot(x='Model', y='Inference Time (ms)', data=results_df, ax=axes[0, 2])\n",
    "        axes[0, 2].set_title('Inference Time Comparison (milliseconds)')\n",
    "        axes[0, 2].set_xticklabels(axes[0, 2].get_xticklabels(), rotation=45, ha='right')\n",
    "        axes[0, 2].set_yscale('log')  # Log scale for better visualization\n",
    "        \n",
    "        # Memory usage\n",
    "        sns.barplot(x='Model', y='Memory Usage (MB)', data=results_df, ax=axes[1, 0])\n",
    "        axes[1, 0].set_title('Memory Usage Comparison (MB)')\n",
    "        axes[1, 0].set_xticklabels(axes[1, 0].get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Model size\n",
    "        if 'Model Size (MB)' in results_df.columns:\n",
    "            sns.barplot(x='Model', y='Model Size (MB)', data=results_df, ax=axes[1, 1])\n",
    "            axes[1, 1].set_title('Model Size Comparison (MB)')\n",
    "            axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        # Efficiency score\n",
    "        if 'Efficiency Score' in results_df.columns:\n",
    "            sns.barplot(x='Model', y='Efficiency Score', data=results_df, ax=axes[1, 2])\n",
    "            axes[1, 2].set_title('Efficiency Score Comparison')\n",
    "            axes[1, 2].set_xticklabels(axes[1, 2].get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_confusion_matrices(self, y_true, y_pred_dict):\n",
    "        \"\"\"Plot confusion matrices for all models\"\"\"\n",
    "        n_models = len(y_pred_dict)\n",
    "        fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 5))\n",
    "        \n",
    "        if n_models == 1:\n",
    "            axes = [axes]\n",
    "            \n",
    "        for i, (model_name, y_pred) in enumerate(y_pred_dict.items()):\n",
    "            cm = confusion_matrix(y_true, y_pred)\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
    "            axes[i].set_title(f'Confusion Matrix - {model_name}')\n",
    "            axes[i].set_xlabel('Predicted')\n",
    "            axes[i].set_ylabel('True')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_pr_curves(self, y_true, y_prob_dict):\n",
    "        \"\"\"Plot precision-recall curves for all models\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for model_name, y_prob in y_prob_dict.items():\n",
    "            precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            plt.plot(recall, precision, lw=2, label=f'{model_name} (AUC = {pr_auc:.3f})')\n",
    "            \n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curves')\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = ModelEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data path based on environment\n",
    "if IN_COLAB:\n",
    "    data_path = '/content/hai-security-dataset/hai-20.07/'\n",
    "else:\n",
    "    data_path = 'hai-security-dataset/hai-20.07/'\n",
    "\n",
    "# Load training datasets\n",
    "train1 = pd.read_csv(f'{data_path}train1.csv', sep=';')\n",
    "train2 = pd.read_csv(f'{data_path}train2.csv', sep=';')\n",
    "\n",
    "# Load testing datasets\n",
    "test1 = pd.read_csv(f'{data_path}test1.csv', sep=';')\n",
    "test2 = pd.read_csv(f'{data_path}test2.csv', sep=';')\n",
    "\n",
    "# Display basic information about the datasets\n",
    "print(\"Training Dataset 1 Shape:\", train1.shape)\n",
    "print(\"Training Dataset 2 Shape:\", train2.shape)\n",
    "print(\"Testing Dataset 1 Shape:\", test1.shape)\n",
    "print(\"Testing Dataset 2 Shape:\", test2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert time column to datetime\n",
    "train1['time'] = pd.to_datetime(train1['time'])\n",
    "train2['time'] = pd.to_datetime(train2['time'])\n",
    "test1['time'] = pd.to_datetime(test1['time'])\n",
    "test2['time'] = pd.to_datetime(test2['time'])\n",
    "\n",
    "# Identify feature columns (excluding time and attack labels)\n",
    "feature_columns = [col for col in train1.columns if col not in ['time', 'attack', 'attack_P1', 'attack_P2', 'attack_P3']]\n",
    "print(f\"Number of feature columns: {len(feature_columns)}\")\n",
    "print(f\"First few feature columns: {feature_columns[:5]}...\")\n",
    "\n",
    "# Check attack distribution\n",
    "print(\"\\nAttack distribution in train1:\")\n",
    "print(train1['attack'].value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nAttack distribution in train2:\")\n",
    "print(train2['attack'].value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nAttack distribution in test1:\")\n",
    "print(test1['attack'].value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nAttack distribution in test2:\")\n",
    "print(test2['attack'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the class distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "train_combined = pd.concat([train1, train2], ignore_index=True)\n",
    "train_attack_counts = train_combined['attack'].value_counts()\n",
    "plt.pie(train_attack_counts, labels=['Normal', 'Attack'] if len(train_attack_counts) > 1 else ['Normal'], \n",
    "        autopct='%1.1f%%', startangle=90, colors=['lightblue', 'salmon'] if len(train_attack_counts) > 1 else ['lightblue'])\n",
    "plt.title('Training Data Class Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "test_combined = pd.concat([test1, test2], ignore_index=True)\n",
    "test_attack_counts = test_combined['attack'].value_counts()\n",
    "plt.pie(test_attack_counts, labels=['Normal', 'Attack'], autopct='%1.1f%%', startangle=90, colors=['lightblue', 'salmon'])\n",
    "plt.title('Testing Data Class Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training datasets\n",
    "train_combined = pd.concat([train1, train2], ignore_index=True)\n",
    "\n",
    "# Combine testing datasets\n",
    "test_combined = pd.concat([test1, test2], ignore_index=True)\n",
    "\n",
    "# Extract features and target\n",
    "X_train = train_combined[feature_columns]\n",
    "y_train = train_combined['attack']\n",
    "\n",
    "X_test = test_combined[feature_columns]\n",
    "y_test = test_combined['attack']\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in training data:\")\n",
    "print(X_train.isnull().sum().sum())\n",
    "print(\"Missing values in testing data:\")\n",
    "print(X_test.isnull().sum().sum())\n",
    "\n",
    "# Check for infinite values\n",
    "print(\"Infinite values in training data:\")\n",
    "print(np.isinf(X_train).sum().sum())\n",
    "print(\"Infinite values in testing data:\")\n",
    "print(np.isinf(X_test).sum().sum())\n",
    "\n",
    "# Replace any infinite values with NaN and then fill with column mean\n",
    "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
    "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Fill missing values with column mean\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_train.mean())  # Use training mean for test data\n",
    "\n",
    "# Use RobustScaler instead of StandardScaler to be less affected by outliers\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "print(\"X_test_scaled shape:\", X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to add time-based features\n",
    "def add_time_features(df):\n",
    "    # Make a copy to avoid modifying the original dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Extract time-based features\n",
    "    df_copy['hour'] = df_copy['time'].dt.hour\n",
    "    df_copy['minute'] = df_copy['time'].dt.minute\n",
    "    df_copy['second'] = df_copy['time'].dt.second\n",
    "    df_copy['day_of_week'] = df_copy['time'].dt.dayofweek\n",
    "    \n",
    "    # Add cyclical time features (to capture periodicity)\n",
    "    df_copy['hour_sin'] = np.sin(2 * np.pi * df_copy['hour'] / 24)\n",
    "    df_copy['hour_cos'] = np.cos(2 * np.pi * df_copy['hour'] / 24)\n",
    "    df_copy['minute_sin'] = np.sin(2 * np.pi * df_copy['minute'] / 60)\n",
    "    df_copy['minute_cos'] = np.cos(2 * np.pi * df_copy['minute'] / 60)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Create a function to add rolling window statistics\n",
    "def add_rolling_features(df, window_sizes=[5, 10], top_n=3):\n",
    "    # Make a copy to avoid modifying the original dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Sort by time to ensure correct rolling window calculation\n",
    "    df_copy = df_copy.sort_values('time')\n",
    "    \n",
    "    # Select features for rolling statistics (exclude time and attack columns)\n",
    "    rolling_features = [col for col in df_copy.columns if col not in ['time', 'attack', 'attack_P1', 'attack_P2', 'attack_P3']]\n",
    "    \n",
    "    # Use only the top N most important features for rolling statistics to keep it efficient\n",
    "    # In a real scenario, you would select based on feature importance\n",
    "    top_features = rolling_features[:top_n]\n",
    "    \n",
    "    # Calculate rolling statistics for different window sizes\n",
    "    for window_size in window_sizes:\n",
    "        for feature in top_features:\n",
    "            df_copy[f'{feature}_rolling_mean_{window_size}'] = df_copy[feature].rolling(window=window_size).mean()\n",
    "            df_copy[f'{feature}_rolling_std_{window_size}'] = df_copy[feature].rolling(window=window_size).std()\n",
    "    \n",
    "    # Drop NaN values created by rolling window\n",
    "    df_copy = df_copy.dropna()\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Create a function to add lag features\n",
    "def add_lag_features(df, lag_steps=[1, 2], top_n=3):\n",
    "    # Make a copy to avoid modifying the original dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Sort by time to ensure correct lag calculation\n",
    "    df_copy = df_copy.sort_values('time')\n",
    "    \n",
    "    # Select features for lag (exclude time and attack columns)\n",
    "    lag_features = [col for col in df_copy.columns if col not in ['time', 'attack', 'attack_P1', 'attack_P2', 'attack_P3']]\n",
    "    \n",
    "    # Use only the top N most important features for lag to keep it efficient\n",
    "    top_features = lag_features[:top_n]\n",
    "    \n",
    "    # Calculate lag features\n",
    "    for lag in lag_steps:\n",
    "        for feature in top_features:\n",
    "            df_copy[f'{feature}_lag_{lag}'] = df_copy[feature].shift(lag)\n",
    "    \n",
    "    # Drop NaN values created by lag\n",
    "    df_copy = df_copy.dropna()\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Create a function to add statistical features\n",
    "def add_statistical_features(df):\n",
    "    # Make a copy to avoid modifying the original dataframe\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Select features for statistics (exclude time and attack columns)\n",
    "    stat_features = [col for col in df_copy.columns if col not in ['time', 'attack', 'attack_P1', 'attack_P2', 'attack_P3']]\n",
    "    \n",
    "    # Group features by prefix (e.g., P1_, P2_, etc.)\n",
    "    prefixes = set([col.split('_')[0] for col in stat_features])\n",
    "    \n",
    "    # Calculate statistics for each group\n",
    "    for prefix in prefixes:\n",
    "        group_cols = [col for col in stat_features if col.startswith(prefix)]\n",
    "        if len(group_cols) > 1:  # Only calculate if there are multiple columns in the group\n",
    "            df_copy[f'{prefix}_mean'] = df_copy[group_cols].mean(axis=1)\n",
    "            df_copy[f'{prefix}_std'] = df_copy[group_cols].std(axis=1)\n",
    "            df_copy[f'{prefix}_max'] = df_copy[group_cols].max(axis=1)\n",
    "            df_copy[f'{prefix}_min'] = df_copy[group_cols].min(axis=1)\n",
    "            df_copy[f'{prefix}_range'] = df_copy[f'{prefix}_max'] - df_copy[f'{prefix}_min']\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering steps\n",
    "print(\"Applying feature engineering...\")\n",
    "\n",
    "# Add time features\n",
    "train_features = add_time_features(train_combined)\n",
    "test_features = add_time_features(test_combined)\n",
    "\n",
    "# Add statistical features\n",
    "train_features = add_statistical_features(train_features)\n",
    "test_features = add_statistical_features(test_features)\n",
    "\n",
    "# Add rolling features (with smaller window sizes and fewer top features)\n",
    "train_features = add_rolling_features(train_features, window_sizes=[5, 10], top_n=3)\n",
    "test_features = add_rolling_features(test_features, window_sizes=[5, 10], top_n=3)\n",
    "\n",
    "# Add lag features (with fewer lag steps and fewer top features)\n",
    "train_features = add_lag_features(train_features, lag_steps=[1, 2], top_n=3)\n",
    "test_features = add_lag_features(test_features, lag_steps=[1, 2], top_n=3)\n",
    "\n",
    "# Extract features and target from the enhanced datasets\n",
    "feature_columns_enhanced = [col for col in train_features.columns\n",
    "                           if col not in ['time', 'attack', 'attack_P1', 'attack_P2', 'attack_P3']]\n",
    "\n",
    "X_train_enhanced = train_features[feature_columns_enhanced]\n",
    "y_train_enhanced = train_features['attack']\n",
    "\n",
    "X_test_enhanced = test_features[feature_columns_enhanced]\n",
    "y_test_enhanced = test_features['attack']\n",
    "\n",
    "# Scale the enhanced features\n",
    "scaler_enhanced = RobustScaler()\n",
    "X_train_enhanced_scaled = scaler_enhanced.fit_transform(X_train_enhanced)\n",
    "X_test_enhanced_scaled = scaler_enhanced.transform(X_test_enhanced)\n",
    "\n",
    "print(\"X_train_enhanced_scaled shape:\", X_train_enhanced_scaled.shape)\n",
    "print(\"X_test_enhanced_scaled shape:\", X_test_enhanced_scaled.shape)\n",
    "print(f\"Number of enhanced features: {X_train_enhanced_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SMOTE for handling class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Class distribution in training data:\")\n",
    "print(y_train_enhanced.value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nClass distribution in testing data:\")\n",
    "print(y_test_enhanced.value_counts(normalize=True) * 100)\n",
    "\n",
    "# Apply SMOTE to handle class imbalance\n",
    "print(\"\\nApplying SMOTE to balance the training data...\")\n",
    "smote = SMOTE(random_state=RANDOM_SEED, sampling_strategy=0.3)  # Create minority class samples to be 30% of majority\n",
    "X_train_enhanced_balanced, y_train_enhanced_balanced = smote.fit_resample(X_train_enhanced_scaled, y_train_enhanced)\n",
    "\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_enhanced_balanced).value_counts(normalize=True) * 100)\n",
    "print(f\"X_train_enhanced_balanced shape: {X_train_enhanced_balanced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prepare Data for Sequence Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences for time series models\n",
    "def create_sequences(X, y, time_steps=10, stride=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(0, len(X) - time_steps, stride):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps - 1])  # Use the label of the last timestep\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "# Create sequences for training and testing with a stride of 5 to reduce data size\n",
    "TIME_STEPS = 10\n",
    "STRIDE = 5\n",
    "\n",
    "# Create sequences from the original scaled data\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train.values, TIME_STEPS, STRIDE)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test.values, TIME_STEPS, STRIDE)\n",
    "\n",
    "print(\"X_train_seq shape:\", X_train_seq.shape)\n",
    "print(\"y_train_seq shape:\", y_train_seq.shape)\n",
    "print(\"X_test_seq shape:\", X_test_seq.shape)\n",
    "print(\"y_test_seq shape:\", y_test_seq.shape)\n",
    "\n",
    "# Check class distribution in sequence data\n",
    "print(\"\\nClass distribution in training sequence data:\")\n",
    "print(pd.Series(y_train_seq).value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nClass distribution in testing sequence data:\")\n",
    "print(pd.Series(y_test_seq).value_counts(normalize=True) * 100)\n",
    "\n",
    "# Apply SMOTE to handle class imbalance in sequence data\n",
    "print(\"\\nApplying SMOTE to balance the training sequence data...\")\n",
    "# Reshape sequences for SMOTE\n",
    "X_train_seq_reshaped = X_train_seq.reshape(X_train_seq.shape[0], -1)\n",
    "smote_seq = SMOTE(random_state=RANDOM_SEED, sampling_strategy=0.3)  # Create minority class samples to be 30% of majority\n",
    "X_train_seq_balanced_reshaped, y_train_seq_balanced = smote_seq.fit_resample(X_train_seq_reshaped, y_train_seq)\n",
    "# Reshape back to sequences\n",
    "X_train_seq_balanced = X_train_seq_balanced_reshaped.reshape(-1, TIME_STEPS, X_train_scaled.shape[1])\n",
    "\n",
    "print(\"Class distribution after SMOTE:\")\n",
    "print(pd.Series(y_train_seq_balanced).value_counts(normalize=True) * 100)\n",
    "print(f\"X_train_seq_balanced shape: {X_train_seq_balanced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Preprocessed Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data for model training\n",
    "import pickle\n",
    "\n",
    "# Create a directory for preprocessed data if it doesn't exist\n",
    "if not os.path.exists('preprocessed_data'):\n",
    "    os.makedirs('preprocessed_data')\n",
    "\n",
    "# Save tabular data\n",
    "with open('preprocessed_data/tabular_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_train_enhanced_scaled': X_train_enhanced_scaled,\n",
    "        'y_train_enhanced': y_train_enhanced,\n",
    "        'X_test_enhanced_scaled': X_test_enhanced_scaled,\n",
    "        'y_test_enhanced': y_test_enhanced,\n",
    "        'X_train_enhanced_balanced': X_train_enhanced_balanced,\n",
    "        'y_train_enhanced_balanced': y_train_enhanced_balanced,\n",
    "        'feature_columns_enhanced': feature_columns_enhanced,\n",
    "        'scaler_enhanced': scaler_enhanced\n",
    "    }, f)\n",
    "\n",
    "# Save sequence data\n",
    "with open('preprocessed_data/sequence_data.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'X_train_seq': X_train_seq,\n",
    "        'y_train_seq': y_train_seq,\n",
    "        'X_test_seq': X_test_seq,\n",
    "        'y_test_seq': y_test_seq,\n",
    "        'X_train_seq_balanced': X_train_seq_balanced,\n",
    "        'y_train_seq_balanced': y_train_seq_balanced,\n",
    "        'TIME_STEPS': TIME_STEPS,\n",
    "        'STRIDE': STRIDE,\n",
    "        'scaler': scaler\n",
    "    }, f)\n",
    "\n",
    "print(\"Preprocessed data saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}