{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI Security Dataset Preprocessing\n",
    "\n",
    "This notebook focuses on preprocessing the HIL-based Augmented ICS (HAI) Security Dataset. The dataset contains data collected from a realistic industrial control system (ICS) testbed augmented with a hardware-in-the-loop (HIL) simulator.\n",
    "\n",
    "We'll start with the HAI-20.07 version and implement efficient preprocessing techniques to handle the large volume of data, optimizing for GPU processing (A100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "import cudf  # GPU-accelerated DataFrame library\n",
    "import cupy as cp  # GPU-accelerated NumPy-like library\n",
    "import dask.dataframe as dd  # For parallel computing with DataFrames\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import joblib  # For saving preprocessing objects\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview\n",
    "\n",
    "First, let's explore the available dataset versions and their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# List all available dataset versions\n",
    "dataset_path = './hai-security-dataset/'\n",
    "dataset_versions = [os.path.basename(version) for version in glob.glob(f'{dataset_path}hai-*')]\n",
    "dataset_versions.extend([os.path.basename(version) for version in glob.glob(f'{dataset_path}haiend-*')])\n",
    "print(f\"Available dataset versions: {dataset_versions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring HAI-20.07 Dataset\n",
    "\n",
    "Let's start by exploring the HAI-20.07 dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# List all files in HAI-20.07\n",
    "hai_20_07_path = f'{dataset_path}hai-20.07/'\n",
    "hai_20_07_files = [os.path.basename(file) for file in glob.glob(f'{hai_20_07_path}*.csv')]\n",
    "print(f\"Files in HAI-20.07: {hai_20_07_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Efficient Data Loading\n",
    "\n",
    "For HAI-20.07, we need to handle the semicolon-separated CSV files efficiently. We'll use Dask for parallel processing and then convert to cuDF for GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_hai_20_07_file(file_path, sample_rows=None):\n",
    "    \"\"\"\n",
    "    Load HAI-20.07 CSV file efficiently using Dask and cuDF.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        sample_rows: Number of rows to sample (None for all rows)\n",
    "        \n",
    "    Returns:\n",
    "        cudf.DataFrame: GPU-accelerated DataFrame\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # First, use pandas to read a few rows to get column names and dtypes\n",
    "    sample_df = pd.read_csv(file_path, sep=';', nrows=5)\n",
    "    \n",
    "    # Create a dictionary of dtypes to optimize memory usage\n",
    "    dtypes = {}\n",
    "    for col in sample_df.columns:\n",
    "        if col == 'time':\n",
    "            dtypes[col] = 'object'  # Keep timestamp as object initially\n",
    "        elif 'attack' in col.lower():\n",
    "            dtypes[col] = 'int8'  # Binary labels can be stored as int8\n",
    "        elif sample_df[col].dtype == 'float64':\n",
    "            dtypes[col] = 'float32'  # Reduce precision to save memory\n",
    "        elif sample_df[col].dtype == 'int64':\n",
    "            dtypes[col] = 'int32'  # Reduce precision to save memory\n",
    "    \n",
    "    # Use Dask to read the file in parallel chunks\n",
    "    if sample_rows:\n",
    "        # For sampling, use pandas directly\n",
    "        df = pd.read_csv(file_path, sep=';', dtype=dtypes, nrows=sample_rows)\n",
    "    else:\n",
    "        # For full file, use Dask\n",
    "        dask_df = dd.read_csv(file_path, sep=';', dtype=dtypes, blocksize=\"64MB\")\n",
    "        df = dask_df.compute()\n",
    "    \n",
    "    # Convert to cuDF for GPU acceleration\n",
    "    try:\n",
    "        gpu_df = cudf.DataFrame.from_pandas(df)\n",
    "        print(f\"Data loaded and converted to GPU DataFrame in {time.time() - start_time:.2f} seconds\")\n",
    "        return gpu_df\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not convert to GPU DataFrame: {e}\")\n",
    "        print(f\"Data loaded in pandas DataFrame in {time.time() - start_time:.2f} seconds\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load a sample of the first training file to explore its structure\n",
    "train1_path = f'{hai_20_07_path}train1.csv'\n",
    "train1_sample = load_hai_20_07_file(train1_path, sample_rows=10000)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Shape of train1.csv sample: {train1_sample.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(train1_sample.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check column names and data types\n",
    "print(f\"Number of columns: {len(train1_sample.columns)}\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(train1_sample.columns.tolist())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types:\")\n",
    "for col, dtype in zip(train1_sample.dtypes.index, train1_sample.dtypes):\n",
    "    print(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Timestamp Conversion\n",
    "\n",
    "Convert the timestamp column to datetime format for time-series analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def convert_timestamp(df, timestamp_col='time'):\n",
    "    \"\"\"\n",
    "    Convert timestamp column to datetime format.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame (pandas or cuDF)\n",
    "        timestamp_col: Name of the timestamp column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with converted timestamp\n",
    "    \"\"\"\n",
    "    if isinstance(df, cudf.DataFrame):\n",
    "        # For cuDF DataFrame\n",
    "        if timestamp_col in df.columns:\n",
    "            df[timestamp_col] = df[timestamp_col].astype('datetime64[ns]')\n",
    "    else:\n",
    "        # For pandas DataFrame\n",
    "        if timestamp_col in df.columns:\n",
    "            df[timestamp_col] = pd.to_datetime(df[timestamp_col])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert timestamp\n",
    "train1_sample = convert_timestamp(train1_sample)\n",
    "\n",
    "# Check time range\n",
    "if isinstance(train1_sample, cudf.DataFrame):\n",
    "    # For cuDF DataFrame\n",
    "    start_time = train1_sample['time'].min()\n",
    "    end_time = train1_sample['time'].max()\n",
    "    print(f\"Start time: {start_time}\")\n",
    "    print(f\"End time: {end_time}\")\n",
    "else:\n",
    "    # For pandas DataFrame\n",
    "    print(f\"Start time: {train1_sample['time'].min()}\")\n",
    "    print(f\"End time: {train1_sample['time'].max()}\")\n",
    "    print(f\"Duration: {train1_sample['time'].max() - train1_sample['time'].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Analyzing Attack Labels\n",
    "\n",
    "Check if the dataset contains attack labels and analyze their distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if attack labels are present\n",
    "attack_columns = [col for col in train1_sample.columns if 'attack' in col.lower()]\n",
    "print(f\"Attack label columns: {attack_columns}\")\n",
    "\n",
    "if attack_columns:\n",
    "    # Count attack instances\n",
    "    for col in attack_columns:\n",
    "        if isinstance(train1_sample, cudf.DataFrame):\n",
    "            # For cuDF DataFrame\n",
    "            attack_count = train1_sample[col].sum()\n",
    "            attack_percentage = (attack_count / len(train1_sample)) * 100\n",
    "        else:\n",
    "            # For pandas DataFrame\n",
    "            attack_count = train1_sample[col].sum()\n",
    "            attack_percentage = (attack_count / len(train1_sample)) * 100\n",
    "            \n",
    "        print(f\"{col}: {attack_count} attacks ({attack_percentage:.2f}% of data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load a test dataset to see if it contains attack labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load a sample of the first test file\n",
    "test1_path = f'{hai_20_07_path}test1.csv'\n",
    "test1_sample = load_hai_20_07_file(test1_path, sample_rows=10000)\n",
    "test1_sample = convert_timestamp(test1_sample)\n",
    "\n",
    "# Check if attack labels are present\n",
    "attack_columns = [col for col in test1_sample.columns if 'attack' in col.lower()]\n",
    "print(f\"Attack label columns in test dataset: {attack_columns}\")\n",
    "\n",
    "if attack_columns:\n",
    "    # Count attack instances\n",
    "    for col in attack_columns:\n",
    "        if isinstance(test1_sample, cudf.DataFrame):\n",
    "            # For cuDF DataFrame\n",
    "            attack_count = test1_sample[col].sum()\n",
    "            attack_percentage = (attack_count / len(test1_sample)) * 100\n",
    "        else:\n",
    "            # For pandas DataFrame\n",
    "            attack_count = test1_sample[col].sum()\n",
    "            attack_percentage = (attack_count / len(test1_sample)) * 100\n",
    "            \n",
    "        print(f\"{col}: {attack_count} attacks ({attack_percentage:.2f}% of data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Data Visualization\n",
    "\n",
    "Visualize key data points and attack patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_time_series(df, columns, timestamp_col='time', title=None, figsize=(14, 10)):\n",
    "    \"\"\"\n",
    "    Plot time series data for specified columns.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame (pandas or cuDF)\n",
    "        columns: List of columns to plot\n",
    "        timestamp_col: Name of the timestamp column\n",
    "        title: Plot title\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    # Convert cuDF to pandas if needed\n",
    "    if isinstance(df, cudf.DataFrame):\n",
    "        plot_df = df.to_pandas()\n",
    "    else:\n",
    "        plot_df = df\n",
    "    \n",
    "    # Sample data to reduce plotting time (every 100th point)\n",
    "    plot_df = plot_df.iloc[::100].copy()\n",
    "    \n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(len(columns), 1, figsize=figsize)\n",
    "    if len(columns) == 1:\n",
    "        axes = [axes]  # Make axes iterable if only one subplot\n",
    "    \n",
    "    # Plot each column\n",
    "    for i, col in enumerate(columns):\n",
    "        if col in plot_df.columns:\n",
    "            axes[i].plot(plot_df[timestamp_col], plot_df[col])\n",
    "            axes[i].set_title(f'{col} Time Series')\n",
    "            axes[i].set_xlabel('Time')\n",
    "            axes[i].set_ylabel('Value')\n",
    "    \n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define key data points based on the HAI dataset technical details\n",
    "key_points = [\n",
    "    'P1_B2016',  # Pressure demand for thermal power output control\n",
    "    'P1_PIT01',  # Heat-exchanger outlet pressure\n",
    "    'P1_B3004',  # Water level setpoint (return water tank)\n",
    "    'P1_LIT01',  # Water level of the return water tank\n",
    "    'P1_B3005',  # Discharge flowrate setpoint (return water tank)\n",
    "    'P1_FT03',   # Measured flowrate of the return water tank\n",
    "    'P1_B4022',  # Temperature demand for thermal power output control\n",
    "    'P1_TIT01'   # Heat-exchanger outlet temperature\n",
    "]\n",
    "\n",
    "# Check if these columns exist in the dataset\n",
    "existing_key_points = [col for col in key_points if col in test1_sample.columns]\n",
    "print(f\"Available key points: {existing_key_points}\")\n",
    "\n",
    "# Plot time series for key data points\n",
    "if existing_key_points:\n",
    "    plot_time_series(test1_sample, existing_key_points, title=\"Key Data Points in Test Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Visualizing Attack Patterns\n",
    "\n",
    "If attack labels are present, visualize the attack patterns along with key data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_attack_periods(df, attack_col, data_cols, timestamp_col='time', max_periods=3, figsize=(14, 12)):\n",
    "    \"\"\"\n",
    "    Plot data during attack periods.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame (pandas or cuDF)\n",
    "        attack_col: Attack label column\n",
    "        data_cols: List of data columns to plot\n",
    "        timestamp_col: Name of the timestamp column\n",
    "        max_periods: Maximum number of attack periods to plot\n",
    "        figsize: Figure size\n",
    "    \"\"\"\n",
    "    # Convert cuDF to pandas if needed\n",
    "    if isinstance(df, cudf.DataFrame):\n",
    "        plot_df = df.to_pandas()\n",
    "    else:\n",
    "        plot_df = df\n",
    "    \n",
    "    # Find attack periods\n",
    "    attack_starts = []\n",
    "    attack_ends = []\n",
    "    in_attack = False\n",
    "    \n",
    "    for i, row in plot_df.iterrows():\n",
    "        if row[attack_col] == 1 and not in_attack:\n",
    "            attack_starts.append(i)\n",
    "            in_attack = True\n",
    "        elif row[attack_col] == 0 and in_attack:\n",
    "            attack_ends.append(i-1)\n",
    "            in_attack = False\n",
    "    \n",
    "    if in_attack:  # If dataset ends during an attack\n",
    "        attack_ends.append(len(plot_df)-1)\n",
    "    \n",
    "    print(f\"Found {len(attack_starts)} attack periods\")\n",
    "    \n",
    "    # Visualize the first few attack periods\n",
    "    num_attacks_to_show = min(max_periods, len(attack_starts))\n",
    "    \n",
    "    for i in range(num_attacks_to_show):\n",
    "        start_idx = max(0, attack_starts[i] - 100)  # Include some pre-attack data\n",
    "        end_idx = min(len(plot_df)-1, attack_ends[i] + 100)  # Include some post-attack data\n",
    "        \n",
    "        attack_df = plot_df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        # Plot key data points during this attack\n",
    "        fig, axes = plt.subplots(len(data_cols)+1, 1, figsize=figsize)\n",
    "        \n",
    "        # Plot attack label\n",
    "        axes[0].plot(attack_df[timestamp_col], attack_df[attack_col], 'r-')\n",
    "        axes[0].set_title(f'Attack Period {i+1}')\n",
    "        axes[0].set_ylabel('Attack')\n",
    "        \n",
    "        # Plot each data point\n",
    "        for j, point in enumerate(data_cols):\n",
    "            if point in attack_df.columns:\n",
    "                axes[j+1].plot(attack_df[timestamp_col], attack_df[point])\n",
    "                axes[j+1].set_title(f'{point} During Attack Period {i+1}')\n",
    "                axes[j+1].set_ylabel('Value')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot attack periods if attack labels are present\n",
    "if attack_columns and existing_key_points:\n",
    "    plot_attack_periods(test1_sample, attack_columns[0], existing_key_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing Pipeline\n",
    "\n",
    "Now let's create a comprehensive preprocessing pipeline for the HAI-20.07 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class HAIDataPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocessing pipeline for HAI security dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_version='hai-20.07', scaler_type='standard'):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor.\n",
    "        \n",
    "        Args:\n",
    "            dataset_version: Dataset version (e.g., 'hai-20.07')\n",
    "            scaler_type: Type of scaler ('standard' or 'minmax')\n",
    "        \"\"\"\n",
    "        self.dataset_version = dataset_version\n",
    "        self.dataset_path = f'./hai-security-dataset/{dataset_version}/'\n",
    "        self.scaler_type = scaler_type\n",
    "        self.scaler = None\n",
    "        self.timestamp_col = 'time' if dataset_version == 'hai-20.07' or dataset_version == 'hai-21.03' else 'timestamp'\n",
    "        self.separator = ';' if dataset_version == 'hai-20.07' else ','\n",
    "        self.feature_columns = None\n",
    "        self.attack_columns = None\n",
    "        \n",
    "    def get_file_list(self):\n",
    "        \"\"\"\n",
    "        Get list of training and test files.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (train_files, test_files)\n",
    "        \"\"\"\n",
    "        all_files = glob.glob(f'{self.dataset_path}*.csv')\n",
    "        train_files = [f for f in all_files if 'train' in os.path.basename(f).lower()]\n",
    "        test_files = [f for f in all_files if 'test' in os.path.basename(f).lower()]\n",
    "        \n",
    "        return train_files, test_files\n",
    "    \n",
    "    def load_file(self, file_path, sample_rows=None):\n",
    "        \"\"\"\n",
    "        Load a CSV file efficiently.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the CSV file\n",
    "            sample_rows: Number of rows to sample (None for all rows)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Loaded data\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # First, use pandas to read a few rows to get column names and dtypes\n",
    "        sample_df = pd.read_csv(file_path, sep=self.separator, nrows=5)\n",
    "        \n",
    "        # Create a dictionary of dtypes to optimize memory usage\n",
    "        dtypes = {}\n",
    "        for col in sample_df.columns:\n",
    "            if col == self.timestamp_col:\n",
    "                dtypes[col] = 'object'  # Keep timestamp as object initially\n",
    "            elif 'attack' in col.lower():\n",
    "                dtypes[col] = 'int8'  # Binary labels can be stored as int8\n",
    "            elif sample_df[col].dtype == 'float64':\n",
    "                dtypes[col] = 'float32'  # Reduce precision to save memory\n",
    "            elif sample_df[col].dtype == 'int64':\n",
    "                dtypes[col] = 'int32'  # Reduce precision to save memory\n",
    "        \n",
    "        # Use Dask to read the file in parallel chunks\n",
    "        if sample_rows:\n",
    "            # For sampling, use pandas directly\n",
    "            df = pd.read_csv(file_path, sep=self.separator, dtype=dtypes, nrows=sample_rows)\n",
    "        else:\n",
    "            # For full file, use Dask\n",
    "            dask_df = dd.read_csv(file_path, sep=self.separator, dtype=dtypes, blocksize=\"64MB\")\n",
    "            df = dask_df.compute()\n",
    "        \n",
    "        # Convert timestamp\n",
    "        if self.timestamp_col in df.columns:\n",
    "            df[self.timestamp_col] = pd.to_datetime(df[self.timestamp_col])\n",
    "        \n",
    "        # Identify feature and attack columns\n",
    "        if self.feature_columns is None:\n",
    "            self.attack_columns = [col for col in df.columns if 'attack' in col.lower()]\n",
    "            self.feature_columns = [col for col in df.columns \n",
    "                                   if col != self.timestamp_col and col not in self.attack_columns]\n",
    "        \n",
    "        print(f\"Data loaded in {time.time() - start_time:.2f} seconds\")\n",
    "        return df\n",
    "    \n",
    "    def fit_scaler(self, train_files, sample_size=10000):\n",
    "        \"\"\"\n",
    "        Fit a scaler on training data.\n",
    "        \n",
    "        Args:\n",
    "            train_files: List of training file paths\n",
    "            sample_size: Number of rows to sample from each file for fitting\n",
    "            \n",
    "        Returns:\n",
    "            self: The fitted preprocessor\n",
    "        \"\"\"\n",
    "        print(f\"Fitting {self.scaler_type} scaler on training data...\")\n",
    "        \n",
    "        # Initialize the appropriate scaler\n",
    "        if self.scaler_type == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "        elif self.scaler_type == 'minmax':\n",
    "            self.scaler = MinMaxScaler()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scaler type: {self.scaler_type}\")\n",
    "        \n",
    "        # Sample data from each training file\n",
    "        all_samples = []\n",
    "        for file in train_files:\n",
    "            df = self.load_file(file, sample_rows=sample_size)\n",
    "            all_samples.append(df[self.feature_columns])\n",
    "        \n",
    "        # Concatenate samples and fit the scaler\n",
    "        combined_samples = pd.concat(all_samples)\n",
    "        self.scaler.fit(combined_samples)\n",
    "        \n",
    "        print(f\"Scaler fitted on {len(combined_samples)} samples\")\n",
    "        return self\n",
    "    \n",
    "    def transform_data(self, df):\n",
    "        \"\"\"\n",
    "        Apply preprocessing transformations to a DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (X, y, timestamps) - Features, labels, and timestamps\n",
    "        \"\"\"\n",
    "        # Extract features, labels, and timestamps\n",
    "        X = df[self.feature_columns].copy()\n",
    "        timestamps = df[self.timestamp_col].copy() if self.timestamp_col in df.columns else None\n",
    "        \n",
    "        # Extract labels if available\n",
    "        y = None\n",
    "        if self.attack_columns and all(col in df.columns for col in self.attack_columns):\n",
    "            if len(self.attack_columns) == 1:\n",
    "                y = df[self.attack_columns[0]].copy()\n",
    "            else:\n",
    "                y = df[self.attack_columns].copy()\n",
    "        \n",
    "        # Apply scaling if scaler is fitted\n",
    "        if self.scaler is not None:\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "            X = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "        \n",
    "        return X, y, timestamps\n",
    "    \n",
    "    def save_preprocessor(self, output_dir='./models'):\n",
    "        \"\"\"\n",
    "        Save the preprocessor to disk.\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory to save the preprocessor\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to the saved preprocessor\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = f\"{output_dir}/hai_{self.dataset_version.replace('-', '_')}_{self.scaler_type}_preprocessor.joblib\"\n",
    "        \n",
    "        # Create a dictionary with all necessary attributes\n",
    "        preprocessor_dict = {\n",
    "            'scaler': self.scaler,\n",
    "            'feature_columns': self.feature_columns,\n",
    "            'attack_columns': self.attack_columns,\n",
    "            'timestamp_col': self.timestamp_col,\n",
    "            'separator': self.separator,\n",
    "            'dataset_version': self.dataset_version,\n",
    "            'scaler_type': self.scaler_type\n",
    "        }\n",
    "        \n",
    "        # Save to disk\n",
    "        joblib.dump(preprocessor_dict, output_path)\n",
    "        print(f\"Preprocessor saved to {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "    \n",
    "    @classmethod\n",
    "    def load_preprocessor(cls, input_path):\n",
    "        \"\"\"\n",
    "        Load a preprocessor from disk.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to the saved preprocessor\n",
    "            \n",
    "        Returns:\n",
    "            HAIDataPreprocessor: Loaded preprocessor\n",
    "        \"\"\"\n",
    "        # Load from disk\n",
    "        preprocessor_dict = joblib.load(input_path)\n",
    "        \n",
    "        # Create a new instance\n",
    "        preprocessor = cls(dataset_version=preprocessor_dict['dataset_version'],\n",
    "                          scaler_type=preprocessor_dict['scaler_type'])\n",
    "        \n",
    "        # Restore attributes\n",
    "        preprocessor.scaler = preprocessor_dict['scaler']\n",
    "        preprocessor.feature_columns = preprocessor_dict['feature_columns']\n",
    "        preprocessor.attack_columns = preprocessor_dict['attack_columns']\n",
    "        preprocessor.timestamp_col = preprocessor_dict['timestamp_col']\n",
    "        preprocessor.separator = preprocessor_dict['separator']\n",
    "        \n",
    "        print(f\"Preprocessor loaded from {input_path}\")\n",
    "        return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Fit Preprocessor on HAI-20.07 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize preprocessor for HAI-20.07\n",
    "preprocessor = HAIDataPreprocessor(dataset_version='hai-20.07', scaler_type='standard')\n",
    "\n",
    "# Get training and test files\n",
    "train_files, test_files = preprocessor.get_file_list()\n",
    "print(f\"Training files: {[os.path.basename(f) for f in train_files]}\")\n",
    "print(f\"Test files: {[os.path.basename(f) for f in test_files]}\")\n",
    "\n",
    "# Fit scaler on training data\n",
    "preprocessor.fit_scaler(train_files)\n",
    "\n",
    "# Save preprocessor\n",
    "preprocessor.save_preprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Process and Save Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_and_save_data(preprocessor, file_paths, output_dir='./processed_data', chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Process and save data in chunks.\n",
    "    \n",
    "    Args:\n",
    "        preprocessor: HAIDataPreprocessor instance\n",
    "        file_paths: List of file paths to process\n",
    "        output_dir: Directory to save processed data\n",
    "        chunk_size: Size of each chunk\n",
    "        \n",
    "    Returns:\n",
    "        list: Paths to saved files\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    saved_files = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        print(f\"Processing {file_name}...\")\n",
    "        \n",
    "        # Process in chunks\n",
    "        chunk_reader = pd.read_csv(file_path, sep=preprocessor.separator, chunksize=chunk_size)\n",
    "        \n",
    "        for i, chunk in enumerate(chunk_reader):\n",
    "            # Convert timestamp\n",
    "            if preprocessor.timestamp_col in chunk.columns:\n",
    "                chunk[preprocessor.timestamp_col] = pd.to_datetime(chunk[preprocessor.timestamp_col])\n",
    "            \n",
    "            # Transform data\n",
    "            X, y, timestamps = preprocessor.transform_data(chunk)\n",
    "            \n",
    "            # Create output DataFrame\n",
    "            output_df = X.copy()\n",
    "            if timestamps is not None:\n",
    "                output_df[preprocessor.timestamp_col] = timestamps\n",
    "            if y is not None:\n",
    "                if isinstance(y, pd.Series):\n",
    "                    output_df['attack'] = y\n",
    "                else:  # DataFrame\n",
    "                    for col in y.columns:\n",
    "                        output_df[col] = y[col]\n",
    "            \n",
    "            # Save chunk\n",
    "            output_path = f\"{output_dir}/{file_name.replace('.csv', f'_chunk{i}.npz')}\"\n",
    "            np.savez_compressed(output_path, \n",
    "                               data=output_df.to_numpy(), \n",
    "                               columns=output_df.columns.tolist())\n",
    "            \n",
    "            saved_files.append(output_path)\n",
    "            print(f\"  Saved chunk {i} to {output_path}\")\n",
    "    \n",
    "    return saved_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Process and save training data\n",
    "train_output_dir = './processed_data/hai-20.07/train'\n",
    "train_saved_files = process_and_save_data(preprocessor, train_files, output_dir=train_output_dir)\n",
    "\n",
    "# Process and save test data\n",
    "test_output_dir = './processed_data/hai-20.07/test'\n",
    "test_saved_files = process_and_save_data(preprocessor, test_files, output_dir=test_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Load and Verify Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_processed_data(file_path):\n",
    "    \"\"\"\n",
    "    Load processed data from NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the NPZ file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Loaded data\n",
    "    \"\"\"\n",
    "    # Load NPZ file\n",
    "    npz_data = np.load(file_path)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(npz_data['data'], columns=npz_data['columns'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load and verify a processed file\n",
    "if train_saved_files:\n",
    "    sample_file = train_saved_files[0]\n",
    "    print(f\"Loading {sample_file}...\")\n",
    "    \n",
    "    loaded_df = load_processed_data(sample_file)\n",
    "    print(f\"Loaded data shape: {loaded_df.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(loaded_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preprocessing for Other Dataset Versions\n",
    "\n",
    "The same preprocessing pipeline can be applied to other HAI dataset versions with minor adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_dataset_version(version, scaler_type='standard'):\n",
    "    \"\"\"\n",
    "    Preprocess a specific HAI dataset version.\n",
    "    \n",
    "    Args:\n",
    "        version: Dataset version (e.g., 'hai-21.03')\n",
    "        scaler_type: Type of scaler ('standard' or 'minmax')\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (preprocessor, train_saved_files, test_saved_files)\n",
    "    \"\"\"\n",
    "    print(f\"Preprocessing {version} dataset...\")\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = HAIDataPreprocessor(dataset_version=version, scaler_type=scaler_type)\n",
    "    \n",
    "    # Get training and test files\n",
    "    train_files, test_files = preprocessor.get_file_list()\n",
    "    print(f\"Training files: {[os.path.basename(f) for f in train_files]}\")\n",
    "    print(f\"Test files: {[os.path.basename(f) for f in test_files]}\")\n",
    "    \n",
    "    # Fit scaler on training data\n",
    "    preprocessor.fit_scaler(train_files)\n",
    "    \n",
    "    # Save preprocessor\n",
    "    preprocessor.save_preprocessor()\n",
    "    \n",
    "    # Process and save training data\n",
    "    train_output_dir = f'./processed_data/{version}/train'\n",
    "    train_saved_files = process_and_save_data(preprocessor, train_files, output_dir=train_output_dir)\n",
    "    \n",
    "    # Process and save test data\n",
    "    test_output_dir = f'./processed_data/{version}/test'\n",
    "    test_saved_files = process_and_save_data(preprocessor, test_files, output_dir=test_output_dir)\n",
    "    \n",
    "    return preprocessor, train_saved_files, test_saved_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example: Preprocess HAI-21.03 dataset\n",
    "# Uncomment to run\n",
    "# preprocessor_21_03, train_files_21_03, test_files_21_03 = preprocess_dataset_version('hai-21.03')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this notebook, we've implemented a comprehensive preprocessing pipeline for the HAI security dataset, focusing on:\n",
    "\n",
    "1. Efficient data loading using Dask and cuDF for GPU acceleration\n",
    "2. Data exploration and visualization\n",
    "3. Feature scaling and normalization\n",
    "4. Processing and saving data in a format optimized for model training\n",
    "\n",
    "The preprocessed data and fitted preprocessors are saved and ready to be used for training anomaly detection models in separate notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}