{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI Security Dataset - Data Preprocessing\n",
    "\n",
    "This notebook provides utility functions for processing the HAI security dataset using the Polars framework for efficient data handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required libraries\n",
    "!pip install polars pyarrow matplotlib seaborn scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def lazy_load_csv(file_path, time_column='timestamp', time_format='%Y-%m-%d %H:%M:%S'):\n",
    "    \"\"\"\n",
    "    Lazy load CSV file and convert time column to datetime format\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to CSV file\n",
    "        time_column (str): Name of time column\n",
    "        time_format (str): Time format\n",
    "        \n",
    "    Returns:\n",
    "        pl.LazyFrame: Lazy loaded DataFrame\n",
    "    \"\"\"\n",
    "    print(f\"Lazy loading file: {file_path}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # First read a small sample to get column names and data types\n",
    "    sample_df = pl.read_csv(file_path, n_rows=1000)\n",
    "    \n",
    "    # Create a better schema to optimize data types\n",
    "    dtypes = {}\n",
    "    for col in sample_df.columns:\n",
    "        if col == time_column:\n",
    "            dtypes[col] = pl.Utf8  # Read as string first, convert to datetime later\n",
    "        elif sample_df[col].dtype == pl.Float64:\n",
    "            # Use Float32 for floating point numbers to save memory\n",
    "            dtypes[col] = pl.Float32\n",
    "        elif sample_df[col].dtype == pl.Int64:\n",
    "            # Use Int32 for integers to save memory\n",
    "            dtypes[col] = pl.Int32\n",
    "    \n",
    "    # Lazy load CSV with optimized schema\n",
    "    df_lazy = pl.scan_csv(file_path, dtypes=dtypes)\n",
    "    \n",
    "    # Convert time column to datetime\n",
    "    df_lazy = df_lazy.with_column(\n",
    "        pl.col(time_column).str.strptime(pl.Datetime, time_format).alias(time_column)\n",
    "    )\n",
    "    \n",
    "    print(f\"Loading completed in {time.time() - start_time:.2f} seconds\")\n",
    "    return df_lazy\n",
    "\n",
    "def get_file_info(file_path):\n",
    "    \"\"\"\n",
    "    Get basic information about a file\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to file\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing file information\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    file_size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "    \n",
    "    # Read a few rows to get column count\n",
    "    sample_df = pl.read_csv(file_path, n_rows=1)\n",
    "    num_columns = len(sample_df.columns)\n",
    "    \n",
    "    # Estimate row count (not accurate but fast)\n",
    "    with open(file_path, 'r') as f:\n",
    "        first_line = f.readline()\n",
    "        f.seek(0, os.SEEK_END)\n",
    "        file_size = f.tell()\n",
    "    \n",
    "    estimated_rows = file_size / (len(first_line) + 2)  # +2 for newline chars\n",
    "    \n",
    "    return {\n",
    "        \"file_name\": file_path.name,\n",
    "        \"file_size_mb\": file_size_mb,\n",
    "        \"num_columns\": num_columns,\n",
    "        \"estimated_rows\": int(estimated_rows)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunked Processing for Large Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_in_chunks(file_path, process_func, chunk_size=100000, **kwargs):\n",
    "    \"\"\"\n",
    "    Process large CSV files in chunks\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to CSV file\n",
    "        process_func (callable): Function to process each chunk\n",
    "        chunk_size (int): Size of each chunk\n",
    "        **kwargs: Additional arguments to pass to process_func\n",
    "        \n",
    "    Returns:\n",
    "        list: List of results from processing each chunk\n",
    "    \"\"\"\n",
    "    print(f\"Starting chunked processing of file: {file_path}, chunk size: {chunk_size}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    results = []\n",
    "    total_chunks = 0\n",
    "    \n",
    "    # Use polars batched reading feature\n",
    "    for i, chunk in enumerate(pl.read_csv_batched(file_path, batch_size=chunk_size)):\n",
    "        print(f\"Processing chunk {i+1}, size: {len(chunk)}\")\n",
    "        chunk_start = time.time()\n",
    "        \n",
    "        # Process current chunk\n",
    "        result = process_func(chunk, **kwargs)\n",
    "        results.append(result)\n",
    "        \n",
    "        total_chunks += 1\n",
    "        print(f\"Chunk {i+1} processed in {time.time() - chunk_start:.2f} seconds\")\n",
    "    \n",
    "    print(f\"All chunks processed in {time.time() - start_time:.2f} seconds, total chunks: {total_chunks}\")\n",
    "    return results\n",
    "\n",
    "def save_to_efficient_format(df, output_path, format='parquet', partition_cols=None):\n",
    "    \"\"\"\n",
    "    Save DataFrame to an efficient format (parquet or arrow)\n",
    "    \n",
    "    Args:\n",
    "        df (pl.DataFrame or pl.LazyFrame): DataFrame to save\n",
    "        output_path (str): Output path\n",
    "        format (str): Output format ('parquet' or 'arrow')\n",
    "        partition_cols (list): Columns to partition by (for parquet)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # If LazyFrame, collect it\n",
    "    if isinstance(df, pl.LazyFrame):\n",
    "        df = df.collect()\n",
    "    \n",
    "    if format.lower() == 'parquet':\n",
    "        if partition_cols:\n",
    "            df.write_parquet(output_path, use_pyarrow=True, compression=\"zstd\", \n",
    "                             partition_cols=partition_cols)\n",
    "        else:\n",
    "            df.write_parquet(output_path, use_pyarrow=True, compression=\"zstd\")\n",
    "    elif format.lower() == 'arrow':\n",
    "        df.write_ipc(output_path, compression=\"zstd\")\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format: {format}. Use 'parquet' or 'arrow'.\")\n",
    "    \n",
    "    print(f\"Data saved to {output_path} in {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def add_time_features(df_lazy, time_column='timestamp'):\n",
    "    \"\"\"\n",
    "    Add time-based features to DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df_lazy (pl.LazyFrame): Input LazyFrame\n",
    "        time_column (str): Name of time column\n",
    "        \n",
    "    Returns:\n",
    "        pl.LazyFrame: LazyFrame with time features added\n",
    "    \"\"\"\n",
    "    return df_lazy.with_columns([\n",
    "        pl.col(time_column).dt.hour().alias('hour'),\n",
    "        pl.col(time_column).dt.day_of_week().alias('day_of_week'),\n",
    "        pl.col(time_column).dt.day().alias('day'),\n",
    "        pl.col(time_column).dt.month().alias('month'),\n",
    "        pl.col(time_column).dt.year().alias('year'),\n",
    "        # Is weekend feature\n",
    "        (pl.col(time_column).dt.day_of_week() >= 5).alias('is_weekend'),\n",
    "        # Time of day category\n",
    "        pl.when(pl.col(time_column).dt.hour() < 6).then(pl.lit('night'))\n",
    "          .when(pl.col(time_column).dt.hour() < 12).then(pl.lit('morning'))\n",
    "          .when(pl.col(time_column).dt.hour() < 18).then(pl.lit('afternoon'))\n",
    "          .otherwise(pl.lit('evening')).alias('time_of_day')\n",
    "    ])\n",
    "\n",
    "def add_lag_features(df, columns, lags=[1, 5, 10], group_by=None):\n",
    "    \"\"\"\n",
    "    Add lag features to DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df (pl.DataFrame): Input DataFrame\n",
    "        columns (list): Columns to create lag features for\n",
    "        lags (list): List of lag values\n",
    "        group_by (str): Column to group by before creating lags\n",
    "        \n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame with lag features added\n",
    "    \"\"\"\n",
    "    result = df.clone()\n",
    "    \n",
    "    for col in columns:\n",
    "        for lag in lags:\n",
    "            if group_by:\n",
    "                result = result.with_column(\n",
    "                    pl.col(col).shift(lag).over(group_by).alias(f\"{col}_lag_{lag}\")\n",
    "                )\n",
    "            else:\n",
    "                result = result.with_column(\n",
    "                    pl.col(col).shift(lag).alias(f\"{col}_lag_{lag}\")\n",
    "                )\n",
    "    \n",
    "    return result\n",
    "\n",
    "def add_rolling_features(df, columns, windows=[5, 10, 30], group_by=None):\n",
    "    \"\"\"\n",
    "    Add rolling window features to DataFrame\n",
    "    \n",
    "    Args:\n",
    "        df (pl.DataFrame): Input DataFrame\n",
    "        columns (list): Columns to create rolling features for\n",
    "        windows (list): List of window sizes\n",
    "        group_by (str): Column to group by before creating rolling features\n",
    "        \n",
    "    Returns:\n",
    "        pl.DataFrame: DataFrame with rolling features added\n",
    "    \"\"\"\n",
    "    result = df.clone()\n",
    "    \n",
    "    for col in columns:\n",
    "        for window in windows:\n",
    "            if group_by:\n",
    "                result = result.with_columns([\n",
    "                    pl.col(col).rolling_mean(window).over(group_by).alias(f\"{col}_rolling_mean_{window}\"),\n",
    "                    pl.col(col).rolling_std(window).over(group_by).alias(f\"{col}_rolling_std_{window}\"),\n",
    "                    pl.col(col).rolling_min(window).over(group_by).alias(f\"{col}_rolling_min_{window}\"),\n",
    "                    pl.col(col).rolling_max(window).over(group_by).alias(f\"{col}_rolling_max_{window}\")\n",
    "                ])\n",
    "            else:\n",
    "                result = result.with_columns([\n",
    "                    pl.col(col).rolling_mean(window).alias(f\"{col}_rolling_mean_{window}\"),\n",
    "                    pl.col(col).rolling_std(window).alias(f\"{col}_rolling_std_{window}\"),\n",
    "                    pl.col(col).rolling_min(window).alias(f\"{col}_rolling_min_{window}\"),\n",
    "                    pl.col(col).rolling_max(window).alias(f\"{col}_rolling_max_{window}\")\n",
    "                ])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_time_series(df, time_column, value_columns, title=None, figsize=(15, 8), attack_column=None):\n",
    "    \"\"\"\n",
    "    Plot time series data\n",
    "    \n",
    "    Args:\n",
    "        df (pl.DataFrame): Input DataFrame\n",
    "        time_column (str): Name of time column\n",
    "        value_columns (list): List of columns to plot\n",
    "        title (str): Plot title\n",
    "        figsize (tuple): Figure size\n",
    "        attack_column (str): Name of attack label column\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Convert to pandas for easier plotting\n",
    "    pdf = df.to_pandas()\n",
    "    \n",
    "    for col in value_columns:\n",
    "        plt.plot(pdf[time_column], pdf[col], label=col)\n",
    "    \n",
    "    # Highlight attack regions if attack_column is provided\n",
    "    if attack_column and attack_column in df.columns:\n",
    "        attack_regions = pdf[pdf[attack_column] > 0]\n",
    "        if not attack_regions.empty:\n",
    "            plt.scatter(attack_regions[time_column], \n",
    "                       attack_regions[value_columns[0]], \n",
    "                       color='red', alpha=0.5, s=10, label='Attack')\n",
    "    \n",
    "    plt.title(title or f\"Time Series Plot of {', '.join(value_columns)}\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_correlation_matrix(df, columns=None, figsize=(12, 10)):\n",
    "    \"\"\"\n",
    "    Plot correlation matrix\n",
    "    \n",
    "    Args:\n",
    "        df (pl.DataFrame): Input DataFrame\n",
    "        columns (list): List of columns to include in correlation matrix\n",
    "        figsize (tuple): Figure size\n",
    "    \"\"\"\n",
    "    # Convert to pandas\n",
    "    if columns:\n",
    "        pdf = df.select(columns).to_pandas()\n",
    "    else:\n",
    "        # Select only numeric columns\n",
    "        numeric_cols = [col for col in df.columns if df[col].dtype in [pl.Float32, pl.Float64, pl.Int32, pl.Int64]]\n",
    "        pdf = df.select(numeric_cols).to_pandas()\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr = pdf.corr()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    \n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, fmt=\".2f\")\n",
    "    \n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_distribution(df, columns, bins=30, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Plot distribution of columns\n",
    "    \n",
    "    Args:\n",
    "        df (pl.DataFrame): Input DataFrame\n",
    "        columns (list): List of columns to plot\n",
    "        bins (int): Number of bins\n",
    "        figsize (tuple): Figure size\n",
    "    \"\"\"\n",
    "    n_cols = 2\n",
    "    n_rows = (len(columns) + 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    pdf = df.to_pandas()\n",
    "    \n",
    "    for i, col in enumerate(columns):\n",
    "        if i < len(axes):\n",
    "            sns.histplot(pdf[col], bins=bins, kde=True, ax=axes[i])\n",
    "            axes[i].set_title(f'Distribution of {col}')\n",
    "            axes[i].grid(True)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(columns), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example of how to use the functions\n",
    "# Uncomment and modify as needed\n",
    "\n",
    "# file_path = 'hai-security-dataset/hai-20.07/train1.csv'\n",
    "# \n",
    "# # Get file info\n",
    "# info = get_file_info(file_path)\n",
    "# print(info)\n",
    "# \n",
    "# # Lazy load data\n",
    "# df_lazy = lazy_load_csv(file_path)\n",
    "# \n",
    "# # Add time features\n",
    "# df_lazy = add_time_features(df_lazy)\n",
    "# \n",
    "# # Collect a sample for visualization\n",
    "# df_sample = df_lazy.limit(10000).collect()\n",
    "# \n",
    "# # Plot time series\n",
    "# plot_time_series(df_sample, 'timestamp', ['P1_PIT01', 'P1_TIT01'], \n",
    "#                  title='Pressure and Temperature Time Series')\n",
    "# \n",
    "# # Plot correlation matrix\n",
    "# plot_correlation_matrix(df_sample, columns=['P1_PIT01', 'P1_TIT01', 'P1_FT01Z', 'P1_LIT01'])\n",
    "# \n",
    "# # Save to efficient format\n",
    "# save_to_efficient_format(df_lazy, 'processed_data/train1.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
