{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from glob import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect separator and load dataset\n",
    "def load_hai_dataset(file_path, chunksize=None):\n",
    "    \"\"\"Load HAI dataset with automatic separator detection\"\"\"\n",
    "    # Read first line to detect separator\n",
    "    with open(file_path, 'r') as f:\n",
    "        first_line = f.readline().strip()\n",
    "    \n",
    "    # Detect separator\n",
    "    if ';' in first_line:\n",
    "        separator = ';'\n",
    "    elif ',' in first_line:\n",
    "        separator = ','\n",
    "    else:\n",
    "        separator = None\n",
    "    \n",
    "    # Load data (with or without chunking)\n",
    "    if chunksize:\n",
    "        df_reader = pd.read_csv(file_path, sep=separator, chunksize=chunksize)\n",
    "        return df_reader\n",
    "    else:\n",
    "        df = pd.read_csv(file_path, sep=separator)\n",
    "        \n",
    "        # Convert timestamp column\n",
    "        time_cols = [col for col in df.columns if col.lower() in ['time', 'timestamp']]\n",
    "        if time_cols:\n",
    "            df['timestamp'] = pd.to_datetime(df[time_cols[0]])\n",
    "            if time_cols[0] != 'timestamp':\n",
    "                df.drop(time_cols[0], axis=1, inplace=True)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert dataset to parquet format\n",
    "def convert_to_parquet(csv_path, parquet_path=None, chunksize=100000):\n",
    "    \"\"\"Convert CSV dataset to Parquet format for faster processing\"\"\"\n",
    "    if parquet_path is None:\n",
    "        parquet_path = os.path.splitext(csv_path)[0] + '.parquet'\n",
    "    \n",
    "    # Check if parquet file already exists\n",
    "    if os.path.exists(parquet_path):\n",
    "        print(f\"Parquet file already exists: {parquet_path}\")\n",
    "        return parquet_path\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f\"Converting {csv_path} to Parquet format...\")\n",
    "    \n",
    "    # Process in chunks for large files\n",
    "    df_reader = load_hai_dataset(csv_path, chunksize=chunksize)\n",
    "    \n",
    "    for i, chunk in enumerate(df_reader):\n",
    "        if i == 0:\n",
    "            # Convert timestamp\n",
    "            time_cols = [col for col in chunk.columns if col.lower() in ['time', 'timestamp']]\n",
    "            if time_cols:\n",
    "                chunk['timestamp'] = pd.to_datetime(chunk[time_cols[0]])\n",
    "                if time_cols[0] != 'timestamp':\n",
    "                    chunk.drop(time_cols[0], axis=1, inplace=True)\n",
    "            \n",
    "            # Write first chunk with schema\n",
    "            chunk.to_parquet(parquet_path, engine='pyarrow', index=False)\n",
    "        else:\n",
    "            # Append subsequent chunks\n",
    "            chunk.to_parquet(parquet_path, engine='pyarrow', index=False, append=True)\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Processed {(i+1)*chunksize} rows...\")\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Conversion completed in {elapsed:.2f} seconds\")\n",
    "    return parquet_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to engineer features\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create engineered features for anomaly detection\"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Time-based features\n",
    "    if 'timestamp' in result.columns:\n",
    "        result['hour'] = result['timestamp'].dt.hour\n",
    "        result['day'] = result['timestamp'].dt.day\n",
    "        result['day_of_week'] = result['timestamp'].dt.dayofweek\n",
    "    \n",
    "    # Control loop features\n",
    "    # P1-PC: Pressure Control\n",
    "    if all(col in result.columns for col in ['P1_B2016', 'P1_PIT01']):\n",
    "        result['P1_PC_error'] = result['P1_B2016'] - result['P1_PIT01']\n",
    "    \n",
    "    # P1-LC: Level Control\n",
    "    if all(col in result.columns for col in ['P1_B3004', 'P1_LIT01']):\n",
    "        result['P1_LC_error'] = result['P1_B3004'] - result['P1_LIT01']\n",
    "    \n",
    "    # P1-FC: Flow Control\n",
    "    if all(col in result.columns for col in ['P1_B3005', 'P1_FT03']):\n",
    "        result['P1_FC_error'] = result['P1_B3005'] - result['P1_FT03']\n",
    "    \n",
    "    # P1-TC: Temperature Control\n",
    "    if all(col in result.columns for col in ['P1_B4022', 'P1_TIT01']):\n",
    "        result['P1_TC_error'] = result['P1_B4022'] - result['P1_TIT01']\n",
    "    \n",
    "    # Rolling window features (5-minute window)\n",
    "    window_size = 300  # 5 minutes at 1-second sampling\n",
    "    \n",
    "    # For key process variables\n",
    "    for col in ['P1_PIT01', 'P1_LIT01', 'P1_FT03', 'P1_TIT01']:\n",
    "        if col in result.columns:\n",
    "            result[f'{col}_rolling_mean'] = result[col].rolling(window=window_size, min_periods=1).mean()\n",
    "            result[f'{col}_rolling_std'] = result[col].rolling(window=window_size, min_periods=1).std()\n",
    "    \n",
    "    # For control errors\n",
    "    for col in ['P1_PC_error', 'P1_LC_error', 'P1_FC_error', 'P1_TC_error']:\n",
    "        if col in result.columns:\n",
    "            result[f'{col}_rolling_mean'] = result[col].rolling(window=window_size, min_periods=1).mean()\n",
    "            result[f'{col}_rolling_std'] = result[col].rolling(window=window_size, min_periods=1).std()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize dataset overview\n",
    "def visualize_dataset_overview(df, title=\"Dataset Overview\"):\n",
    "    \"\"\"Create overview visualizations for a dataset\"\"\"\n",
    "    # Sample data for visualization (every 100th point)\n",
    "    sampled_df = df.iloc[::100].copy()\n",
    "    \n",
    "    # Control loops visualization\n",
    "    control_loops = {\n",
    "        'P1-PC': {\n",
    "            'SP': 'P1_B2016',\n",
    "            'PV': 'P1_PIT01',\n",
    "            'CV': ['P1_PCV01D', 'P1_PCV02D']\n",
    "        },\n",
    "        'P1-LC': {\n",
    "            'SP': 'P1_B3004',\n",
    "            'PV': 'P1_LIT01',\n",
    "            'CV': ['P1_LCV01D']\n",
    "        },\n",
    "        'P1-FC': {\n",
    "            'SP': 'P1_B3005',\n",
    "            'PV': 'P1_FT03',\n",
    "            'CV': ['P1_FCV03D']\n",
    "        },\n",
    "        'P1-TC': {\n",
    "            'SP': 'P1_B4022',\n",
    "            'PV': 'P1_TIT01',\n",
    "            'CV': ['P1_FCV01D', 'P1_FCV02D']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Plot each control loop\n",
    "    for loop_name, loop_vars in control_loops.items():\n",
    "        # Check if all variables exist in the dataset\n",
    "        sp = loop_vars['SP']\n",
    "        pv = loop_vars['PV']\n",
    "        cv = loop_vars['CV']\n",
    "        \n",
    "        if not all(col in df.columns for col in [sp, pv] + cv):\n",
    "            continue\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "        \n",
    "        # Plot setpoint and process variable\n",
    "        axes[0].plot(sampled_df['timestamp'], sampled_df[sp], 'r-', label=f'Setpoint ({sp})')\n",
    "        axes[0].plot(sampled_df['timestamp'], sampled_df[pv], 'b-', label=f'Process Variable ({pv})')\n",
    "        axes[0].set_title(f'{loop_name} Control Loop: Setpoint vs. Process Variable')\n",
    "        axes[0].set_ylabel('Value')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Plot control variables\n",
    "        for control_var in cv:\n",
    "            axes[1].plot(sampled_df['timestamp'], sampled_df[control_var], label=f'Control Variable ({control_var})')\n",
    "        \n",
    "        axes[1].set_title(f'{loop_name} Control Loop: Control Variables')\n",
    "        axes[1].set_xlabel('Time')\n",
    "        axes[1].set_ylabel('Value (%)')\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Correlation heatmap for key variables\n",
    "    key_vars = []\n",
    "    for loop in control_loops.values():\n",
    "        key_vars.extend([loop['SP'], loop['PV']])\n",
    "        key_vars.extend(loop['CV'])\n",
    "    \n",
    "    # Filter to existing columns\n",
    "    key_vars = [col for col in key_vars if col in df.columns]\n",
    "    \n",
    "    if key_vars:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        corr_matrix = df[key_vars].corr()\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        plt.title('Correlation Matrix of Key Variables')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all available dataset versions\n",
    "dataset_versions = glob('hai-security-dataset/hai-*')\n",
    "dataset_versions.extend(glob('hai-security-dataset/haiend-*'))\n",
    "dataset_versions = [os.path.basename(version) for version in dataset_versions]\n",
    "print(f\"Available dataset versions: {dataset_versions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process HAI-20.07 dataset as an example\n",
    "HAI_VERSION = 'hai-20.07'\n",
    "DATA_DIR = f'hai-security-dataset/{HAI_VERSION}'\n",
    "\n",
    "# Convert training files to parquet\n",
    "train_files = sorted(glob(f'{DATA_DIR}/train*.csv'))\n",
    "train_parquet = [convert_to_parquet(file) for file in train_files]\n",
    "\n",
    "# Load and analyze first training file\n",
    "train_df = pd.read_parquet(train_parquet[0])\n",
    "print(f\"\\nLoaded training dataset shape: {train_df.shape}\")\n",
    "\n",
    "# Visualize dataset overview\n",
    "visualize_dataset_overview(train_df, title=\"HAI-20.07 Training Dataset Overview\")\n",
    "\n",
    "# Engineer features\n",
    "train_df_engineered = engineer_features(train_df)\n",
    "\n",
    "# Visualize engineered features\n",
    "if 'P1_PC_error' in train_df_engineered.columns:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(train_df_engineered['timestamp'][::100], train_df_engineered['P1_PC_error'][::100])\n",
    "    plt.title('P1-PC Control Error')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Error (SP - PV)')\n",
    "    plt.show()\n",
    "\n",
    "# Distribution of key process variables\n",
    "key_vars = ['P1_PIT01', 'P1_LIT01', 'P1_FT03', 'P1_TIT01']\n",
    "key_vars = [var for var in key_vars if var in train_df.columns]\n",
    "\n",
    "if key_vars:\n",
    "    fig, axes = plt.subplots(len(key_vars), 1, figsize=(14, 4*len(key_vars)))\n",
    "    \n",
    "    for i, var in enumerate(key_vars):\n",
    "        sns.histplot(train_df[var], kde=True, ax=axes[i])\n",
    "        axes[i].set_title(f'Distribution of {var}')\n",
    "        axes[i].set_xlabel('Value')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
