{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Feature Selection for HAI-21.03 Dataset\n",
    "\n",
    "This notebook implements advanced feature selection techniques for the HAI-21.03 industrial control system security dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from sklearn.decomposition import PCA, KernelPCA, FastICA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE, RFECV\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Global Variables Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "OUTPUT_DIR = '../hai-security-dataset/processed'\n",
    "FEATURE_DIR = '../hai-security-dataset/features'\n",
    "\n",
    "# Create feature directory if it doesn't exist\n",
    "os.makedirs(FEATURE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Enhanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../hai-security-dataset/processed\\\\train_processed_enhanced.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load processed training data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m train_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain_processed_enhanced.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded enhanced training data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_df.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m columns\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load test data\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\WebstormProjects\\hai-dataset-analysis\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\WebstormProjects\\hai-dataset-analysis\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\WebstormProjects\\hai-dataset-analysis\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\WebstormProjects\\hai-dataset-analysis\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\WebstormProjects\\hai-dataset-analysis\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../hai-security-dataset/processed\\\\train_processed_enhanced.csv'"
     ]
    }
   ],
   "source": [
    "# Load processed training data\n",
    "train_df = pd.read_csv(os.path.join(OUTPUT_DIR, 'train_processed_enhanced.csv'))\n",
    "print(f\"Loaded enhanced training data: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n",
    "\n",
    "# Load test data\n",
    "test_files = [f for f in os.listdir(OUTPUT_DIR) if f.startswith('test') and f.endswith('_processed_enhanced.csv')]\n",
    "test_data = {}\n",
    "\n",
    "for file in test_files:\n",
    "    file_path = os.path.join(OUTPUT_DIR, file)\n",
    "    file_name = file.split('_')[0]  # Extract test file name (e.g., 'test1')\n",
    "    df = pd.read_csv(file_path)\n",
    "    test_data[file_name] = df\n",
    "    print(f\"Loaded {file_name}: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_features(df):\n",
    "    \"\"\"\n",
    "    Filter out non-feature columns and handle problematic features.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (filtered_df, feature_cols, target_col)\n",
    "    \"\"\"\n",
    "    # Exclude non-numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    \n",
    "    # Exclude time and attack columns from features\n",
    "    exclude_cols = [col for col in df.columns if col.startswith('time')]\n",
    "    target_col = 'attack'\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_cols = [col for col in numeric_cols if col not in exclude_cols and col != target_col]\n",
    "    \n",
    "    # Check for constant or near-constant features\n",
    "    variance = df[feature_cols].var()\n",
    "    constant_features = variance[variance < 1e-10].index.tolist()\n",
    "    if constant_features:\n",
    "        print(f\"Removing {len(constant_features)} constant or near-constant features\")\n",
    "        feature_cols = [col for col in feature_cols if col not in constant_features]\n",
    "    \n",
    "    # Check for features with too many NaN values\n",
    "    nan_percentage = df[feature_cols].isna().mean()\n",
    "    high_nan_features = nan_percentage[nan_percentage > 0.1].index.tolist()\n",
    "    if high_nan_features:\n",
    "        print(f\"Removing {len(high_nan_features)} features with >10% NaN values\")\n",
    "        feature_cols = [col for col in feature_cols if col not in high_nan_features]\n",
    "    \n",
    "    # Check for features with infinite values\n",
    "    inf_features = []\n",
    "    for col in feature_cols:\n",
    "        if np.isinf(df[col]).any():\n",
    "            inf_features.append(col)\n",
    "    \n",
    "    if inf_features:\n",
    "        print(f\"Removing {len(inf_features)} features with infinite values\")\n",
    "        feature_cols = [col for col in feature_cols if col not in inf_features]\n",
    "    \n",
    "    print(f\"Selected {len(feature_cols)} valid features out of {len(numeric_cols)} numeric columns\")\n",
    "    \n",
    "    return df[feature_cols + [target_col]], feature_cols, target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter features\n",
    "filtered_train_df, feature_cols, target_col = filter_features(train_df)\n",
    "\n",
    "# Extract features and target\n",
    "X_train = filtered_train_df[feature_cols].values\n",
    "y_train = filtered_train_df[target_col].values\n",
    "\n",
    "print(f\"Training data shape: X={X_train.shape}, y={y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_scalers(X_train):\n",
    "    \"\"\"\n",
    "    Compare different scaling methods.\n",
    "    \n",
    "    Args:\n",
    "        X_train (np.array): Training data\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of scaled data and scalers\n",
    "    \"\"\"\n",
    "    # Define scalers to compare\n",
    "    scalers = {\n",
    "        'StandardScaler': StandardScaler(),\n",
    "        'RobustScaler': RobustScaler(),\n",
    "        'MinMaxScaler': MinMaxScaler(),\n",
    "        'QuantileTransformer': QuantileTransformer(output_distribution='normal')\n",
    "    }\n",
    "    \n",
    "    # Apply each scaler\n",
    "    scaled_data = {}\n",
    "    for name, scaler in scalers.items():\n",
    "        print(f\"Applying {name}...\")\n",
    "        X_scaled = scaler.fit_transform(X_train)\n",
    "        scaled_data[name] = {'data': X_scaled, 'scaler': scaler}\n",
    "    \n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different scaling methods\n",
    "scaled_data = compare_scalers(X_train)\n",
    "\n",
    "# Visualize scaled data distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, (name, data_dict) in enumerate(scaled_data.items()):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    \n",
    "    # Sample a few features for visualization\n",
    "    sample_features = np.random.choice(data_dict['data'].shape[1], min(5, data_dict['data'].shape[1]), replace=False)\n",
    "    \n",
    "    for j in sample_features:\n",
    "        sns.kdeplot(data_dict['data'][:, j], label=f'Feature {j}')\n",
    "    \n",
    "    plt.title(f'{name} Distribution')\n",
    "    plt.xlabel('Scaled Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best scaler (RobustScaler is often good for anomaly detection)\n",
    "selected_scaler_name = 'RobustScaler'\n",
    "X_train_scaled = scaled_data[selected_scaler_name]['data']\n",
    "scaler = scaled_data[selected_scaler_name]['scaler']\n",
    "\n",
    "print(f\"Selected {selected_scaler_name} for feature scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filter-Based Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filter_methods(X_train_scaled, y_train, feature_cols, k=50):\n",
    "    \"\"\"\n",
    "    Apply filter-based feature selection methods.\n",
    "    \n",
    "    Args:\n",
    "        X_train_scaled (np.array): Scaled training data\n",
    "        y_train (np.array): Target values\n",
    "        feature_cols (list): Feature column names\n",
    "        k (int): Number of features to select\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of selected features for each method\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Variance Threshold\n",
    "    print(\"Applying Variance Threshold...\")\n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    X_var = selector.fit_transform(X_train_scaled)\n",
    "    var_support = selector.get_support()\n",
    "    var_features = [feature_cols[i] for i in range(len(feature_cols)) if var_support[i]]\n",
    "    results['variance_threshold'] = {\n",
    "        'support': var_support,\n",
    "        'features': var_features,\n",
    "        'scores': selector.variances_,\n",
    "        'data': X_var\n",
    "    }\n",
    "    print(f\"  Selected {len(var_features)} features\")\n",
    "    \n",
    "    # 2. ANOVA F-value\n",
    "    print(\"Applying ANOVA F-value...\")\n",
    "    selector = SelectKBest(f_classif, k=min(k, X_train_scaled.shape[1]))\n",
    "    X_anova = selector.fit_transform(X_train_scaled, y_train)\n",
    "    anova_support = selector.get_support()\n",
    "    anova_features = [feature_cols[i] for i in range(len(feature_cols)) if anova_support[i]]\n",
    "    results['anova'] = {\n",
    "        'support': anova_support,\n",
    "        'features': anova_features,\n",
    "        'scores': selector.scores_,\n",
    "        'data': X_anova\n",
    "    }\n",
    "    print(f\"  Selected {len(anova_features)} features\")\n",
    "    \n",
    "    # 3. Mutual Information\n",
    "    print(\"Applying Mutual Information...\")\n",
    "    selector = SelectKBest(mutual_info_classif, k=min(k, X_train_scaled.shape[1]))\n",
    "    X_mi = selector.fit_transform(X_train_scaled, y_train)\n",
    "    mi_support = selector.get_support()\n",
    "    mi_features = [feature_cols[i] for i in range(len(feature_cols)) if mi_support[i]]\n",
    "    results['mutual_info'] = {\n",
    "        'support': mi_support,\n",
    "        'features': mi_features,\n",
    "        'scores': selector.scores_,\n",
    "        'data': X_mi\n",
    "    }\n",
    "    print(f\"  Selected {len(mi_features)} features\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filter-based methods\n",
    "filter_results = apply_filter_methods(X_train_scaled, y_train, feature_cols, k=50)\n",
    "\n",
    "# Visualize feature importance for each method\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# 1. Variance Threshold\n",
    "plt.subplot(3, 1, 1)\n",
    "var_scores = filter_results['variance_threshold']['scores']\n",
    "var_features = feature_cols\n",
    "sorted_idx = np.argsort(var_scores)[::-1]\n",
    "plt.bar(range(min(20, len(var_features))), var_scores[sorted_idx[:20]])\n",
    "plt.xticks(range(min(20, len(var_features))), [var_features[i] for i in sorted_idx[:20]], rotation=90)\n",
    "plt.title('Top 20 Features by Variance')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Variance')\n",
    "\n",
    "# 2. ANOVA F-value\n",
    "plt.subplot(3, 1, 2)\n",
    "anova_scores = filter_results['anova']['scores']\n",
    "anova_support = filter_results['anova']['support']\n",
    "anova_features = [feature_cols[i] for i in range(len(feature_cols)) if anova_support[i]]\n",
    "sorted_idx = np.argsort(anova_scores)[::-1]\n",
    "plt.bar(range(min(20, len(anova_features))), [anova_scores[i] for i in sorted_idx[:20]])\n",
    "plt.xticks(range(min(20, len(anova_features))), [feature_cols[i] for i in sorted_idx[:20]], rotation=90)\n",
    "plt.title('Top 20 Features by ANOVA F-value')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('F-value')\n",
    "\n",
    "# 3. Mutual Information\n",
    "plt.subplot(3, 1, 3)\n",
    "mi_scores = filter_results['mutual_info']['scores']\n",
    "mi_support = filter_results['mutual_info']['support']\n",
    "mi_features = [feature_cols[i] for i in range(len(feature_cols)) if mi_support[i]]\n",
    "sorted_idx = np.argsort(mi_scores)[::-1]\n",
    "plt.bar(range(min(20, len(mi_features))), [mi_scores[i] for i in sorted_idx[:20]])\n",
    "plt.xticks(range(min(20, len(mi_features))), [feature_cols[i] for i in sorted_idx[:20]], rotation=90)\n",
    "plt.title('Top 20 Features by Mutual Information')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Mutual Information')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Wrapper-Based Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_wrapper_methods(X_train_scaled, y_train, feature_cols):\n",
    "    \"\"\"\n",
    "    Apply wrapper-based feature selection methods.\n",
    "    \n",
    "    Args:\n",
    "        X_train_scaled (np.array): Scaled training data\n",
    "        y_train (np.array): Target values\n",
    "        feature_cols (list): Feature column names\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of selected features for each method\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Recursive Feature Elimination (RFE)\n",
    "    print(\"Applying Recursive Feature Elimination...\")\n",
    "    estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    selector = RFE(estimator, n_features_to_select=min(50, X_train_scaled.shape[1]), step=0.1)\n",
    "    X_rfe = selector.fit_transform(X_train_scaled, y_train)\n",
    "    rfe_support = selector.get_support()\n",
    "    rfe_features = [feature_cols[i] for i in range(len(feature_cols)) if rfe_support[i]]\n",
    "    results['rfe'] = {\n",
    "        'support': rfe_support,\n",
    "        'features': rfe_features,\n",
    "        'ranking': selector.ranking_,\n",
    "        'data': X_rfe\n",
    "    }\n",
    "    print(f\"  Selected {len(rfe_features)} features\")\n",
    "    \n",
    "    # 2. Random Forest Feature Importance\n",
    "    print(\"Applying Random Forest Feature Importance...\")\n",
    "    estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    estimator.fit(X_train_scaled, y_train)\n",
    "    rf_importances = estimator.feature_importances_\n",
    "    rf_indices = np.argsort(rf_importances)[::-1]\n",
    "    rf_features = [feature_cols[i] for i in rf_indices[:50]]\n",
    "    results['random_forest'] = {\n",
    "        'features': rf_features,\n",
    "        'importances': rf_importances,\n",
    "        'indices': rf_indices\n",
    "    }\n",
    "    print(f\"  Selected {len(rf_features)} features\")\n",
    "    \n",
    "    # 3. Gradient Boosting Feature Importance\n",
    "    print(\"Applying Gradient Boosting Feature Importance...\")\n",
    "    estimator = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "    estimator.fit(X_train_scaled, y_train)\n",
    "    gb_importances = estimator.feature_importances_\n",
    "    gb_indices = np.argsort(gb_importances)[::-1]\n",
    "    gb_features = [feature_cols[i] for i in gb_indices[:50]]\n",
    "    results['gradient_boosting'] = {\n",
    "        'features': gb_features,\n",
    "        'importances': gb_importances,\n",
    "        'indices': gb_indices\n",
    "    }\n",
    "    print(f\"  Selected {len(gb_features)} features\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply wrapper-based methods\n",
    "wrapper_results = apply_wrapper_methods(X_train_scaled, y_train, feature_cols)\n",
    "\n",
    "# Visualize feature importance for each method\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# 1. RFE\n",
    "plt.subplot(3, 1, 1)\n",
    "rfe_ranking = wrapper_results['rfe']['ranking']\n",
    "sorted_idx = np.argsort(rfe_ranking)\n",
    "plt.bar(range(20), [rfe_ranking[i] for i in sorted_idx[:20]])\n",
    "plt.xticks(range(20), [feature_cols[i] for i in sorted_idx[:20]], rotation=90)\n",
    "plt.title('Top 20 Features by RFE (lower rank is better)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Ranking')\n",
    "\n",
    "# 2. Random Forest\n",
    "plt.subplot(3, 1, 2)\n",
    "rf_importances = wrapper_results['random_forest']['importances']\n",
    "rf_indices = wrapper_results['random_forest']['indices']\n",
    "plt.bar(range(20), rf_importances[rf_indices[:20]])\n",
    "plt.xticks(range(20), [feature_cols[i] for i in rf_indices[:20]], rotation=90)\n",
    "plt.title('Top 20 Features by Random Forest Importance')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "\n",
    "# 3. Gradient Boosting\n",
    "plt.subplot(3, 1, 3)\n",
    "gb_importances = wrapper_results['gradient_boosting']['importances']\n",
    "gb_indices = wrapper_results['gradient_boosting']['indices']\n",
    "plt.bar(range(20), gb_importances[gb_indices[:20]])\n",
    "plt.xticks(range(20), [feature_cols[i] for i in gb_indices[:20]], rotation=90)\n",
    "plt.title('Top 20 Features by Gradient Boosting Importance')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Embedded-Based Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_embedded_methods(X_train_scaled, y_train, feature_cols):\n",
    "    \"\"\"\n",
    "    Apply embedded feature selection methods.\n",
    "    \n",
    "    Args:\n",
    "        X_train_scaled (np.array): Scaled training data\n",
    "        y_train (np.array): Target values\n",
    "        feature_cols (list): Feature column names\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of selected features for each method\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Lasso\n",
    "    print(\"Applying Lasso...\")\n",
    "    lasso = Lasso(alpha=0.01)\n",
    "    lasso.fit(X_train_scaled, y_train)\n",
    "    lasso_coef = np.abs(lasso.coef_)\n",
    "    lasso_indices = np.argsort(lasso_coef)[::-1]\n",
    "    lasso_features = [feature_cols[i] for i in lasso_indices[:50]]\n",
    "    results['lasso'] = {\n",
    "        'features': lasso_features,\n",
    "        'coefficients': lasso_coef,\n",
    "        'indices': lasso_indices\n",
    "    }\n",
    "    print(f\"  Selected {len(lasso_features)} features\")\n",
    "    \n",
    "    # 2. SelectFromModel with Random Forest\n",
    "    print(\"Applying SelectFromModel with Random Forest...\")\n",
    "    estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    selector = SelectFromModel(estimator, threshold='median')\n",
    "    X_sfm = selector.fit_transform(X_train_scaled, y_train)\n",
    "    sfm_support = selector.get_support()\n",
    "    sfm_features = [feature_cols[i] for i in range(len(feature_cols)) if sfm_support[i]]\n",
    "    results['select_from_model'] = {\n",
    "        'support': sfm_support,\n",
    "        'features': sfm_features,\n",
    "        'data': X_sfm\n",
    "    }\n",
    "    print(f\"  Selected {len(sfm_features)} features\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply embedded-based methods\n",
    "embedded_results = apply_embedded_methods(X_train_scaled, y_train, feature_cols)\n",
    "\n",
    "# Visualize feature importance for each method\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 1. Lasso\n",
    "plt.subplot(2, 1, 1)\n",
    "lasso_coef = embedded_results['lasso']['coefficients']\n",
    "lasso_indices = embedded_results['lasso']['indices']\n",
    "plt.bar(range(20), lasso_coef[lasso_indices[:20]])\n",
    "plt.xticks(range(20), [feature_cols[i] for i in lasso_indices[:20]], rotation=90)\n",
    "plt.title('Top 20 Features by Lasso Coefficients')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Coefficient')\n",
    "\n",
    "# 2. SelectFromModel\n",
    "plt.subplot(2, 1, 2)\n",
    "sfm_support = embedded_results['select_from_model']['support']\n",
    "sfm_features = embedded_results['select_from_model']['features']\n",
    "plt.bar(range(len(sfm_features)), [1] * len(sfm_features))\n",
    "plt.xticks(range(len(sfm_features)), sfm_features, rotation=90)\n",
    "plt.title('Features Selected by SelectFromModel')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Selected')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Ranking and Ensemble Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_feature_selection(filter_results, wrapper_results, embedded_results, feature_cols):\n",
    "    \"\"\"\n",
    "    Combine results from different feature selection methods.\n",
    "    \n",
    "    Args:\n",
    "        filter_results (dict): Results from filter-based methods\n",
    "        wrapper_results (dict): Results from wrapper-based methods\n",
    "        embedded_results (dict): Results from embedded-based methods\n",
    "        feature_cols (list): Feature column names\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (ensemble_features, feature_scores)\n",
    "    \"\"\"\n",
    "    # Initialize feature scores\n",
    "    feature_scores = {feature: 0 for feature in feature_cols}\n",
    "    \n",
    "    # Collect all selected features\n",
    "    all_selected_features = []\n",
    "    \n",
    "    # Add filter-based features\n",
    "    all_selected_features.extend(filter_results['anova']['features'])\n",
    "    all_selected_features.extend(filter_results['mutual_info']['features'])\n",
    "    \n",
    "    # Add wrapper-based features\n",
    "    all_selected_features.extend(wrapper_results['rfe']['features'])\n",
    "    all_selected_features.extend(wrapper_results['random_forest']['features'])\n",
    "    all_selected_features.extend(wrapper_results['gradient_boosting']['features'])\n",
    "    \n",
    "    # Add embedded-based features\n",
    "    all_selected_features.extend(embedded_results['lasso']['features'])\n",
    "    all_selected_features.extend(embedded_results['select_from_model']['features'])\n",
    "    \n",
    "    # Count occurrences of each feature\n",
    "    for feature in all_selected_features:\n",
    "        feature_scores[feature] += 1\n",
    "    \n",
    "    # Sort features by score\n",
    "    sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Select top features\n",
    "    top_n = 50\n",
    "    ensemble_features = [feature for feature, score in sorted_features[:top_n]]\n",
    "    \n",
    "    return ensemble_features, feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ensemble feature selection\n",
    "ensemble_features, feature_scores = ensemble_feature_selection(\n",
    "    filter_results, wrapper_results, embedded_results, feature_cols\n",
    ")\n",
    "\n",
    "print(f\"Selected {len(ensemble_features)} features using ensemble method\")\n",
    "\n",
    "# Visualize top features\n",
    "plt.figure(figsize=(15, 8))\n",
    "sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)[:30]\n",
    "plt.bar([f[0] for f in sorted_features], [f[1] for f in sorted_features])\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Top 30 Features by Ensemble Selection')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Score (number of methods that selected the feature)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dimensionality_reduction(X_train_scaled, ensemble_features, feature_cols):\n",
    "    \"\"\"\n",
    "    Apply dimensionality reduction techniques.\n",
    "    \n",
    "    Args:\n",
    "        X_train_scaled (np.array): Scaled training data\n",
    "        ensemble_features (list): Selected features from ensemble method\n",
    "        feature_cols (list): Feature column names\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of reduced data and transformers\n",
    "    \"\"\"\n",
    "    # Get indices of ensemble features\n",
    "    ensemble_indices = [feature_cols.index(feature) for feature in ensemble_features]\n",
    "    X_ensemble = X_train_scaled[:, ensemble_indices]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. PCA\n",
    "    print(\"Applying PCA...\")\n",
    "    n_components = min(20, X_ensemble.shape[1])\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_ensemble)\n",
    "    explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "    results['pca'] = {\n",
    "        'data': X_pca,\n",
    "        'transformer': pca,\n",
    "        'explained_variance': explained_variance\n",
    "    }\n",
    "    print(f\"  Reduced to {n_components} components with {explained_variance:.4f} explained variance\")\n",
    "    \n",
    "    # 2. Kernel PCA\n",
    "    print(\"Applying Kernel PCA...\")\n",
    "    kpca = KernelPCA(n_components=n_components, kernel='rbf')\n",
    "    X_kpca = kpca.fit_transform(X_ensemble)\n",
    "    results['kpca'] = {\n",
    "        'data': X_kpca,\n",
    "        'transformer': kpca\n",
    "    }\n",
    "    print(f\"  Reduced to {n_components} components\")\n",
    "    \n",
    "    # 3. FastICA\n",
    "    print(\"Applying FastICA...\")\n",
    "    ica = FastICA(n_components=n_components, random_state=42)\n",
    "    X_ica = ica.fit_transform(X_ensemble)\n",
    "    results['ica'] = {\n",
    "        'data': X_ica,\n",
    "        'transformer': ica\n",
    "    }\n",
    "    print(f\"  Reduced to {n_components} components\")\n",
    "    \n",
    "    return results, ensemble_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction\n",
    "reduction_results, ensemble_indices = apply_dimensionality_reduction(\n",
    "    X_train_scaled, ensemble_features, feature_cols\n",
    ")\n",
    "\n",
    "# Visualize PCA components\n",
    "plt.figure(figsize=(10, 6))\n",
    "pca = reduction_results['pca']['transformer']\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize first two components of each method\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 1. PCA\n",
    "plt.subplot(1, 3, 1)\n",
    "X_pca = reduction_results['pca']['data']\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train, cmap='viridis', alpha=0.5)\n",
    "plt.title('PCA')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.colorbar(label='Attack')\n",
    "\n",
    "# 2. Kernel PCA\n",
    "plt.subplot(1, 3, 2)\n",
    "X_kpca = reduction_results['kpca']['data']\n",
    "plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=y_train, cmap='viridis', alpha=0.5)\n",
    "plt.title('Kernel PCA')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.colorbar(label='Attack')\n",
    "\n",
    "# 3. FastICA\n",
    "plt.subplot(1, 3, 3)\n",
    "X_ica = reduction_results['ica']['data']\n",
    "plt.scatter(X_ica[:, 0], X_ica[:, 1], c=y_train, cmap='viridis', alpha=0.5)\n",
    "plt.title('FastICA')\n",
    "plt.xlabel('Component 1')\n",
    "plt.ylabel('Component 2')\n",
    "plt.colorbar(label='Attack')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Selected Features and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save selected features\n",
    "feature_selection_results = {\n",
    "    'ensemble_features': ensemble_features,\n",
    "    'feature_scores': feature_scores,\n",
    "    'ensemble_indices': ensemble_indices,\n",
    "    'scaler': scaler,\n",
    "    'pca': reduction_results['pca']['transformer'],\n",
    "    'kpca': reduction_results['kpca']['transformer'],\n",
    "    'ica': reduction_results['ica']['transformer']\n",
    "}\n",
    "\n",
    "with open(os.path.join(FEATURE_DIR, 'feature_selection_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(feature_selection_results, f)\n",
    "\n",
    "print(f\"Saved feature selection results to {os.path.join(FEATURE_DIR, 'feature_selection_results.pkl')}\")\n",
    "\n",
    "# Save feature names\n",
    "with open(os.path.join(FEATURE_DIR, 'selected_features.txt'), 'w') as f:\n",
    "    for feature in ensemble_features:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "\n",
    "print(f\"Saved selected feature names to {os.path.join(FEATURE_DIR, 'selected_features.txt')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature selection to test data\n",
    "test_data_selected = {}\n",
    "\n",
    "for name, df in test_data.items():\n",
    "    print(f\"Applying feature selection to {name}...\")\n",
    "    \n",
    "    # Extract features and target\n",
    "    X_test = df[feature_cols].values\n",
    "    y_test = df[target_col].values if target_col in df.columns else None\n",
    "    \n",
    "    # Scale features\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Select ensemble features\n",
    "    X_test_ensemble = X_test_scaled[:, ensemble_indices]\n",
    "    \n",
    "    # Apply PCA\n",
    "    X_test_pca = reduction_results['pca']['transformer'].transform(X_test_ensemble)\n",
    "    \n",
    "    test_data_selected[name] = {\n",
    "        'X_scaled': X_test_scaled,\n",
    "        'X_ensemble': X_test_ensemble,\n",
    "        'X_pca': X_test_pca,\n",
    "        'y': y_test\n",
    "    }\n",
    "    \n",
    "    print(f\"  Processed {name}: {X_test.shape} -> {X_test_ensemble.shape} -> {X_test_pca.shape}\")\n",
    "\n",
    "# Save processed test data\n",
    "with open(os.path.join(FEATURE_DIR, 'test_data_selected.pkl'), 'wb') as f:\n",
    "    pickle.dump(test_data_selected, f)\n",
    "\n",
    "print(f\"Saved selected test data to {os.path.join(FEATURE_DIR, 'test_data_selected.pkl')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
