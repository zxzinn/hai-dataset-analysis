{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eTaPR Evaluation for HAI-21.03 Dataset\n",
    "\n",
    "This notebook implements evaluation using eTaPR (Time-series Aware Precision and Recall) metrics for the HAI-21.03 industrial control system security dataset. eTaPR is specifically designed for time series anomaly detection evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Global Variables Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "OUTPUT_DIR = 'hai-security-dataset/processed'\n",
    "MODEL_DIR = 'hai-security-dataset/models'\n",
    "RESULTS_DIR = 'hai-security-dataset/results'\n",
    "ETAPR_DIR = 'eTaPR'\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import eTaPR Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add eTaPR directory to path\n",
    "sys.path.append(ETAPR_DIR)\n",
    "\n",
    "# Import eTaPR\n",
    "try:\n",
    "    from eTaPR_pkg import etapr\n",
    "    print(\"Successfully imported eTaPR package\")\n",
    "except ImportError:\n",
    "    print(\"Failed to import eTaPR package. Installing from wheel file...\")\n",
    "    \n",
    "    # Install eTaPR package\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", os.path.join(ETAPR_DIR, \"eTaPR-22.6.1-py3-none-any.whl\")])\n",
    "    \n",
    "    # Try importing again\n",
    "    try:\n",
    "        from eTaPR_pkg import etapr\n",
    "        print(\"Successfully installed and imported eTaPR package\")\n",
    "    except ImportError:\n",
    "        print(\"Failed to install and import eTaPR package. Please install manually.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load operating point results\n",
    "with open(os.path.join(MODEL_DIR, 'operating_point_results.pkl'), 'rb') as f:\n",
    "    operating_point_results = pickle.load(f)\n",
    "\n",
    "print(f\"Loaded operating point results for {len(operating_point_results)} operating points\")\n",
    "\n",
    "# Load test results\n",
    "test_results_path = os.path.join(RESULTS_DIR, 'test_results.csv')\n",
    "if os.path.exists(test_results_path):\n",
    "    test_results = pd.read_csv(test_results_path)\n",
    "    print(f\"Loaded test results for {len(test_results)} test files\")\n",
    "else:\n",
    "    print(\"Test results file not found. Will generate results from scratch.\")\n",
    "    test_results = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detector\n",
    "with open(os.path.join(MODEL_DIR, 'multi_threshold_detector.pkl'), 'rb') as f:\n",
    "    detector = pickle.load(f)\n",
    "\n",
    "print(\"Loaded multi-threshold detector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_files = [f for f in os.listdir(OUTPUT_DIR) if f.startswith('test') and f.endswith('_processed_enhanced.csv')]\n",
    "test_data = {}\n",
    "\n",
    "for file in test_files:\n",
    "    file_path = os.path.join(OUTPUT_DIR, file)\n",
    "    file_name = file.split('_')[0]  # Extract test file name (e.g., 'test1')\n",
    "    df = pd.read_csv(file_path)\n",
    "    test_data[file_name] = df\n",
    "    print(f\"Loaded {file_name}: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Data for eTaPR Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_etapr_data(ground_truth, predictions, test_name, operating_point):\n",
    "    \"\"\"\n",
    "    Prepare data for eTaPR evaluation.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth (np.array): Ground truth labels\n",
    "        predictions (np.array): Predicted labels\n",
    "        test_name (str): Name of the test file\n",
    "        operating_point (str): Operating point used\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (ground_truth_ranges, prediction_ranges)\n",
    "    \"\"\"\n",
    "    # Convert binary labels to ranges (start, end) for ground truth\n",
    "    ground_truth_ranges = []\n",
    "    i = 0\n",
    "    while i < len(ground_truth):\n",
    "        if ground_truth[i] == 1:\n",
    "            start = i\n",
    "            while i < len(ground_truth) and ground_truth[i] == 1:\n",
    "                i += 1\n",
    "            end = i - 1\n",
    "            ground_truth_ranges.append((start, end))\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    # Convert binary labels to ranges (start, end) for predictions\n",
    "    prediction_ranges = []\n",
    "    i = 0\n",
    "    while i < len(predictions):\n",
    "        if predictions[i] == 1:\n",
    "            start = i\n",
    "            while i < len(predictions) and predictions[i] == 1:\n",
    "                i += 1\n",
    "            end = i - 1\n",
    "            prediction_ranges.append((start, end))\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Test: {test_name}, Operating Point: {operating_point}\")\n",
    "    print(f\"Ground Truth: {len(ground_truth_ranges)} anomaly ranges\")\n",
    "    print(f\"Predictions: {len(prediction_ranges)} anomaly ranges\")\n",
    "    \n",
    "    return ground_truth_ranges, prediction_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_etapr_files(ground_truth_ranges, prediction_ranges, test_name, operating_point):\n",
    "    \"\"\"\n",
    "    Generate files for eTaPR evaluation.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth_ranges (list): List of (start, end) tuples for ground truth\n",
    "        prediction_ranges (list): List of (start, end) tuples for predictions\n",
    "        test_name (str): Name of the test file\n",
    "        operating_point (str): Operating point used\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (ground_truth_file, prediction_file)\n",
    "    \"\"\"\n",
    "    # Create directory for eTaPR files\n",
    "    etapr_files_dir = os.path.join(RESULTS_DIR, 'etapr_files')\n",
    "    os.makedirs(etapr_files_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate file names\n",
    "    ground_truth_file = os.path.join(etapr_files_dir, f\"{test_name}_ground_truth.csv\")\n",
    "    prediction_file = os.path.join(etapr_files_dir, f\"{test_name}_{operating_point}_predictions.csv\")\n",
    "    \n",
    "    # Write ground truth file\n",
    "    with open(ground_truth_file, 'w') as f:\n",
    "        for start, end in ground_truth_ranges:\n",
    "            f.write(f\"{start},{end}\\n\")\n",
    "    \n",
    "    # Write prediction file\n",
    "    with open(prediction_file, 'w') as f:\n",
    "        for start, end in prediction_ranges:\n",
    "            f.write(f\"{start},{end}\\n\")\n",
    "    \n",
    "    print(f\"Generated eTaPR files: {ground_truth_file}, {prediction_file}\")\n",
    "    \n",
    "    return ground_truth_file, prediction_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate eTaPR data for operating point results\n",
    "etapr_data = {}\n",
    "\n",
    "# Get first test file from operating point results\n",
    "test_name = list(operating_point_results.values())[0]['test_name']\n",
    "print(f\"Generating eTaPR data for test file: {test_name}\")\n",
    "\n",
    "for operating_point, result in operating_point_results.items():\n",
    "    # Prepare data\n",
    "    ground_truth = result['ground_truth']\n",
    "    predictions = result['anomaly_labels']\n",
    "    \n",
    "    # Convert to ranges\n",
    "    ground_truth_ranges, prediction_ranges = prepare_etapr_data(ground_truth, predictions, test_name, operating_point)\n",
    "    \n",
    "    # Generate files\n",
    "    ground_truth_file, prediction_file = generate_etapr_files(ground_truth_ranges, prediction_ranges, test_name, operating_point)\n",
    "    \n",
    "    # Store data\n",
    "    etapr_data[operating_point] = {\n",
    "        'test_name': test_name,\n",
    "        'ground_truth_ranges': ground_truth_ranges,\n",
    "        'prediction_ranges': prediction_ranges,\n",
    "        'ground_truth_file': ground_truth_file,\n",
    "        'prediction_file': prediction_file\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run eTaPR Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_etapr_evaluation(ground_truth_file, prediction_file, theta=0.5):\n",
    "    \"\"\"\n",
    "    Run eTaPR evaluation.\n",
    "    \n",
    "    Args:\n",
    "        ground_truth_file (str): Path to ground truth file\n",
    "        prediction_file (str): Path to prediction file\n",
    "        theta (float): Theta parameter for eTaPR\n",
    "        \n",
    "    Returns:\n",
    "        dict: eTaPR results\n",
    "    \"\"\"\n",
    "    # Run eTaPR evaluation\n",
    "    try:\n",
    "        # Create eTaPR object\n",
    "        etapr_obj = etapr.eTaPR()\n",
    "        \n",
    "        # Load ground truth and predictions\n",
    "        etapr_obj.load_anomalies(ground_truth_file)\n",
    "        etapr_obj.load_predictions(prediction_file)\n",
    "        \n",
    "        # Set theta parameter\n",
    "        etapr_obj.set_theta(theta)\n",
    "        \n",
    "        # Calculate eTaPR metrics\n",
    "        etapr_obj.calculate()\n",
    "        \n",
    "        # Get results\n",
    "        precision = etapr_obj.precision\n",
    "        recall = etapr_obj.recall\n",
    "        f1 = etapr_obj.f1\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"eTaPR Results (theta={theta})\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        \n",
    "        # Return results\n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'theta': theta\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error running eTaPR evaluation: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run eTaPR evaluation for each operating point\n",
    "etapr_results = {}\n",
    "\n",
    "for operating_point, data in etapr_data.items():\n",
    "    print(f\"\\nRunning eTaPR evaluation for operating point: {operating_point}\")\n",
    "    \n",
    "    # Run evaluation with different theta values\n",
    "    theta_results = {}\n",
    "    for theta in [0.1, 0.3, 0.5, 0.7, 0.9]:\n",
    "        result = run_etapr_evaluation(data['ground_truth_file'], data['prediction_file'], theta=theta)\n",
    "        if result:\n",
    "            theta_results[theta] = result\n",
    "    \n",
    "    # Store results\n",
    "    etapr_results[operating_point] = theta_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize eTaPR Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with eTaPR results\n",
    "etapr_df = []\n",
    "\n",
    "for operating_point, theta_results in etapr_results.items():\n",
    "    for theta, result in theta_results.items():\n",
    "        etapr_df.append({\n",
    "            'Operating Point': operating_point,\n",
    "            'Theta': theta,\n",
    "            'Precision': result['precision'],\n",
    "            'Recall': result['recall'],\n",
    "            'F1 Score': result['f1_score']\n",
    "        })\n",
    "\n",
    "etapr_df = pd.DataFrame(etapr_df)\n",
    "etapr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize eTaPR results for different operating points (theta=0.5)\n",
    "theta_05_df = etapr_df[etapr_df['Theta'] == 0.5]\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot precision, recall, and F1 score\n",
    "metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.bar(theta_05_df['Operating Point'], theta_05_df[metric])\n",
    "    plt.title(f'{metric} (eTaPR, theta=0.5)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, v in enumerate(theta_05_df[metric]):\n",
    "        plt.text(j, v + 0.02, f\"{v:.4f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize eTaPR results for different theta values (balanced operating point)\n",
    "balanced_df = etapr_df[etapr_df['Operating Point'] == 'balanced']\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot precision, recall, and F1 score\n",
    "metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.plot(balanced_df['Theta'], balanced_df[metric], marker='o')\n",
    "    plt.title(f'{metric} vs. Theta (eTaPR, balanced)')\n",
    "    plt.xlabel('Theta')\n",
    "    plt.ylabel(metric)\n",
    "    plt.grid(True)\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare eTaPR with Traditional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load threshold comparison results\n",
    "threshold_comparison_path = os.path.join(RESULTS_DIR, 'threshold_comparison.csv')\n",
    "if os.path.exists(threshold_comparison_path):\n",
    "    traditional_df = pd.read_csv(threshold_comparison_path)\n",
    "    print(f\"Loaded threshold comparison results for {len(traditional_df)} operating points\")\n",
    "else:\n",
    "    print(\"Threshold comparison file not found. Using operating point results.\")\n",
    "    \n",
    "    # Create DataFrame from operating point results\n",
    "    traditional_df = []\n",
    "    for operating_point, result in operating_point_results.items():\n",
    "        traditional_df.append({\n",
    "            'Operating Point': operating_point,\n",
    "            'Precision': result['precision'],\n",
    "            'Recall': result['recall'],\n",
    "            'F1 Score': result['f1_score']\n",
    "        })\n",
    "    traditional_df = pd.DataFrame(traditional_df)\n",
    "\n",
    "traditional_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge traditional and eTaPR results (theta=0.5)\n",
    "comparison_df = pd.merge(traditional_df, theta_05_df, on='Operating Point', suffixes=('_Traditional', '_eTaPR'))\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison between traditional and eTaPR metrics\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# Plot precision, recall, and F1 score\n",
    "metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(3, 1, i+1)\n",
    "    \n",
    "    # Plot traditional metrics\n",
    "    plt.bar(np.arange(len(comparison_df)) - 0.2, comparison_df[f'{metric}_Traditional'], width=0.4, label='Traditional')\n",
    "    \n",
    "    # Plot eTaPR metrics\n",
    "    plt.bar(np.arange(len(comparison_df)) + 0.2, comparison_df[f'{metric}_eTaPR'], width=0.4, label='eTaPR (theta=0.5)')\n",
    "    \n",
    "    plt.title(f'Comparison of {metric}')\n",
    "    plt.xticks(np.arange(len(comparison_df)), comparison_df['Operating Point'], rotation=45)\n",
    "    plt.ylabel(metric)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, v in enumerate(comparison_df[f'{metric}_Traditional']):\n",
    "        plt.text(j - 0.2, v + 0.02, f\"{v:.2f}\", ha='center')\n",
    "    \n",
    "    for j, v in enumerate(comparison_df[f'{metric}_eTaPR']):\n",
    "        plt.text(j + 0.2, v + 0.02, f\"{v:.2f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate All Test Files with eTaPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_file_etapr(test_name, test_df, operating_point='balanced', theta=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate a test file using eTaPR metrics.\n",
    "    \n",
    "    Args:\n",
    "        test_name (str): Name of the test file\n",
    "        test_df (pd.DataFrame): Test dataframe\n",
    "        operating_point (str): Operating point to use\n",
    "        theta (float): Theta parameter for eTaPR\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {test_name} with operating point: {operating_point}, theta: {theta}\")\n",
    "    \n",
    "    # Extract features and target\n",
    "    feature_cols = [col for col in test_df.columns if col not in ['time', 'attack']]\n",
    "    X_test = test_df[feature_cols].values\n",
    "    y_test = test_df['attack'].values\n",
    "    \n",
    "    # Predict anomalies\n",
    "    anomaly_scores, anomaly_labels, threshold = detector.predict(X_test, operating_point=operating_point)\n",
    "    \n",
    "    # Apply post-processing\n",
    "    processed_labels = detector.post_process_anomalies(anomaly_labels)\n",
    "    \n",
    "    # Prepare data for eTaPR\n",
    "    ground_truth_ranges, prediction_ranges = prepare_etapr_data(y_test, processed_labels, test_name, operating_point)\n",
    "    \n",
    "    # Generate files for eTaPR\n",
    "    ground_truth_file, prediction_file = generate_etapr_files(ground_truth_ranges, prediction_ranges, test_name, operating_point)\n",
    "    \n",
    "    # Run eTaPR evaluation\n",
    "    etapr_result = run_etapr_evaluation(ground_truth_file, prediction_file, theta=theta)\n",
    "    \n",
    "    if etapr_result:\n",
    "        return {\n",
    "            'test_name': test_name,\n",
    "            'operating_point': operating_point,\n",
    "            'theta': theta,\n",
    "            'precision': etapr_result['precision'],\n",
    "            'recall': etapr_result['recall'],\n",
    "            'f1_score': etapr_result['f1_score'],\n",
    "            'ground_truth_ranges': ground_truth_ranges,\n",
    "            'prediction_ranges': prediction_ranges\n",
    "        }\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all test files with eTaPR\n",
    "best_operating_point = 'balanced'  # Use the balanced operating point\n",
    "theta = 0.5  # Use theta=0.5\n",
    "\n",
    "all_etapr_results = []\n",
    "for test_name, test_df in test_data.items():\n",
    "    result = evaluate_test_file_etapr(test_name, test_df, operating_point=best_operating_point, theta=theta)\n",
    "    if result:\n",
    "        all_etapr_results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with all eTaPR results\n",
    "all_etapr_df = pd.DataFrame([\n",
    "    {\n",
    "        'Test File': result['test_name'],\n",
    "        'Operating Point': result['operating_point'],\n",
    "        'Theta': result['theta'],\n",
    "        'Precision': result['precision'],\n",
    "        'Recall': result['recall'],\n",
    "        'F1 Score': result['f1_score']\n",
    "    }\n",
    "    for result in all_etapr_results\n",
    "])\n",
    "\n",
    "all_etapr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize eTaPR results across test files\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot precision, recall, and F1 score\n",
    "metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.bar(all_etapr_df['Test File'], all_etapr_df[metric])\n",
    "    plt.title(f'{metric} (eTaPR, theta={theta})')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, v in enumerate(all_etapr_df[metric]):\n",
    "        plt.text(j, v + 0.02, f\"{v:.4f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average eTaPR metrics\n",
    "avg_etapr_metrics = {\n",
    "    'Precision': all_etapr_df['Precision'].mean(),\n",
    "    'Recall': all_etapr_df['Recall'].mean(),\n",
    "    'F1 Score': all_etapr_df['F1 Score'].mean()\n",
    "}\n",
    "\n",
    "# Display average metrics\n",
    "print(f\"Average eTaPR Metrics Across All Test Files (theta={theta}):\")\n",
    "for metric, value in avg_etapr_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare eTaPR with Traditional Metrics Across Test Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test results with traditional metrics\n",
    "if test_results is not None:\n",
    "    # Merge with eTaPR results\n",
    "    test_comparison = pd.merge(test_results, all_etapr_df, left_on='Test File', right_on='Test File', suffixes=('_Traditional', '_eTaPR'))\n",
    "    \n",
    "    # Display comparison\n",
    "    test_comparison[['Test File', 'Precision_Traditional', 'Precision_eTaPR', 'Recall_Traditional', 'Recall_eTaPR', 'F1 Score_Traditional', 'F1 Score_eTaPR']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison between traditional and eTaPR metrics across test files\n",
    "if test_results is not None:\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    # Plot precision, recall, and F1 score\n",
    "    metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "    for i, metric in enumerate(metrics):\n",
    "        plt.subplot(3, 1, i+1)\n",
    "        \n",
    "        # Plot traditional metrics\n",
    "        plt.bar(np.arange(len(test_comparison)) - 0.2, test_comparison[f'{metric}_Traditional'], width=0.4, label='Traditional')\n",
    "        \n",
    "        # Plot eTaPR metrics\n",
    "        plt.bar(np.arange(len(test_comparison)) + 0.2, test_comparison[f'{metric}_eTaPR'], width=0.4, label=f'eTaPR (theta={theta})')\n",
    "        \n",
    "        plt.title(f'Comparison of {metric} Across Test Files')\n",
    "        plt.xticks(np.arange(len(test_comparison)), test_comparison['Test File'], rotation=45)\n",
    "        plt.ylabel(metric)\n",
    "        plt.ylim(0, 1)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Add value labels\n",
    "        for j, v in enumerate(test_comparison[f'{metric}_Traditional']):\n",
    "            plt.text(j - 0.2, v + 0.02, f\"{v:.2f}\", ha='center')\n",
    "        \n",
    "        for j, v in enumerate(test_comparison[f'{metric}_eTaPR']):\n",
    "            plt.text(j + 0.2, v + 0.02, f\"{v:.2f}\", ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save eTaPR Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save eTaPR results\n",
    "etapr_df.to_csv(os.path.join(RESULTS_DIR, 'etapr_results.csv'), index=False)\n",
    "print(f\"Saved eTaPR results to {os.path.join(RESULTS_DIR, 'etapr_results.csv')}\")\n",
    "\n",
    "# Save all test file eTaPR results\n",
    "all_etapr_df.to_csv(os.path.join(RESULTS_DIR, 'etapr_test_results.csv'), index=False)\n",
    "print(f\"Saved eTaPR test results to {os.path.join(RESULTS_DIR, 'etapr_test_results.csv')}\")\n",
    "\n",
    "# Save comparison results if available\n",
    "if test_results is not None:\n",
    "    test_comparison.to_csv(os.path.join(RESULTS_DIR, 'metrics_comparison.csv'), index=False)\n",
    "    print(f\"Saved metrics comparison to {os.path.join(RESULTS_DIR, 'metrics_comparison.csv')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}