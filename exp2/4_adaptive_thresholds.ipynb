{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive and Multi-Threshold Strategies for HAI-21.03 Dataset\n",
    "\n",
    "This notebook implements advanced adaptive and multi-threshold strategies for anomaly detection on the HAI-21.03 industrial control system security dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve, roc_curve, auc\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Global Variables Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "OUTPUT_DIR = 'hai-security-dataset/processed'\n",
    "FEATURE_DIR = 'hai-security-dataset/features'\n",
    "MODEL_DIR = 'hai-security-dataset/models'\n",
    "RESULTS_DIR = 'hai-security-dataset/results'\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Set parameters for post-processing\n",
    "MIN_ANOMALY_LENGTH = 30  # Minimum length of anomalies to keep\n",
    "GAP_THRESHOLD = 3        # Maximum gap between anomalies to merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = load_model(os.path.join(MODEL_DIR, 'improved_lstm_autoencoder.h5'))\n",
    "print(\"Loaded model\")\n",
    "\n",
    "# Load model parameters\n",
    "with open(os.path.join(MODEL_DIR, 'improved_model_params.pkl'), 'rb') as f:\n",
    "    model_params = pickle.load(f)\n",
    "\n",
    "SEQ_LENGTH = model_params['seq_length']\n",
    "STRIDE = model_params['stride']\n",
    "ensemble_features = model_params['ensemble_features']\n",
    "ensemble_indices = model_params['ensemble_indices']\n",
    "thresholds = model_params['thresholds']\n",
    "operating_points = model_params['operating_points']\n",
    "\n",
    "print(f\"Loaded model parameters: SEQ_LENGTH={SEQ_LENGTH}, STRIDE={STRIDE}\")\n",
    "print(f\"Number of selected features: {len(ensemble_features)}\")\n",
    "\n",
    "# Load detector\n",
    "with open(os.path.join(MODEL_DIR, 'multi_threshold_detector.pkl'), 'rb') as f:\n",
    "    detector = pickle.load(f)\n",
    "\n",
    "print(\"Loaded multi-threshold detector\")\n",
    "print(\"Operating points:\")\n",
    "for point, threshold in detector.operating_points.items():\n",
    "    print(f\"  {point}: {threshold:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_files = [f for f in os.listdir(OUTPUT_DIR) if f.startswith('test') and f.endswith('_processed_enhanced.csv')]\n",
    "test_data = {}\n",
    "\n",
    "for file in test_files:\n",
    "    file_path = os.path.join(OUTPUT_DIR, file)\n",
    "    file_name = file.split('_')[0]  # Extract test file name (e.g., 'test1')\n",
    "    df = pd.read_csv(file_path)\n",
    "    test_data[file_name] = df\n",
    "    print(f\"Loaded {file_name}: {df.shape[0]} rows, {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_anomalies(anomaly_labels, min_anomaly_length=30, gap_threshold=3):\n",
    "    \"\"\"\n",
    "    Apply post-processing to reduce false positives and false negatives.\n",
    "    \n",
    "    Args:\n",
    "        anomaly_labels (np.array): Binary anomaly labels\n",
    "        min_anomaly_length (int): Minimum length of anomalies to keep\n",
    "        gap_threshold (int): Maximum gap between anomalies to merge\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Processed binary anomaly labels\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    processed_labels = anomaly_labels.copy()\n",
    "    \n",
    "    # Remove short anomalies (likely false positives)\n",
    "    i = 0\n",
    "    while i < len(processed_labels):\n",
    "        if processed_labels[i] == 1:\n",
    "            # Find the end of this anomaly\n",
    "            j = i\n",
    "            while j < len(processed_labels) and processed_labels[j] == 1:\n",
    "                j += 1\n",
    "            \n",
    "            # If anomaly is too short, remove it\n",
    "            if j - i < min_anomaly_length:\n",
    "                processed_labels[i:j] = 0\n",
    "            \n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    # Merge nearby anomalies\n",
    "    i = 0\n",
    "    while i < len(processed_labels):\n",
    "        if processed_labels[i] == 1:\n",
    "            # Find the end of this anomaly\n",
    "            j = i\n",
    "            while j < len(processed_labels) and processed_labels[j] == 1:\n",
    "                j += 1\n",
    "            \n",
    "            # Look for another anomaly nearby\n",
    "            if j < len(processed_labels) - gap_threshold:\n",
    "                next_start = j\n",
    "                while next_start < j + gap_threshold and next_start < len(processed_labels) and processed_labels[next_start] == 0:\n",
    "                    next_start += 1\n",
    "                \n",
    "                if next_start < j + gap_threshold and next_start < len(processed_labels) and processed_labels[next_start] == 1:\n",
    "                    processed_labels[j:next_start] = 1\n",
    "            \n",
    "            i = j\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    return processed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate model performance.\n",
    "    \n",
    "    Args:\n",
    "        y_true (np.array): Ground truth labels\n",
    "        y_pred (np.array): Predicted labels\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    false_negative_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'false_negative_rate': false_negative_rate,\n",
    "        'confusion_matrix': cm,\n",
    "        'true_positives': tp,\n",
    "        'false_positives': fp,\n",
    "        'true_negatives': tn,\n",
    "        'false_negatives': fn\n",
    "    }\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n",
    "    print(f\"False Negative Rate: {false_negative_rate:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        cm (np.array): Confusion matrix\n",
    "        classes (list): Class names\n",
    "        title (str): Plot title\n",
    "        cmap: Colormap\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate with Different Operating Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_file(test_name, test_df, operating_point='balanced'):\n",
    "    \"\"\"\n",
    "    Evaluate model on a single test file with a specific operating point.\n",
    "    \n",
    "    Args:\n",
    "        test_name (str): Name of the test file\n",
    "        test_df (pd.DataFrame): Test dataframe\n",
    "        operating_point (str): Operating point to use\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {test_name} with operating point: {operating_point}\")\n",
    "    \n",
    "    # Extract features and target\n",
    "    feature_cols = [col for col in test_df.columns if col not in ['time', 'attack']]\n",
    "    X_test = test_df[feature_cols].values\n",
    "    y_test = test_df['attack'].values\n",
    "    \n",
    "    # Predict anomalies\n",
    "    anomaly_scores, anomaly_labels, threshold = detector.predict(X_test, operating_point=operating_point)\n",
    "    \n",
    "    # Apply post-processing\n",
    "    processed_labels = post_process_anomalies(anomaly_labels, min_anomaly_length=MIN_ANOMALY_LENGTH, gap_threshold=GAP_THRESHOLD)\n",
    "    \n",
    "    # Evaluate model\n",
    "    eval_results = evaluate_model(y_test, processed_labels)\n",
    "    \n",
    "    # Visualize results\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    \n",
    "    # Plot ground truth\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(y_test, 'b-', label='Ground Truth')\n",
    "    plt.title(f'Ground Truth - {test_name}')\n",
    "    plt.ylabel('Anomaly')\n",
    "    plt.yticks([0, 1])\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot anomaly scores\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(anomaly_scores, 'r-', label='Anomaly Scores')\n",
    "    plt.axhline(y=threshold, color='g', linestyle='--', label=f'Threshold ({threshold:.4f})')\n",
    "    plt.title(f'Anomaly Scores - {test_name} ({operating_point})')\n",
    "    plt.ylabel('Score')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot predictions\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(processed_labels, 'g-', label='Predictions')\n",
    "    plt.title(f'Predictions - {test_name} ({operating_point})')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Anomaly')\n",
    "    plt.yticks([0, 1])\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(eval_results['confusion_matrix'], classes=['Normal', 'Anomaly'], title=f'Confusion Matrix - {test_name} ({operating_point})')\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'test_name': test_name,\n",
    "        'operating_point': operating_point,\n",
    "        'threshold': threshold,\n",
    "        'precision': eval_results['precision'],\n",
    "        'recall': eval_results['recall'],\n",
    "        'f1_score': eval_results['f1_score'],\n",
    "        'accuracy': eval_results['accuracy'],\n",
    "        'false_positive_rate': eval_results['false_positive_rate'],\n",
    "        'false_negative_rate': eval_results['false_negative_rate'],\n",
    "        'anomaly_scores': anomaly_scores,\n",
    "        'anomaly_labels': processed_labels,\n",
    "        'ground_truth': y_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define operating points to evaluate\n",
    "operating_points = ['high_precision', 'balanced', 'high_recall', 'adaptive']\n",
    "\n",
    "# Evaluate first test file with different operating points\n",
    "test_name = list(test_data.keys())[0]\n",
    "test_df = test_data[test_name]\n",
    "\n",
    "results = {}\n",
    "for point in operating_points:\n",
    "    results[point] = evaluate_test_file(test_name, test_df, operating_point=point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare results across operating points\n",
    "comparison = []\n",
    "for point in operating_points:\n",
    "    comparison.append({\n",
    "        'Operating Point': point,\n",
    "        'Threshold': results[point]['threshold'],\n",
    "        'Precision': results[point]['precision'],\n",
    "        'Recall': results[point]['recall'],\n",
    "        'F1 Score': results[point]['f1_score'],\n",
    "        'Accuracy': results[point]['accuracy'],\n",
    "        'False Positive Rate': results[point]['false_positive_rate'],\n",
    "        'False Negative Rate': results[point]['false_negative_rate']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics across operating points\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot precision, recall, and F1 score\n",
    "metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.bar(comparison_df['Operating Point'], comparison_df[metric])\n",
    "    plt.title(metric)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, v in enumerate(comparison_df[metric]):\n",
    "        plt.text(j, v + 0.02, f\"{v:.4f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize error rates\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot false positive and false negative rates\n",
    "error_metrics = ['False Positive Rate', 'False Negative Rate']\n",
    "for i, metric in enumerate(error_metrics):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.bar(comparison_df['Operating Point'], comparison_df[metric])\n",
    "    plt.title(metric)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, max(comparison_df[metric].max() * 1.2, 0.1))\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, v in enumerate(comparison_df[metric]):\n",
    "        plt.text(j, v + 0.005, f\"{v:.4f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate All Test Files with Best Operating Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best operating point based on F1 score\n",
    "best_point = comparison_df.loc[comparison_df['F1 Score'].idxmax(), 'Operating Point']\n",
    "print(f\"Best operating point based on F1 score: {best_point}\")\n",
    "\n",
    "# Evaluate all test files with best operating point\n",
    "all_results = []\n",
    "for test_name, test_df in test_data.items():\n",
    "    result = evaluate_test_file(test_name, test_df, operating_point=best_point)\n",
    "    all_results.append({\n",
    "        'Test File': result['test_name'],\n",
    "        'Operating Point': result['operating_point'],\n",
    "        'Threshold': result['threshold'],\n",
    "        'Precision': result['precision'],\n",
    "        'Recall': result['recall'],\n",
    "        'F1 Score': result['f1_score'],\n",
    "        'Accuracy': result['accuracy'],\n",
    "        'False Positive Rate': result['false_positive_rate'],\n",
    "        'False Negative Rate': result['false_negative_rate']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Display results\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics across test files\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot precision, recall, and F1 score\n",
    "metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.bar(results_df['Test File'], results_df[metric])\n",
    "    plt.title(metric)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, v in enumerate(results_df[metric]):\n",
    "        plt.text(j, v + 0.02, f\"{v:.4f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize error rates\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot false positive and false negative rates\n",
    "error_metrics = ['False Positive Rate', 'False Negative Rate']\n",
    "for i, metric in enumerate(error_metrics):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.bar(results_df['Test File'], results_df[metric])\n",
    "    plt.title(metric)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, max(results_df[metric].max() * 1.2, 0.1))\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, v in enumerate(results_df[metric]):\n",
    "        plt.text(j, v + 0.005, f\"{v:.4f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average metrics\n",
    "avg_metrics = {\n",
    "    'Precision': results_df['Precision'].mean(),\n",
    "    'Recall': results_df['Recall'].mean(),\n",
    "    'F1 Score': results_df['F1 Score'].mean(),\n",
    "    'Accuracy': results_df['Accuracy'].mean(),\n",
    "    'False Positive Rate': results_df['False Positive Rate'].mean(),\n",
    "    'False Negative Rate': results_df['False Negative Rate'].mean()\n",
    "}\n",
    "\n",
    "# Display average metrics\n",
    "print(\"Average Metrics Across All Test Files:\")\n",
    "for metric, value in avg_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implement Dynamic Threshold Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicThresholdDetector:\n",
    "    \"\"\"\n",
    "    Dynamic threshold detector that adapts to data distribution changes.\n",
    "    \"\"\"\n",
    "    def __init__(self, detector, window_size=1000, update_interval=100):\n",
    "        \"\"\"\n",
    "        Initialize the detector.\n",
    "        \n",
    "        Args:\n",
    "            detector: Base detector\n",
    "            window_size: Size of the sliding window for threshold adaptation\n",
    "            update_interval: Interval for updating thresholds\n",
    "        \"\"\"\n",
    "        self.detector = detector\n",
    "        self.window_size = window_size\n",
    "        self.update_interval = update_interval\n",
    "        self.error_history = []\n",
    "        self.threshold_history = []\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Predict anomalies with dynamic threshold adaptation.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data\n",
    "            y: Ground truth labels (optional, for evaluation)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (anomaly_scores, anomaly_labels, thresholds)\n",
    "        \"\"\"\n",
    "        # Scale features\n",
    "        X_scaled = self.detector.scaler.transform(X)\n",
    "        \n",
    "        # Select ensemble features\n",
    "        X_ensemble = X_scaled[:, self.detector.ensemble_indices]\n",
    "        \n",
    "        # Apply PCA\n",
    "        X_pca = self.detector.pca.transform(X_ensemble)\n",
    "        \n",
    "        # Create sequences\n",
    "        X_seq = create_sequences(X_pca, self.detector.seq_length, self.detector.stride)\n",
    "        \n",
    "        # Get predictions\n",
    "        X_pred = self.detector.model.predict(X_seq)\n",
    "        \n",
    "        # Calculate reconstruction errors\n",
    "        mse = np.mean(np.square(X_seq - X_pred), axis=(1, 2))\n",
    "        \n",
    "        # Initialize arrays\n",
    "        anomaly_scores = np.zeros(len(X))\n",
    "        count = np.zeros(len(X))\n",
    "        thresholds = np.zeros(len(X))\n",
    "        \n",
    "        # Initial threshold\n",
    "        current_threshold = self.detector.operating_points['balanced']\n",
    "        \n",
    "        # For each sequence\n",
    "        for i, error in enumerate(mse):\n",
    "            idx = i * self.detector.stride\n",
    "            if idx + self.detector.seq_length <= len(X):\n",
    "                # Add error to history\n",
    "                self.error_history.append(error)\n",
    "                if len(self.error_history) > self.window_size:\n",
    "                    self.error_history.pop(0)\n",
    "                \n",
    "                # Update threshold periodically\n",
    "                if i % self.update_interval == 0 and len(self.error_history) > 0:\n",
    "                    # Calculate new threshold based on recent errors\n",
    "                    mean = np.mean(self.error_history)\n",
    "                    std = np.std(self.error_history)\n",
    "                    current_threshold = mean + 3 * std\n",
    "                    self.threshold_history.append(current_threshold)\n",
    "                \n",
    "                # Store threshold\n",
    "                thresholds[idx:idx+self.detector.seq_length] = current_threshold\n",
    "                \n",
    "                # Calculate anomaly score\n",
    "                anomaly_scores[idx:idx+self.detector.seq_length] += error\n",
    "                count[idx:idx+self.detector.seq_length] += 1\n",
    "        \n",
    "        # Normalize scores by count\n",
    "        anomaly_scores = np.divide(anomaly_scores, count, out=np.zeros_like(anomaly_scores), where=count!=0)\n",
    "        \n",
    "        # Apply threshold to get binary labels\n",
    "        anomaly_labels = (anomaly_scores > thresholds).astype(int)\n",
    "        \n",
    "        # Apply post-processing\n",
    "        processed_labels = self.detector.post_process_anomalies(anomaly_labels)\n",
    "        \n",
    "        return anomaly_scores, processed_labels, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dynamic threshold detector\n",
    "dynamic_detector = DynamicThresholdDetector(detector, window_size=1000, update_interval=100)\n",
    "\n",
    "# Test dynamic threshold detector on first test file\n",
    "test_name = list(test_data.keys())[0]\n",
    "test_df = test_data[test_name]\n",
    "\n",
    "# Extract features and target\n",
    "feature_cols = [col for col in test_df.columns if col not in ['time', 'attack']]\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df['attack'].values\n",
    "\n",
    "# Predict anomalies\n",
    "anomaly_scores, anomaly_labels, thresholds = dynamic_detector.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "eval_results = evaluate_model(y_test, anomaly_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dynamic threshold results\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# Plot ground truth\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(y_test, 'b-', label='Ground Truth')\n",
    "plt.title(f'Ground Truth - {test_name}')\n",
    "plt.ylabel('Anomaly')\n",
    "plt.yticks([0, 1])\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot anomaly scores\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(anomaly_scores, 'r-', label='Anomaly Scores')\n",
    "plt.title(f'Anomaly Scores - {test_name} (Dynamic Threshold)')\n",
    "plt.ylabel('Score')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot dynamic thresholds\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(thresholds, 'g-', label='Dynamic Thresholds')\n",
    "plt.title(f'Dynamic Thresholds - {test_name}')\n",
    "plt.ylabel('Threshold')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot predictions\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(anomaly_labels, 'g-', label='Predictions')\n",
    "plt.title(f'Predictions - {test_name} (Dynamic Threshold)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Anomaly')\n",
    "plt.yticks([0, 1])\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(eval_results['confusion_matrix'], classes=['Normal', 'Anomaly'], title=f'Confusion Matrix - {test_name} (Dynamic Threshold)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dynamic threshold with static thresholds\n",
    "comparison_df = comparison_df.append({\n",
    "    'Operating Point': 'Dynamic',\n",
    "    'Threshold': np.mean(thresholds),\n",
    "    'Precision': eval_results['precision'],\n",
    "    'Recall': eval_results['recall'],\n",
    "    'F1 Score': eval_results['f1_score'],\n",
    "    'Accuracy': eval_results['accuracy'],\n",
    "    'False Positive Rate': eval_results['false_positive_rate'],\n",
    "    'False Negative Rate': eval_results['false_negative_rate']\n",
    "}, ignore_index=True)\n",
    "\n",
    "# Display updated comparison\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize updated comparison\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot precision, recall, and F1 score\n",
    "metrics = ['Precision', 'Recall', 'F1 Score']\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.bar(comparison_df['Operating Point'], comparison_df[metric])\n",
    "    plt.title(metric)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for j, v in enumerate(comparison_df[metric]):\n",
    "        plt.text(j, v + 0.02, f\"{v:.4f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "comparison_df.to_csv(os.path.join(RESULTS_DIR, 'threshold_comparison.csv'), index=False)\n",
    "print(f\"Saved threshold comparison to {os.path.join(RESULTS_DIR, 'threshold_comparison.csv')}\")\n",
    "\n",
    "# Save test results\n",
    "results_df.to_csv(os.path.join(RESULTS_DIR, 'test_results.csv'), index=False)\n",
    "print(f\"Saved test results to {os.path.join(RESULTS_DIR, 'test_results.csv')}\")\n",
    "\n",
    "# Save dynamic threshold detector\n",
    "with open(os.path.join(MODEL_DIR, 'dynamic_threshold_detector.pkl'), 'wb') as f:\n",
    "    pickle.dump(dynamic_detector, f)\n",
    "print(f\"Saved dynamic threshold detector to {os.path.join(MODEL_DIR, 'dynamic_threshold_detector.pkl')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}