{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Feature Selection for HAI-21.03 Dataset (NaN-Handling Version)\n",
    "\n",
    "This notebook implements advanced feature selection techniques for the HAI-21.03 industrial control system security dataset with proper handling of NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Global Variables Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\User\\WebstormProjects\\hai-dataset-analysis\\exp2\n",
      "Output directory: c:\\Users\\User\\WebstormProjects\\hai-dataset-analysis\\hai-security-dataset\\processed\n",
      "Feature directory: c:\\Users\\User\\WebstormProjects\\hai-dataset-analysis\\hai-security-dataset\\features\n",
      "\n",
      "Files in output directory:\n",
      "  enhanced_graph_v2.pkl\n",
      "  feature_info.pkl\n",
      "  merged_train.csv\n",
      "  pca.pkl\n",
      "  scaler.pkl\n",
      "  test1.parquet\n",
      "  test1_processed.csv\n",
      "  test1_processed_enhanced_v2.csv\n",
      "  test2.parquet\n",
      "  test2_processed.csv\n",
      "  test3.parquet\n",
      "  test3_processed.csv\n",
      "  test4.parquet\n",
      "  test4_processed.csv\n",
      "  test5.parquet\n",
      "  test5_processed.csv\n",
      "  train1.parquet\n",
      "  train1_processed_enhanced_v2.csv\n",
      "  train2.parquet\n",
      "  train3.parquet\n",
      "  train_processed.csv\n",
      "\n",
      "Files in feature directory:\n",
      "  feature_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# Set paths - FIXED PATHS to match the directory structure\n",
    "OUTPUT_DIR = '../hai-security-dataset/processed'\n",
    "FEATURE_DIR = '../hai-security-dataset/features'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(FEATURE_DIR, exist_ok=True)\n",
    "\n",
    "# Print current working directory to verify path\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Output directory: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "print(f\"Feature directory: {os.path.abspath(FEATURE_DIR)}\")\n",
    "\n",
    "# List files in directories\n",
    "print(\"\\nFiles in output directory:\")\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    files = os.listdir(OUTPUT_DIR)\n",
    "    for file in files:\n",
    "        print(f\"  {file}\")\n",
    "else:\n",
    "    print(f\"  Directory {OUTPUT_DIR} does not exist\")\n",
    "\n",
    "print(\"\\nFiles in feature directory:\")\n",
    "if os.path.exists(FEATURE_DIR):\n",
    "    files = os.listdir(FEATURE_DIR)\n",
    "    if files:\n",
    "        for file in files:\n",
    "            print(f\"  {file}\")\n",
    "    else:\n",
    "        print(\"  No files found (directory is empty)\")\n",
    "else:\n",
    "    print(f\"  Directory {FEATURE_DIR} does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Enhanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 train files and 1 test files\n"
     ]
    }
   ],
   "source": [
    "# Look for processed files\n",
    "train_files = [f for f in os.listdir(OUTPUT_DIR) if f.startswith('train') and f.endswith('_processed_enhanced_v2.csv')]\n",
    "if not train_files:\n",
    "    train_files = [f for f in os.listdir(OUTPUT_DIR) if f.startswith('train') and f.endswith('_processed.csv')]\n",
    "\n",
    "test_files = [f for f in os.listdir(OUTPUT_DIR) if f.startswith('test') and f.endswith('_processed_enhanced_v2.csv')]\n",
    "if not test_files:\n",
    "    test_files = [f for f in os.listdir(OUTPUT_DIR) if f.startswith('test') and f.endswith('_processed.csv')]\n",
    "\n",
    "print(f\"Found {len(train_files)} train files and {len(test_files)} test files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train1_processed_enhanced_v2.csv...\n",
      "Loaded train1_processed_enhanced_v2.csv: 216001 rows, 822 columns\n"
     ]
    }
   ],
   "source": [
    "# Load the first train file\n",
    "if train_files:\n",
    "    print(f\"Loading {train_files[0]}...\")\n",
    "    train_df = pd.read_csv(os.path.join(OUTPUT_DIR, train_files[0]))\n",
    "    print(f\"Loaded {train_files[0]}: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No processed training data found. Please run the feature engineering notebook first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test files...\n",
      "Loaded test1: 43201 rows, 788 columns\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "test_data = {}\n",
    "if test_files:\n",
    "    print(f\"Loading test files...\")\n",
    "    for file in test_files[:1]:  # Load only the first test file to save memory\n",
    "        file_path = os.path.join(OUTPUT_DIR, file)\n",
    "        file_name = file.split('_')[0]  # Extract test file name (e.g., 'test1')\n",
    "        df = pd.read_csv(file_path)\n",
    "        test_data[file_name] = df\n",
    "        print(f\"Loaded {file_name}: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "else:\n",
    "    print(\"No processed test data found. Will only work with training data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Filtering and NaN Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_features(df):\n",
    "    \"\"\"\n",
    "    Filter out non-feature columns and handle problematic features.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataframe\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (filtered_df, feature_cols, target_col)\n",
    "    \"\"\"\n",
    "    # Exclude non-numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    \n",
    "    # Exclude time and attack columns from features\n",
    "    exclude_cols = [col for col in df.columns if col.startswith('time')]\n",
    "    target_col = 'attack'\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_cols = [col for col in numeric_cols if col not in exclude_cols and col != target_col]\n",
    "    \n",
    "    # Check for constant or near-constant features\n",
    "    variance = df[feature_cols].var()\n",
    "    constant_features = variance[variance < 1e-10].index.tolist()\n",
    "    if constant_features:\n",
    "        print(f\"Removing {len(constant_features)} constant or near-constant features\")\n",
    "        feature_cols = [col for col in feature_cols if col not in constant_features]\n",
    "    \n",
    "    # Check for features with too many NaN values\n",
    "    nan_percentage = df[feature_cols].isna().mean()\n",
    "    high_nan_features = nan_percentage[nan_percentage > 0.1].index.tolist()\n",
    "    if high_nan_features:\n",
    "        print(f\"Removing {len(high_nan_features)} features with >10% NaN values\")\n",
    "        feature_cols = [col for col in feature_cols if col not in high_nan_features]\n",
    "    \n",
    "    # Check for features with infinite values\n",
    "    inf_features = []\n",
    "    for col in feature_cols:\n",
    "        if np.isinf(df[col]).any():\n",
    "            inf_features.append(col)\n",
    "    \n",
    "    if inf_features:\n",
    "        print(f\"Removing {len(inf_features)} features with infinite values\")\n",
    "        feature_cols = [col for col in feature_cols if col not in inf_features]\n",
    "    \n",
    "    print(f\"Selected {len(feature_cols)} valid features out of {len(numeric_cols)} numeric columns\")\n",
    "    \n",
    "    # Create a filtered dataframe with selected features and target\n",
    "    filtered_df = df[feature_cols + [target_col]].copy()\n",
    "    \n",
    "    # Check for any remaining NaN values\n",
    "    nan_count = filtered_df[feature_cols].isna().sum().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"Found {nan_count} remaining NaN values. Filling with median values.\")\n",
    "        # Fill remaining NaN values with median for each column\n",
    "        for col in feature_cols:\n",
    "            if filtered_df[col].isna().any():\n",
    "                median_val = filtered_df[col].median()\n",
    "                filtered_df[col] = filtered_df[col].fillna(median_val)\n",
    "    \n",
    "    # Verify no NaN values remain\n",
    "    nan_count_after = filtered_df[feature_cols].isna().sum().sum()\n",
    "    if nan_count_after > 0:\n",
    "        print(f\"Warning: {nan_count_after} NaN values still remain after filling!\")\n",
    "    else:\n",
    "        print(\"All NaN values have been handled successfully.\")\n",
    "    \n",
    "    return filtered_df, feature_cols, target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 53 constant or near-constant features\n",
      "Removing 143 features with >10% NaN values\n",
      "Selected 624 valid features out of 821 numeric columns\n",
      "Found 1170000 remaining NaN values. Filling with median values.\n",
      "All NaN values have been handled successfully.\n",
      "Training data shape: X=(216001, 624), y=(216001,)\n",
      "No NaN values found in X_train.\n"
     ]
    }
   ],
   "source": [
    "# Filter features and handle NaN values\n",
    "filtered_train_df, feature_cols, target_col = filter_features(train_df)\n",
    "\n",
    "# Extract features and target\n",
    "X_train = filtered_train_df[feature_cols].values\n",
    "y_train = filtered_train_df[target_col].values\n",
    "\n",
    "print(f\"Training data shape: X={X_train.shape}, y={y_train.shape}\")\n",
    "\n",
    "# Double-check for NaN values\n",
    "nan_count = np.isnan(X_train).sum()\n",
    "if nan_count > 0:\n",
    "    print(f\"Warning: Found {nan_count} NaN values in X_train. Using SimpleImputer to handle them.\")\n",
    "    # Use SimpleImputer to handle any remaining NaN values\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train = imputer.fit_transform(X_train)\n",
    "    print(f\"After imputation, NaN count: {np.isnan(X_train).sum()}\")\n",
    "else:\n",
    "    print(\"No NaN values found in X_train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No NaN or infinite values found after scaling.\n",
      "Saved scaler to ../hai-security-dataset/features\\feature_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# Apply RobustScaler (good for anomaly detection)\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Check for NaN or infinite values after scaling\n",
    "nan_count = np.isnan(X_train_scaled).sum()\n",
    "inf_count = np.isinf(X_train_scaled).sum()\n",
    "\n",
    "if nan_count > 0 or inf_count > 0:\n",
    "    print(f\"Warning: Found {nan_count} NaN values and {inf_count} infinite values after scaling.\")\n",
    "    print(\"Replacing NaN and infinite values with 0...\")\n",
    "    X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    print(f\"After replacement, NaN count: {np.isnan(X_train_scaled).sum()}, Inf count: {np.isinf(X_train_scaled).sum()}\")\n",
    "else:\n",
    "    print(\"No NaN or infinite values found after scaling.\")\n",
    "\n",
    "# Save the scaler\n",
    "with open(os.path.join(FEATURE_DIR, 'feature_scaler.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"Saved scaler to {os.path.join(FEATURE_DIR, 'feature_scaler.pkl')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Filter-Based Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filter_methods(X_train_scaled, y_train, feature_cols, k=50):\n",
    "    \"\"\"\n",
    "    Apply filter-based feature selection methods with error handling.\n",
    "    \"\"\"\n",
    "    # Verify no NaN values in input data\n",
    "    if np.isnan(X_train_scaled).any() or np.isnan(y_train).any():\n",
    "        print(\"Warning: NaN values detected in input data. Replacing with zeros.\")\n",
    "        X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0)\n",
    "        y_train = np.nan_to_num(y_train, nan=0.0)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Variance Threshold\n",
    "    print(\"Applying Variance Threshold...\")\n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    X_var = selector.fit_transform(X_train_scaled)\n",
    "    var_support = selector.get_support()\n",
    "    var_features = [feature_cols[i] for i in range(len(feature_cols)) if var_support[i]]\n",
    "    results['variance_threshold'] = {\n",
    "        'support': var_support,\n",
    "        'features': var_features,\n",
    "        'scores': selector.variances_,\n",
    "        'data': X_var\n",
    "    }\n",
    "    print(f\"  Selected {len(var_features)} features\")\n",
    "    \n",
    "    # 2. ANOVA F-value\n",
    "    print(\"Applying ANOVA F-value...\")\n",
    "    try:\n",
    "        selector = SelectKBest(f_classif, k=min(k, X_train_scaled.shape[1]))\n",
    "        X_anova = selector.fit_transform(X_train_scaled, y_train)\n",
    "        anova_support = selector.get_support()\n",
    "        anova_features = [feature_cols[i] for i in range(len(feature_cols)) if anova_support[i]]\n",
    "        results['anova'] = {\n",
    "            'support': anova_support,\n",
    "            'features': anova_features,\n",
    "            'scores': selector.scores_,\n",
    "            'data': X_anova\n",
    "        }\n",
    "        print(f\"  Selected {len(anova_features)} features\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error applying ANOVA F-value: {e}\")\n",
    "        print(\"  Using RandomForest feature importance as a fallback\")\n",
    "        estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        estimator.fit(X_train_scaled, y_train)\n",
    "        rf_importances = estimator.feature_importances_\n",
    "        rf_indices = np.argsort(rf_importances)[::-1][:k]\n",
    "        anova_features = [feature_cols[i] for i in rf_indices]\n",
    "        anova_support = np.zeros(len(feature_cols), dtype=bool)\n",
    "        anova_support[rf_indices] = True\n",
    "        results['anova'] = {\n",
    "            'support': anova_support,\n",
    "            'features': anova_features,\n",
    "            'scores': rf_importances,\n",
    "            'data': X_train_scaled[:, rf_indices]\n",
    "        }\n",
    "        print(f\"  Selected {len(anova_features)} features using RandomForest\")\n",
    "    \n",
    "    # 3. Mutual Information\n",
    "    print(\"Applying Mutual Information...\")\n",
    "    try:\n",
    "        selector = SelectKBest(mutual_info_classif, k=min(k, X_train_scaled.shape[1]))\n",
    "        X_mi = selector.fit_transform(X_train_scaled, y_train)\n",
    "        mi_support = selector.get_support()\n",
    "        mi_features = [feature_cols[i] for i in range(len(feature_cols)) if mi_support[i]]\n",
    "        results['mutual_info'] = {\n",
    "            'support': mi_support,\n",
    "            'features': mi_features,\n",
    "            'scores': selector.scores_,\n",
    "            'data': X_mi\n",
    "        }\n",
    "        print(f\"  Selected {len(mi_features)} features\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error applying Mutual Information: {e}\")\n",
    "        print(\"  Using GradientBoosting feature importance as a fallback\")\n",
    "        estimator = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "        estimator.fit(X_train_scaled, y_train)\n",
    "        gb_importances = estimator.feature_importances_\n",
    "        gb_indices = np.argsort(gb_importances)[::-1][:k]\n",
    "        mi_features = [feature_cols[i] for i in gb_indices]\n",
    "        mi_support = np.zeros(len(feature_cols), dtype=bool)\n",
    "        mi_support[gb_indices] = True\n",
    "        results['mutual_info'] = {\n",
    "            'support': mi_support,\n",
    "            'features': mi_features,\n",
    "            'scores': gb_importances,\n",
    "            'data': X_train_scaled[:, gb_indices]\n",
    "        }\n",
    "        print(f\"  Selected {len(mi_features)} features using GradientBoosting\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Variance Threshold...\n",
      "  Selected 618 features\n",
      "Applying ANOVA F-value...\n",
      "  Selected 50 features\n",
      "Applying Mutual Information...\n",
      "  Selected 50 features\n",
      "Saved filter results to ../hai-security-dataset/features\\filter_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# Apply filter-based methods\n",
    "filter_results = apply_filter_methods(X_train_scaled, y_train, feature_cols, k=50)\n",
    "\n",
    "# Save filter results\n",
    "with open(os.path.join(FEATURE_DIR, 'filter_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(filter_results, f)\n",
    "print(f\"Saved filter results to {os.path.join(FEATURE_DIR, 'filter_results.pkl')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Wrapper-Based Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_wrapper_methods(X_train_scaled, y_train, feature_cols):\n",
    "    \"\"\"\n",
    "    Apply wrapper-based feature selection methods with error handling.\n",
    "    \"\"\"\n",
    "    # Verify no NaN values in input data\n",
    "    if np.isnan(X_train_scaled).any() or np.isnan(y_train).any():\n",
    "        print(\"Warning: NaN values detected in input data. Replacing with zeros.\")\n",
    "        X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0)\n",
    "        y_train = np.nan_to_num(y_train, nan=0.0)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Recursive Feature Elimination (RFE)\n",
    "    print(\"Applying Recursive Feature Elimination...\")\n",
    "    try:\n",
    "        estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        selector = RFE(estimator, n_features_to_select=min(50, X_train_scaled.shape[1]), step=0.1)\n",
    "        X_rfe = selector.fit_transform(X_train_scaled, y_train)\n",
    "        rfe_support = selector.get_support()\n",
    "        rfe_features = [feature_cols[i] for i in range(len(feature_cols)) if rfe_support[i]]\n",
    "        results['rfe'] = {\n",
    "            'support': rfe_support,\n",
    "            'features': rfe_features,\n",
    "            'ranking': selector.ranking_,\n",
    "            'data': X_rfe\n",
    "        }\n",
    "        print(f\"  Selected {len(rfe_features)} features\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error applying RFE: {e}\")\n",
    "        print(\"  Using RandomForest feature importance directly\")\n",
    "        estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        estimator.fit(X_train_scaled, y_train)\n",
    "        rf_importances = estimator.feature_importances_\n",
    "        rf_indices = np.argsort(rf_importances)[::-1][:50]\n",
    "        rfe_features = [feature_cols[i] for i in rf_indices]\n",
    "        rfe_support = np.zeros(len(feature_cols), dtype=bool)\n",
    "        rfe_support[rf_indices] = True\n",
    "        rfe_ranking = np.ones(len(feature_cols)) * 100  # High rank for non-selected features\n",
    "        rfe_ranking[rf_indices] = np.arange(1, len(rf_indices) + 1)  # Rank selected features\n",
    "        results['rfe'] = {\n",
    "            'support': rfe_support,\n",
    "            'features': rfe_features,\n",
    "            'ranking': rfe_ranking,\n",
    "            'data': X_train_scaled[:, rf_indices]\n",
    "        }\n",
    "        print(f\"  Selected {len(rfe_features)} features using RandomForest importance\")\n",
    "    \n",
    "    # 2. Random Forest Feature Importance\n",
    "    print(\"Applying Random Forest Feature Importance...\")\n",
    "    estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    estimator.fit(X_train_scaled, y_train)\n",
    "    rf_importances = estimator.feature_importances_\n",
    "    rf_indices = np.argsort(rf_importances)[::-1]\n",
    "    rf_features = [feature_cols[i] for i in rf_indices[:50]]\n",
    "    results['random_forest'] = {\n",
    "        'features': rf_features,\n",
    "        'importances': rf_importances,\n",
    "        'indices': rf_indices\n",
    "    }\n",
    "    print(f\"  Selected {len(rf_features)} features\")\n",
    "    \n",
    "    # 3. Gradient Boosting Feature Importance\n",
    "    print(\"Applying Gradient Boosting Feature Importance...\")\n",
    "    try:\n",
    "        estimator = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "        estimator.fit(X_train_scaled, y_train)\n",
    "        gb_importances = estimator.feature_importances_\n",
    "        gb_indices = np.argsort(gb_importances)[::-1]\n",
    "        gb_features = [feature_cols[i] for i in gb_indices[:50]]\n",
    "        results['gradient_boosting'] = {\n",
    "            'features': gb_features,\n",
    "            'importances': gb_importances,\n",
    "            'indices': gb_indices\n",
    "        }\n",
    "        print(f\"  Selected {len(gb_features)} features\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error applying Gradient Boosting: {e}\")\n",
    "        print(\"  Using Random Forest results as a fallback\")\n",
    "        results['gradient_boosting'] = results['random_forest']\n",
    "        print(f\"  Selected {len(rf_features)} features using Random Forest\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Recursive Feature Elimination...\n",
      "  Selected 50 features\n",
      "Applying Random Forest Feature Importance...\n",
      "  Selected 50 features\n",
      "Applying Gradient Boosting Feature Importance...\n",
      "  Error applying Gradient Boosting: y contains 1 class after sample_weight trimmed classes with zero weights, while a minimum of 2 classes are required.\n",
      "  Using Random Forest results as a fallback\n",
      "  Selected 50 features using Random Forest\n",
      "Saved wrapper results to ../hai-security-dataset/features\\wrapper_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# Apply wrapper-based methods\n",
    "wrapper_results = apply_wrapper_methods(X_train_scaled, y_train, feature_cols)\n",
    "\n",
    "# Save wrapper results\n",
    "with open(os.path.join(FEATURE_DIR, 'wrapper_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(wrapper_results, f)\n",
    "print(f\"Saved wrapper results to {os.path.join(FEATURE_DIR, 'wrapper_results.pkl')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Embedded-Based Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_embedded_methods(X_train_scaled, y_train, feature_cols):\n",
    "    \"\"\"\n",
    "    Apply embedded feature selection methods with error handling.\n",
    "    \"\"\"\n",
    "    # Verify no NaN values in input data\n",
    "    if np.isnan(X_train_scaled).any() or np.isnan(y_train).any():\n",
    "        print(\"Warning: NaN values detected in input data. Replacing with zeros.\")\n",
    "        X_train_scaled = np.nan_to_num(X_train_scaled, nan=0.0)\n",
    "        y_train = np.nan_to_num(y_train, nan=0.0)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Lasso\n",
    "    print(\"Applying Lasso...\")\n",
    "    try:\n",
    "        lasso = Lasso(alpha=0.01)\n",
    "        lasso.fit(X_train_scaled, y_train)\n",
    "        lasso_coef = np.abs(lasso.coef_)\n",
    "        lasso_indices = np.argsort(lasso_coef)[::-1]\n",
    "        lasso_features = [feature_cols[i] for i in lasso_indices[:50]]\n",
    "        results['lasso'] = {\n",
    "            'features': lasso_features,\n",
    "            'coefficients': lasso_coef,\n",
    "            'indices': lasso_indices\n",
    "        }\n",
    "        print(f\"  Selected {len(lasso_features)} features\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error applying Lasso: {e}\")\n",
    "        print(\"  Using Random Forest feature importance as a fallback\")\n",
    "        estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        estimator.fit(X_train_scaled, y_train)\n",
    "        rf_importances = estimator.feature_importances_\n",
    "        rf_indices = np.argsort(rf_importances)[::-1][:50]\n",
    "        lasso_features = [feature_cols[i] for i in rf_indices]\n",
    "        results['lasso'] = {\n",
    "            'features': lasso_features,\n",
    "            'coefficients': rf_importances,\n",
    "            'indices': rf_indices\n",
    "        }\n",
    "        print(f\"  Selected {len(lasso_features)} features using Random Forest\")\n",
    "    \n",
    "    # 2. SelectFromModel with Random Forest\n",
    "    print(\"Applying SelectFromModel with Random Forest...\")\n",
    "    try:\n",
    "        estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        selector = SelectFromModel(estimator, threshold='median')\n",
    "        X_sfm = selector.fit_transform(X_train_scaled, y_train)\n",
    "        sfm_support = selector.get_support()\n",
    "        sfm_features = [feature_cols[i] for i in range(len(feature_cols)) if sfm_support[i]]\n",
    "        results['select_from_model'] = {\n",
    "            'support': sfm_support,\n",
    "            'features': sfm_features,\n",
    "            'data': X_sfm\n",
    "        }\n",
    "        print(f\"  Selected {len(sfm_features)} features\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error applying SelectFromModel: {e}\")\n",
    "        print(\"  Using top 50 features from Random Forest importance directly\")\n",
    "        estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        estimator.fit(X_train_scaled, y_train)\n",
    "        rf_importances = estimator.feature_importances_\n",
    "        rf_indices = np.argsort(rf_importances)[::-1][:50]\n",
    "        sfm_features = [feature_cols[i] for i in rf_indices]\n",
    "        sfm_support = np.zeros(len(feature_cols), dtype=bool)\n",
    "        sfm_support[rf_indices] = True\n",
    "        results['select_from_model'] = {\n",
    "            'support': sfm_support,\n",
    "            'features': sfm_features,\n",
    "            'data': X_train_scaled[:, rf_indices]\n",
    "        }\n",
    "        print(f\"  Selected {len(sfm_features)} features using Random Forest importance\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Lasso...\n",
      "  Selected 50 features\n",
      "Applying SelectFromModel with Random Forest...\n",
      "  Selected 624 features\n",
      "Saved embedded results to ../hai-security-dataset/features\\embedded_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# Apply embedded-based methods\n",
    "embedded_results = apply_embedded_methods(X_train_scaled, y_train, feature_cols)\n",
    "\n",
    "# Save embedded results\n",
    "with open(os.path.join(FEATURE_DIR, 'embedded_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(embedded_results, f)\n",
    "print(f\"Saved embedded results to {os.path.join(FEATURE_DIR, 'embedded_results.pkl')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Ranking and Ensemble Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_feature_selection(filter_results, wrapper_results, embedded_results, feature_cols):\n",
    "    \"\"\"\n",
    "    Combine results from different feature selection methods.\n",
    "    \"\"\"\n",
    "    # Initialize feature scores\n",
    "    feature_scores = {feature: 0 for feature in feature_cols}\n",
    "    \n",
    "    # Collect all selected features\n",
    "    all_selected_features = []\n",
    "    \n",
    "    # Add filter-based features\n",
    "    all_selected_features.extend(filter_results['anova']['features'])\n",
    "    all_selected_features.extend(filter_results['mutual_info']['features'])\n",
    "    \n",
    "    # Add wrapper-based features\n",
    "    all_selected_features.extend(wrapper_results['rfe']['features'])\n",
    "    all_selected_features.extend(wrapper_results['random_forest']['features'])\n",
    "    all_selected_features.extend(wrapper_results['gradient_boosting']['features'])\n",
    "    \n",
    "    # Add embedded-based features\n",
    "    all_selected_features.extend(embedded_results['lasso']['features'])\n",
    "    all_selected_features.extend(embedded_results['select_from_model']['features'])\n",
    "    \n",
    "    # Count occurrences of each feature\n",
    "    for feature in all_selected_features:\n",
    "        if feature in feature_scores:  # Ensure feature exists in the dictionary\n",
    "            feature_scores[feature] += 1\n",
    "    \n",
    "    # Sort features by score\n",
    "    sorted_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Select top features\n",
    "    top_n = 50\n",
    "    ensemble_features = [feature for feature, score in sorted_features[:top_n]]\n",
    "    \n",
    "    return ensemble_features, feature_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 50 features using ensemble method\n",
      "Saved ensemble features to ../hai-security-dataset/features\\ensemble_features.pkl\n",
      "Saved selected feature names to ../hai-security-dataset/features\\selected_features.txt\n"
     ]
    }
   ],
   "source": [
    "# Apply ensemble feature selection\n",
    "ensemble_features, feature_scores = ensemble_feature_selection(\n",
    "    filter_results, wrapper_results, embedded_results, feature_cols\n",
    ")\n",
    "\n",
    "print(f\"Selected {len(ensemble_features)} features using ensemble method\")\n",
    "\n",
    "# Save ensemble features\n",
    "with open(os.path.join(FEATURE_DIR, 'ensemble_features.pkl'), 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'ensemble_features': ensemble_features,\n",
    "        'feature_scores': feature_scores\n",
    "    }, f)\n",
    "print(f\"Saved ensemble features to {os.path.join(FEATURE_DIR, 'ensemble_features.pkl')}\")\n",
    "\n",
    "# Save feature names to text file\n",
    "with open(os.path.join(FEATURE_DIR, 'selected_features.txt'), 'w') as f:\n",
    "    for feature in ensemble_features:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "print(f\"Saved selected feature names to {os.path.join(FEATURE_DIR, 'selected_features.txt')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dimensionality_reduction(X_train_scaled, ensemble_features, feature_cols):\n",
    "    \"\"\"\n",
    "    Apply dimensionality reduction techniques with error handling.\n",
    "    \"\"\"\n",
    "    # Get indices of ensemble features\n",
    "    ensemble_indices = [feature_cols.index(feature) for feature in ensemble_features if feature in feature_cols]\n",
    "    X_ensemble = X_train_scaled[:, ensemble_indices]\n",
    "    \n",
    "    # Check for NaN values\n",
    "    if np.isnan(X_ensemble).any():\n",
    "        print(\"Warning: NaN values detected in ensemble features. Replacing with zeros.\")\n",
    "        X_ensemble = np.nan_to_num(X_ensemble, nan=0.0)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. PCA\n",
    "    print(\"Applying PCA...\")\n",
    "    try:\n",
    "        n_components = min(20, X_ensemble.shape[1])\n",
    "        pca = PCA(n_components=n_components)\n",
    "        X_pca = pca.fit_transform(X_ensemble)\n",
    "        explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "        results['pca'] = {\n",
    "            'data': X_pca,\n",
    "            'transformer': pca,\n",
    "            'explained_variance': explained_variance\n",
    "        }\n",
    "        print(f\"  Reduced to {n_components} components with {explained_variance:.4f} explained variance\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error applying PCA: {e}\")\n",
    "        print(\"  Skipping PCA\")\n",
    "        results['pca'] = {\n",
    "            'data': X_ensemble,\n",
    "            'transformer': None,\n",
    "            'explained_variance': 0.0\n",
    "        }\n",
    "    \n",
    "    # 2. Kernel PCA\n",
    "    print(\"Applying Kernel PCA...\")\n",
    "    try:\n",
    "        kpca = KernelPCA(n_components=n_components, kernel='rbf')\n",
    "        X_kpca = kpca.fit_transform(X_ensemble)\n",
    "        results['kpca'] = {\n",
    "            'data': X_kpca,\n",
    "            'transformer': kpca\n",
    "        }\n",
    "        print(f\"  Reduced to {n_components} components\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error applying Kernel PCA: {e}\")\n",
    "        print(\"  Skipping Kernel PCA\")\n",
    "        results['kpca'] = {\n",
    "            'data': X_ensemble,\n",
    "            'transformer': None\n",
    "        }\n",
    "    \n",
    "    return results, ensemble_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying PCA...\n",
      "  Reduced to 20 components with 0.9992 explained variance\n",
      "Applying Kernel PCA...\n",
      "  Error applying Kernel PCA: Unable to allocate 348. GiB for an array with shape (216001, 216001) and data type float64\n",
      "  Skipping Kernel PCA\n",
      "Saved dimensionality reduction results to ../hai-security-dataset/features\\reduction_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# Apply dimensionality reduction\n",
    "reduction_results, ensemble_indices = apply_dimensionality_reduction(\n",
    "    X_train_scaled, ensemble_features, feature_cols\n",
    ")\n",
    "\n",
    "# Save dimensionality reduction results\n",
    "with open(os.path.join(FEATURE_DIR, 'reduction_results.pkl'), 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'pca': reduction_results['pca']['transformer'],\n",
    "        'kpca': reduction_results['kpca']['transformer'],\n",
    "        'ensemble_indices': ensemble_indices\n",
    "    }, f)\n",
    "print(f\"Saved dimensionality reduction results to {os.path.join(FEATURE_DIR, 'reduction_results.pkl')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Feature Selection Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved comprehensive feature selection results to ../hai-security-dataset/features\\feature_selection_results.pkl\n",
      "\n",
      "Files in feature directory:\n",
      "  embedded_results.pkl\n",
      "  ensemble_features.pkl\n",
      "  feature_scaler.pkl\n",
      "  feature_selection_results.pkl\n",
      "  filter_results.pkl\n",
      "  reduction_results.pkl\n",
      "  selected_features.txt\n",
      "  wrapper_results.pkl\n"
     ]
    }
   ],
   "source": [
    "# Create a comprehensive feature selection results dictionary\n",
    "feature_selection_results = {\n",
    "    'ensemble_features': ensemble_features,\n",
    "    'feature_scores': feature_scores,\n",
    "    'ensemble_indices': ensemble_indices,\n",
    "    'scaler': scaler,\n",
    "    'pca': reduction_results['pca']['transformer'],\n",
    "    'kpca': reduction_results['kpca']['transformer']\n",
    "}\n",
    "\n",
    "# Save comprehensive results\n",
    "with open(os.path.join(FEATURE_DIR, 'feature_selection_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(feature_selection_results, f)\n",
    "\n",
    "print(f\"Saved comprehensive feature selection results to {os.path.join(FEATURE_DIR, 'feature_selection_results.pkl')}\")\n",
    "\n",
    "# List all files in the feature directory to confirm they were created\n",
    "print(\"\\nFiles in feature directory:\")\n",
    "if os.path.exists(FEATURE_DIR):\n",
    "    files = os.listdir(FEATURE_DIR)\n",
    "    if files:\n",
    "        for file in files:\n",
    "            print(f\"  {file}\")\n",
    "    else:\n",
    "        print(\"  No files found (directory is empty)\")\n",
    "else:\n",
    "    print(f\"  Directory {FEATURE_DIR} does not exist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
