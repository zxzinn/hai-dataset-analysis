{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Model Training with Adaptive Thresholds for HAI-21.03 Dataset\n",
    "\n",
    "This notebook implements an improved LSTM autoencoder model with adaptive thresholds for anomaly detection on the HAI-21.03 industrial control system security dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, RepeatVector, TimeDistributed, Dropout, Bidirectional, Attention, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, auc, roc_curve, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Global Variables Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "OUTPUT_DIR = 'hai-security-dataset/processed'\n",
    "FEATURE_DIR = 'hai-security-dataset/features'\n",
    "MODEL_DIR = 'hai-security-dataset/models'\n",
    "\n",
    "# Create model directory if it doesn't exist\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Set model parameters\n",
    "SEQ_LENGTH = 128  # Sequence length for LSTM\n",
    "STRIDE = 10       # Stride for sliding window\n",
    "BATCH_SIZE = 64   # Batch size for training\n",
    "EPOCHS = 50       # Maximum number of epochs\n",
    "PATIENCE = 10     # Patience for early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Selected Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature selection results\n",
    "with open(os.path.join(FEATURE_DIR, 'feature_selection_results.pkl'), 'rb') as f:\n",
    "    feature_selection_results = pickle.load(f)\n",
    "\n",
    "ensemble_features = feature_selection_results['ensemble_features']\n",
    "ensemble_indices = feature_selection_results['ensemble_indices']\n",
    "scaler = feature_selection_results['scaler']\n",
    "pca = feature_selection_results['pca']\n",
    "\n",
    "print(f\"Loaded {len(ensemble_features)} selected features\")\n",
    "print(f\"PCA components: {pca.n_components_}\")\n",
    "print(f\"PCA explained variance: {np.sum(pca.explained_variance_ratio_):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed training data\n",
    "train_df = pd.read_csv(os.path.join(OUTPUT_DIR, 'train_processed_enhanced.csv'))\n",
    "print(f\"Loaded enhanced training data: {train_df.shape[0]} rows, {train_df.shape[1]} columns\")\n",
    "\n",
    "# Extract feature columns and target\n",
    "feature_cols = [col for col in train_df.columns if col not in ['time', 'attack']]\n",
    "target_col = 'attack'\n",
    "\n",
    "# Extract features and target\n",
    "X_train = train_df[feature_cols].values\n",
    "y_train = train_df[target_col].values\n",
    "\n",
    "# Scale features\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Select ensemble features\n",
    "X_train_ensemble = X_train_scaled[:, ensemble_indices]\n",
    "\n",
    "# Apply PCA\n",
    "X_train_pca = pca.transform(X_train_ensemble)\n",
    "\n",
    "print(f\"Training data shape: X={X_train.shape} -> X_ensemble={X_train_ensemble.shape} -> X_pca={X_train_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Sequences for Time Series Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, seq_length=128, stride=10):\n",
    "    \"\"\"\n",
    "    Create sequences for time series models.\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): Input data\n",
    "        seq_length (int): Sequence length\n",
    "        stride (int): Stride for sliding window\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Sequences\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(0, len(data) - seq_length + 1, stride):\n",
    "        seq = data[i:i+seq_length]\n",
    "        sequences.append(seq)\n",
    "    \n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "X_train_seq = create_sequences(X_train_pca, SEQ_LENGTH, STRIDE)\n",
    "print(f\"Training sequences shape: {X_train_seq.shape}\")\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train_seq, X_val_seq = train_test_split(X_train_seq, test_size=0.2, random_state=42)\n",
    "print(f\"Training set: {X_train_seq.shape}, Validation set: {X_val_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Improved LSTM Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_autoencoder(seq_length, n_features, latent_dim=64, dropout_rate=0.2, use_bidirectional=True, use_attention=True):\n",
    "    \"\"\"\n",
    "    Build an improved LSTM autoencoder model.\n",
    "    \n",
    "    Args:\n",
    "        seq_length (int): Sequence length\n",
    "        n_features (int): Number of features\n",
    "        latent_dim (int): Dimension of latent space\n",
    "        dropout_rate (float): Dropout rate\n",
    "        use_bidirectional (bool): Whether to use bidirectional LSTM\n",
    "        use_attention (bool): Whether to use attention mechanism\n",
    "        \n",
    "    Returns:\n",
    "        Model: Keras model\n",
    "    \"\"\"\n",
    "    # Input layer\n",
    "    inputs = Input(shape=(seq_length, n_features))\n",
    "    \n",
    "    # Encoder\n",
    "    if use_bidirectional:\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True))(inputs)\n",
    "    else:\n",
    "        x = LSTM(128, return_sequences=True)(inputs)\n",
    "    \n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    if use_bidirectional:\n",
    "        x = Bidirectional(LSTM(64, return_sequences=False))(x)\n",
    "    else:\n",
    "        x = LSTM(64, return_sequences=False)(x)\n",
    "    \n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # Latent space\n",
    "    encoded = Dense(latent_dim)(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Dense(64)(encoded)\n",
    "    x = RepeatVector(seq_length)(x)\n",
    "    \n",
    "    if use_bidirectional:\n",
    "        x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    else:\n",
    "        x = LSTM(64, return_sequences=True)(x)\n",
    "    \n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    if use_bidirectional:\n",
    "        x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "    else:\n",
    "        x = LSTM(128, return_sequences=True)(x)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = TimeDistributed(Dense(n_features))(x)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "model = build_lstm_autoencoder(\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    n_features=X_train_pca.shape[1],\n",
    "    latent_dim=32,\n",
    "    dropout_rate=0.2,\n",
    "    use_bidirectional=True,\n",
    "    use_attention=True\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    os.path.join(MODEL_DIR, 'improved_lstm_autoencoder.h5'),\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(MODEL_DIR, 'logs'))\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train_seq, X_train_seq,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_val_seq, X_val_seq),\n",
    "    callbacks=[early_stopping, model_checkpoint, reduce_lr, tensorboard],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate Reconstruction Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model = load_model(os.path.join(MODEL_DIR, 'improved_lstm_autoencoder.h5'))\n",
    "\n",
    "# Get predictions for validation set\n",
    "X_val_pred = model.predict(X_val_seq)\n",
    "\n",
    "# Calculate reconstruction errors (MSE)\n",
    "mse = np.mean(np.square(X_val_seq - X_val_pred), axis=(1, 2))\n",
    "\n",
    "# Plot reconstruction error distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(mse, bins=50, alpha=0.7)\n",
    "plt.title('Reconstruction Error Distribution')\n",
    "plt.xlabel('Reconstruction Error (MSE)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implement Adaptive Thresholding Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_thresholds(mse, methods=['percentile', 'iqr', 'dynamic']):\n",
    "    \"\"\"\n",
    "    Calculate different thresholds for anomaly detection.\n",
    "    \n",
    "    Args:\n",
    "        mse (np.array): Reconstruction errors\n",
    "        methods (list): List of threshold methods to use\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of thresholds\n",
    "    \"\"\"\n",
    "    thresholds = {}\n",
    "    \n",
    "    if 'percentile' in methods:\n",
    "        # Percentile-based threshold (e.g., 95th, 99th percentile)\n",
    "        thresholds['percentile_95'] = np.percentile(mse, 95)\n",
    "        thresholds['percentile_99'] = np.percentile(mse, 99)\n",
    "    \n",
    "    if 'iqr' in methods:\n",
    "        # IQR-based threshold\n",
    "        q1 = np.percentile(mse, 25)\n",
    "        q3 = np.percentile(mse, 75)\n",
    "        iqr = q3 - q1\n",
    "        thresholds['iqr'] = q3 + 1.5 * iqr\n",
    "    \n",
    "    if 'dynamic' in methods:\n",
    "        # Dynamic threshold based on mean and standard deviation\n",
    "        mean = np.mean(mse)\n",
    "        std = np.std(mse)\n",
    "        thresholds['dynamic'] = mean + 3 * std\n",
    "    \n",
    "    if 'kmeans' in methods:\n",
    "        # K-means clustering to separate normal and anomalous errors\n",
    "        from sklearn.cluster import KMeans\n",
    "        kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "        kmeans.fit(mse.reshape(-1, 1))\n",
    "        centers = kmeans.cluster_centers_.flatten()\n",
    "        thresholds['kmeans'] = (max(centers) + min(centers)) / 2\n",
    "    \n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate thresholds\n",
    "thresholds = calculate_thresholds(mse, methods=['percentile', 'iqr', 'dynamic', 'kmeans'])\n",
    "\n",
    "# Print thresholds\n",
    "for method, threshold in thresholds.items():\n",
    "    print(f\"{method} threshold: {threshold:.6f}\")\n",
    "\n",
    "# Plot reconstruction error distribution with thresholds\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(mse, bins=50, alpha=0.7)\n",
    "\n",
    "colors = ['r', 'g', 'b', 'm', 'c']\n",
    "for i, (method, threshold) in enumerate(thresholds.items()):\n",
    "    plt.axvline(x=threshold, color=colors[i % len(colors)], linestyle='--', label=f'{method} threshold: {threshold:.6f}')\n",
    "\n",
    "plt.title('Reconstruction Error Distribution with Thresholds')\n",
    "plt.xlabel('Reconstruction Error (MSE)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Implement Multi-Threshold Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiThresholdDetector:\n",
    "    \"\"\"\n",
    "    Multi-threshold anomaly detector that can adapt to different operating points.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, scaler, pca, ensemble_indices, seq_length, stride):\n",
    "        \"\"\"\n",
    "        Initialize the detector.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained autoencoder model\n",
    "            scaler: Feature scaler\n",
    "            pca: PCA transformer\n",
    "            ensemble_indices: Indices of selected features\n",
    "            seq_length: Sequence length\n",
    "            stride: Stride for sliding window\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "        self.pca = pca\n",
    "        self.ensemble_indices = ensemble_indices\n",
    "        self.seq_length = seq_length\n",
    "        self.stride = stride\n",
    "        self.thresholds = {}\n",
    "        self.operating_points = {}\n",
    "    \n",
    "    def fit(self, X_train, y_train=None):\n",
    "        \"\"\"\n",
    "        Fit the detector by calculating thresholds.\n",
    "        \n",
    "        Args:\n",
    "            X_train: Training data\n",
    "            y_train: Training labels (optional)\n",
    "        \"\"\"\n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.transform(X_train)\n",
    "        \n",
    "        # Select ensemble features\n",
    "        X_train_ensemble = X_train_scaled[:, self.ensemble_indices]\n",
    "        \n",
    "        # Apply PCA\n",
    "        X_train_pca = self.pca.transform(X_train_ensemble)\n",
    "        \n",
    "        # Create sequences\n",
    "        X_train_seq = create_sequences(X_train_pca, self.seq_length, self.stride)\n",
    "        \n",
    "        # Get predictions\n",
    "        X_train_pred = self.model.predict(X_train_seq)\n",
    "        \n",
    "        # Calculate reconstruction errors\n",
    "        mse = np.mean(np.square(X_train_seq - X_train_pred), axis=(1, 2))\n",
    "        \n",
    "        # Calculate thresholds\n",
    "        self.thresholds = calculate_thresholds(mse, methods=['percentile', 'iqr', 'dynamic', 'kmeans'])\n",
    "        \n",
    "        # Define operating points\n",
    "        self.operating_points = {\n",
    "            'high_precision': self.thresholds['percentile_99'],  # Fewer false positives\n",
    "            'balanced': self.thresholds['percentile_95'],        # Balanced precision and recall\n",
    "            'high_recall': self.thresholds['iqr'],               # Fewer false negatives\n",
    "            'adaptive': self.thresholds['dynamic']               # Adaptive to data distribution\n",
    "        }\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, operating_point='balanced'):\n",
    "        \"\"\"\n",
    "        Predict anomalies.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data\n",
    "            operating_point: Operating point to use ('high_precision', 'balanced', 'high_recall', 'adaptive')\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (anomaly_scores, anomaly_labels)\n",
    "        \"\"\"\n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # Select ensemble features\n",
    "        X_ensemble = X_scaled[:, self.ensemble_indices]\n",
    "        \n",
    "        # Apply PCA\n",
    "        X_pca = self.pca.transform(X_ensemble)\n",
    "        \n",
    "        # Create sequences\n",
    "        X_seq = create_sequences(X_pca, self.seq_length, self.stride)\n",
    "        \n",
    "        # Get predictions\n",
    "        X_pred = self.model.predict(X_seq)\n",
    "        \n",
    "        # Calculate reconstruction errors\n",
    "        mse = np.mean(np.square(X_seq - X_pred), axis=(1, 2))\n",
    "        \n",
    "        # Initialize anomaly scores array\n",
    "        anomaly_scores = np.zeros(len(X))\n",
    "        count = np.zeros(len(X))\n",
    "        \n",
    "        # For each sequence, if it's anomalous, increment the score for all points in the sequence\n",
    "        for i, error in enumerate(mse):\n",
    "            idx = i * self.stride\n",
    "            if idx + self.seq_length <= len(X):\n",
    "                anomaly_scores[idx:idx+self.seq_length] += error\n",
    "                count[idx:idx+self.seq_length] += 1\n",
    "        \n",
    "        # Normalize scores by count\n",
    "        anomaly_scores = np.divide(anomaly_scores, count, out=np.zeros_like(anomaly_scores), where=count!=0)\n",
    "        \n",
    "        # Get threshold for the specified operating point\n",
    "        threshold = self.operating_points.get(operating_point, self.operating_points['balanced'])\n",
    "        \n",
    "        # Apply threshold to get binary labels\n",
    "        anomaly_labels = (anomaly_scores > threshold).astype(int)\n",
    "        \n",
    "        return anomaly_scores, anomaly_labels, threshold\n",
    "    \n",
    "    def post_process_anomalies(self, anomaly_labels, min_anomaly_length=30, gap_threshold=3):\n",
    "        \"\"\"\n",
    "        Apply post-processing to reduce false positives and false negatives.\n",
    "        \n",
    "        Args:\n",
    "            anomaly_labels (np.array): Binary anomaly labels\n",
    "            min_anomaly_length (int): Minimum length of anomalies to keep\n",
    "            gap_threshold (int): Maximum gap between anomalies to merge\n",
    "            \n",
    "        Returns:\n",
    "            np.array: Processed binary anomaly labels\n",
    "        \"\"\"\n",
    "        # Make a copy to avoid modifying the original\n",
    "        processed_labels = anomaly_labels.copy()\n",
    "        \n",
    "        # Remove short anomalies (likely false positives)\n",
    "        i = 0\n",
    "        while i < len(processed_labels):\n",
    "            if processed_labels[i] == 1:\n",
    "                # Find the end of this anomaly\n",
    "                j = i\n",
    "                while j < len(processed_labels) and processed_labels[j] == 1:\n",
    "                    j += 1\n",
    "                \n",
    "                # If anomaly is too short, remove it\n",
    "                if j - i < min_anomaly_length:\n",
    "                    processed_labels[i:j] = 0\n",
    "                \n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        # Merge nearby anomalies\n",
    "        i = 0\n",
    "        while i < len(processed_labels):\n",
    "            if processed_labels[i] == 1:\n",
    "                # Find the end of this anomaly\n",
    "                j = i\n",
    "                while j < len(processed_labels) and processed_labels[j] == 1:\n",
    "                    j += 1\n",
    "                \n",
    "                # Look for another anomaly nearby\n",
    "                if j < len(processed_labels) - gap_threshold:\n",
    "                    next_start = j\n",
    "                    while next_start < j + gap_threshold and next_start < len(processed_labels) and processed_labels[next_start] == 0:\n",
    "                        next_start += 1\n",
    "                    \n",
    "                    if next_start < j + gap_threshold and next_start < len(processed_labels) and processed_labels[next_start] == 1:\n",
    "                        processed_labels[j:next_start] = 1\n",
    "                \n",
    "                i = j\n",
    "            else:\n",
    "                i += 1\n",
    "        \n",
    "        return processed_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize multi-threshold detector\n",
    "detector = MultiThresholdDetector(\n",
    "    model=model,\n",
    "    scaler=scaler,\n",
    "    pca=pca,\n",
    "    ensemble_indices=ensemble_indices,\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    stride=STRIDE\n",
    ")\n",
    "\n",
    "# Fit detector\n",
    "detector.fit(X_train)\n",
    "\n",
    "# Print operating points\n",
    "print(\"Operating points:\")\n",
    "for point, threshold in detector.operating_points.items():\n",
    "    print(f\"  {point}: {threshold:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Model and Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model parameters\n",
    "model_params = {\n",
    "    'seq_length': SEQ_LENGTH,\n",
    "    'stride': STRIDE,\n",
    "    'n_components': pca.n_components_,\n",
    "    'thresholds': detector.thresholds,\n",
    "    'operating_points': detector.operating_points,\n",
    "    'ensemble_features': ensemble_features,\n",
    "    'ensemble_indices': ensemble_indices\n",
    "}\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, 'improved_model_params.pkl'), 'wb') as f:\n",
    "    pickle.dump(model_params, f)\n",
    "\n",
    "print(f\"Saved model parameters to {os.path.join(MODEL_DIR, 'improved_model_params.pkl')}\")\n",
    "\n",
    "# Save detector\n",
    "with open(os.path.join(MODEL_DIR, 'multi_threshold_detector.pkl'), 'wb') as f:\n",
    "    pickle.dump(detector, f)\n",
    "\n",
    "print(f\"Saved detector to {os.path.join(MODEL_DIR, 'multi_threshold_detector.pkl')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test on Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "with open(os.path.join(FEATURE_DIR, 'test_data_selected.pkl'), 'rb') as f:\n",
    "    test_data_selected = pickle.load(f)\n",
    "\n",
    "# Get first test file\n",
    "test_name = list(test_data_selected.keys())[0]\n",
    "test_data = test_data_selected[test_name]\n",
    "\n",
    "# Load original test data to get features\n",
    "test_files = [f for f in os.listdir(OUTPUT_DIR) if f.startswith('test') and f.endswith('_processed_enhanced.csv')]\n",
    "test_df = pd.read_csv(os.path.join(OUTPUT_DIR, test_files[0]))\n",
    "\n",
    "# Extract features and target\n",
    "X_test = test_df[feature_cols].values\n",
    "y_test = test_df['attack'].values\n",
    "\n",
    "# Test detector with different operating points\n",
    "operating_points = ['high_precision', 'balanced', 'high_recall', 'adaptive']\n",
    "results = {}\n",
    "\n",
    "for point in operating_points:\n",
    "    print(f\"\\nTesting with operating point: {point}\")\n",
    "    \n",
    "    # Predict anomalies\n",
    "    anomaly_scores, anomaly_labels, threshold = detector.predict(X_test, operating_point=point)\n",
    "    \n",
    "    # Apply post-processing\n",
    "    processed_labels = detector.post_process_anomalies(anomaly_labels, min_anomaly_length=30, gap_threshold=3)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_test, processed_labels)\n",
    "    recall = recall_score(y_test, processed_labels)\n",
    "    f1 = f1_score(y_test, processed_labels)\n",
    "    \n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    results[point] = {\n",
    "        'threshold': threshold,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'anomaly_scores': anomaly_scores,\n",
    "        'anomaly_labels': processed_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results for different operating points\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Plot ground truth\n",
    "plt.subplot(len(operating_points) + 1, 1, 1)\n",
    "plt.plot(y_test, 'b-', label='Ground Truth')\n",
    "plt.title('Ground Truth')\n",
    "plt.ylabel('Anomaly')\n",
    "plt.yticks([0, 1])\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot predictions for each operating point\n",
    "for i, point in enumerate(operating_points):\n",
    "    plt.subplot(len(operating_points) + 1, 1, i + 2)\n",
    "    plt.plot(results[point]['anomaly_labels'], 'g-', label=f'Predictions ({point})')\n",
    "    plt.title(f'Predictions - {point} (Precision: {results[point][\"precision\"]:.4f}, Recall: {results[point][\"recall\"]:.4f}, F1: {results[point][\"f1_score\"]:.4f})')\n",
    "    plt.ylabel('Anomaly')\n",
    "    plt.yticks([0, 1])\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, results['balanced']['anomaly_scores'])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "\n",
    "# Mark operating points\n",
    "for point in operating_points:\n",
    "    # Find closest threshold\n",
    "    idx = np.argmin(np.abs(thresholds - results[point]['threshold']))\n",
    "    plt.plot(fpr[idx], tpr[idx], 'o', markersize=10, label=f'{point} (threshold={results[point][\"threshold\"]:.4f})')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Calculate Precision-Recall curve\n",
    "precision_values, recall_values, thresholds = precision_recall_curve(y_test, results['balanced']['anomaly_scores'])\n",
    "pr_auc = auc(recall_values, precision_values)\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.plot(recall_values, precision_values, color='darkorange', lw=2, label=f'PR curve (area = {pr_auc:.4f})')\n",
    "\n",
    "# Mark operating points\n",
    "for point in operating_points:\n",
    "    plt.plot(results[point]['recall'], results[point]['precision'], 'o', markersize=10, label=f'{point} (threshold={results[point][\"threshold\"]:.4f})')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "with open(os.path.join(MODEL_DIR, 'operating_point_results.pkl'), 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "\n",
    "print(f\"Saved operating point results to {os.path.join(MODEL_DIR, 'operating_point_results.pkl')}\")\n",
    "\n",
    "# Create a summary DataFrame\n",
    "summary = []\n",
    "for point in operating_points:\n",
    "    summary.append({\n",
    "        'Operating Point': point,\n",
    "        'Threshold': results[point]['threshold'],\n",
    "        'Precision': results[point]['precision'],\n",
    "        'Recall': results[point]['recall'],\n",
    "        'F1 Score': results[point]['f1_score']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df.to_csv(os.path.join(MODEL_DIR, 'operating_point_summary.csv'), index=False)\n",
    "\n",
    "print(f\"Saved operating point summary to {os.path.join(MODEL_DIR, 'operating_point_summary.csv')}\")\n",
    "summary_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}