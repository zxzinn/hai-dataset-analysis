{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI-22.04 Anomaly Detection Model Training\n",
    "\n",
    "This notebook trains anomaly detection models on the HAI-22.04 dataset. HAI-22.04 contains 88 columns including 86 data points. The training data does not contain attack labels, while the test data includes attack labels.\n",
    "\n",
    "We will train the following models:\n",
    "1. Isolation Forest\n",
    "2. PCA Reconstruction Error\n",
    "3. LSTM Autoencoder\n",
    "4. Variational Autoencoder (VAE)\n",
    "5. Ensemble Model\n",
    "\n",
    "All models will use GPU acceleration (where applicable) and will be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set dataset path\n",
    "data_path = '../hai-security-dataset/hai-22.04/'\n",
    "\n",
    "# Create directory for saving models if it doesn't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# List all training and test files\n",
    "train_files = sorted(glob(os.path.join(data_path, 'train*.csv')))\n",
    "test_files = sorted(glob(os.path.join(data_path, 'test*.csv')))\n",
    "\n",
    "print(f\"Training files: {[os.path.basename(f) for f in train_files]}\")\n",
    "print(f\"Test files: {[os.path.basename(f) for f in test_files]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load training data\n",
    "train_dfs = []\n",
    "for file in train_files:\n",
    "    print(f\"Loading {os.path.basename(file)}...\")\n",
    "    # HAI-22.04 uses comma as separator\n",
    "    df = pd.read_csv(file)\n",
    "    train_dfs.append(df)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# Concatenate all training data\n",
    "train_df = pd.concat(train_dfs, axis=0, ignore_index=True)\n",
    "print(f\"\\nCombined training data shape: {train_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load test data\n",
    "test_dfs = []\n",
    "for file in test_files:\n",
    "    print(f\"Loading {os.path.basename(file)}...\")\n",
    "    # HAI-22.04 uses comma as separator\n",
    "    df = pd.read_csv(file)\n",
    "    test_dfs.append(df)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# Concatenate all test data\n",
    "test_df = pd.concat(test_dfs, axis=0, ignore_index=True)\n",
    "print(f\"\\nCombined test data shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Exploration and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check column names in training data\n",
    "print(\"Training data column names:\")\n",
    "print(train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check column names in test data\n",
    "print(\"Test data column names:\")\n",
    "print(test_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in training data:\")\n",
    "print(train_df.isnull().sum().sum())\n",
    "\n",
    "print(\"\\nMissing values in test data:\")\n",
    "print(test_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert timestamp column to datetime format\n",
    "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
    "test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n",
    "\n",
    "# Check time range\n",
    "print(f\"Training data time range: {train_df['timestamp'].min()} to {train_df['timestamp'].max()}\")\n",
    "print(f\"Test data time range: {test_df['timestamp'].min()} to {test_df['timestamp'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check attack labels\n",
    "attack_columns = [col for col in test_df.columns if 'attack' in col.lower()]\n",
    "print(f\"Attack label columns: {attack_columns}\")\n",
    "\n",
    "# Calculate attack ratio\n",
    "for col in attack_columns:\n",
    "    attack_count = test_df[col].sum()\n",
    "    attack_percentage = (attack_count / len(test_df)) * 100\n",
    "    print(f\"{col}: {attack_count} attacks ({attack_percentage:.2f}% of data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot attack distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(test_df['timestamp'], test_df['Attack'])\n",
    "plt.title('Attack Distribution')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Attack (1) / Normal (0)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Separate features and labels\n",
    "# Exclude timestamp column and attack label columns\n",
    "feature_columns = [col for col in train_df.columns if col != 'timestamp' and 'attack' not in col.lower()]\n",
    "\n",
    "# Training data\n",
    "X_train = train_df[feature_columns].values\n",
    "\n",
    "# Test data\n",
    "X_test = test_df[feature_columns].values\n",
    "y_test = test_df['Attack'].values\n",
    "\n",
    "print(f\"Training features shape: {X_train.shape}\")\n",
    "print(f\"Test features shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Save scaler\n",
    "with open('models/hai_22_04_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation\n",
    "\n",
    "### 5.1 Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create and train Isolation Forest model\n",
    "print(\"Training Isolation Forest model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "iso_forest = IsolationForest(n_estimators=100, contamination=0.1, random_state=42, n_jobs=-1)\n",
    "iso_forest.fit(X_train_scaled)\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Save model\n",
    "with open('models/hai_22_04_isolation_forest.pkl', 'wb') as f:\n",
    "    pickle.dump(iso_forest, f)\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Make predictions on test set\n",
    "# Isolation Forest returns anomaly scores, need to convert to binary labels\n",
    "# Prediction value of 1 means normal, -1 means anomaly, need to convert to 0 for normal, 1 for anomaly\n",
    "y_pred_iso = iso_forest.predict(X_test_scaled)\n",
    "y_pred_iso = np.where(y_pred_iso == -1, 1, 0)  # Convert labels\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision = precision_score(y_test, y_pred_iso)\n",
    "recall = recall_score(y_test, y_pred_iso)\n",
    "f1 = f1_score(y_test, y_pred_iso)\n",
    "\n",
    "print(f\"Isolation Forest model evaluation:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_iso)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
    "plt.title('Isolation Forest Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 PCA Reconstruction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create and train PCA model\n",
    "print(\"Training PCA model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Choose to retain 95% of variance\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "print(f\"Number of principal components selected: {pca.n_components_}\")\n",
    "\n",
    "# Save model\n",
    "with open('models/hai_22_04_pca.pkl', 'wb') as f:\n",
    "    pickle.dump(pca, f)\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate reconstruction error\n",
    "def reconstruction_error(pca, X):\n",
    "    X_transformed = pca.transform(X)\n",
    "    X_reconstructed = pca.inverse_transform(X_transformed)\n",
    "    error = np.mean(np.square(X - X_reconstructed), axis=1)\n",
    "    return error\n",
    "\n",
    "# Calculate reconstruction error for training set\n",
    "train_error = reconstruction_error(pca, X_train_scaled)\n",
    "\n",
    "# Calculate reconstruction error for test set\n",
    "test_error = reconstruction_error(pca, X_test_scaled)\n",
    "\n",
    "# Set threshold (using 95th percentile of training errors)\n",
    "threshold = np.percentile(train_error, 95)\n",
    "print(f\"Threshold: {threshold:.6f}\")\n",
    "\n",
    "# Make predictions based on threshold\n",
    "y_pred_pca = (test_error > threshold).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision = precision_score(y_test, y_pred_pca)\n",
    "recall = recall_score(y_test, y_pred_pca)\n",
    "f1 = f1_score(y_test, y_pred_pca)\n",
    "\n",
    "print(f\"PCA Reconstruction Error model evaluation:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_pca)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
    "plt.title('PCA Reconstruction Error Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 LSTM Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define LSTM Autoencoder model\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1, dropout=0.2):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=hidden_dim,\n",
    "            hidden_size=input_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        _, (hidden, _) = self.encoder(x)\n",
    "        \n",
    "        # Use the last layer's hidden state\n",
    "        hidden_repeat = hidden[-1].unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        \n",
    "        # Decode\n",
    "        output, _ = self.decoder(hidden_repeat)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare time series data\n",
    "def create_sequences(data, seq_length):\n",
    "    xs = []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        x = data[i:(i + seq_length)]\n",
    "        xs.append(x)\n",
    "    return np.array(xs)\n",
    "\n",
    "# Set sequence length\n",
    "seq_length = 10\n",
    "\n",
    "# Create training sequences\n",
    "X_train_seq = create_sequences(X_train_scaled, seq_length)\n",
    "print(f\"Training sequences shape: {X_train_seq.shape}\")\n",
    "\n",
    "# Create test sequences\n",
    "X_test_seq = create_sequences(X_test_scaled, seq_length)\n",
    "y_test_seq = y_test[seq_length-1:]\n",
    "print(f\"Test sequences shape: {X_test_seq.shape}\")\n",
    "print(f\"Test labels shape: {y_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_seq).to(device)\n",
    "X_test_tensor = torch.FloatTensor(X_test_seq).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train_tensor, X_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "\n",
    "model = LSTMAutoencoder(input_dim, hidden_dim, num_layers).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Print model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train LSTM Autoencoder\n",
    "print(\"Training LSTM Autoencoder...\")\n",
    "start_time = time.time()\n",
    "\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.6f}\")\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'models/hai_22_04_lstm_autoencoder.pt')\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses)\n",
    "plt.title('LSTM Autoencoder Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate LSTM Autoencoder\n",
    "model.eval()\n",
    "\n",
    "# Calculate reconstruction error on training data\n",
    "with torch.no_grad():\n",
    "    X_train_pred = model(X_train_tensor).cpu().numpy()\n",
    "    train_mse = np.mean(np.square(X_train_seq - X_train_pred), axis=(1, 2))\n",
    "    \n",
    "    X_test_pred = model(X_test_tensor).cpu().numpy()\n",
    "    test_mse = np.mean(np.square(X_test_seq - X_test_pred), axis=(1, 2))\n",
    "\n",
    "# Set threshold (using 95th percentile of training errors)\n",
    "threshold = np.percentile(train_mse, 95)\n",
    "print(f\"Threshold: {threshold:.6f}\")\n",
    "\n",
    "# Make predictions based on threshold\n",
    "y_pred_lstm = (test_mse > threshold).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision = precision_score(y_test_seq, y_pred_lstm)\n",
    "recall = recall_score(y_test_seq, y_pred_lstm)\n",
    "f1 = f1_score(y_test_seq, y_pred_lstm)\n",
    "\n",
    "print(f\"LSTM Autoencoder model evaluation:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test_seq, y_pred_lstm)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
    "plt.title('LSTM Autoencoder Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, latent_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Mean and variance layers\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    \n",
    "    def encode(self, x):\n",
    "        hidden = self.encoder(x)\n",
    "        mu = self.fc_mu(hidden)\n",
    "        log_var = self.fc_var(hidden)\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prepare data for VAE\n",
    "X_train_tensor_flat = torch.FloatTensor(X_train_scaled).to(device)\n",
    "X_test_tensor_flat = torch.FloatTensor(X_test_scaled).to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset_vae = TensorDataset(X_train_tensor_flat, X_train_tensor_flat)\n",
    "train_loader_vae = DataLoader(train_dataset_vae, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "hidden_dim = 64\n",
    "latent_dim = 20\n",
    "\n",
    "vae_model = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
    "optimizer = optim.Adam(vae_model.parameters(), lr=0.001)\n",
    "\n",
    "# Print model summary\n",
    "print(vae_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# VAE loss function\n",
    "def vae_loss_function(recon_x, x, mu, log_var):\n",
    "    # Reconstruction loss\n",
    "    recon_loss = nn.MSELoss(reduction='sum')(recon_x, x)\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    \n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train VAE\n",
    "print(\"Training VAE...\")\n",
    "start_time = time.time()\n",
    "\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    vae_model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader_vae):\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, log_var = vae_model(data)\n",
    "        loss = vae_loss_function(recon_batch, data, mu, log_var)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader_vae.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {train_loss:.6f}\")\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Save model\n",
    "torch.save(vae_model.state_dict(), 'models/hai_22_04_vae.pt')\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses)\n",
    "plt.title('VAE Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate VAE\n",
    "vae_model.eval()\n",
    "\n",
    "# Calculate reconstruction error on training data\n",
    "with torch.no_grad():\n",
    "    X_train_recon, _, _ = vae_model(X_train_tensor_flat)\n",
    "    train_mse = nn.MSELoss(reduction='none')(X_train_recon, X_train_tensor_flat).mean(dim=1).cpu().numpy()\n",
    "    \n",
    "    X_test_recon, _, _ = vae_model(X_test_tensor_flat)\n",
    "    test_mse = nn.MSELoss(reduction='none')(X_test_recon, X_test_tensor_flat).mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Set threshold (using 95th percentile of training errors)\n",
    "threshold = np.percentile(train_mse, 95)\n",
    "print(f\"Threshold: {threshold:.6f}\")\n",
    "\n",
    "# Make predictions based on threshold\n",
    "y_pred_vae = (test_mse > threshold).astype(int)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision = precision_score(y_test, y_pred_vae)\n",
    "recall = recall_score(y_test, y_pred_vae)\n",
    "f1 = f1_score(y_test, y_pred_vae)\n",
    "\n",
    "print(f\"VAE model evaluation:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_vae)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
    "plt.title('VAE Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create ensemble predictions\n",
    "# For LSTM predictions, we need to align with other predictions\n",
    "y_pred_lstm_aligned = np.zeros_like(y_test)\n",
    "y_pred_lstm_aligned[seq_length-1:] = y_pred_lstm\n",
    "\n",
    "# Create ensemble by majority voting\n",
    "ensemble_pred = np.zeros_like(y_test)\n",
    "for i in range(len(y_test)):\n",
    "    votes = [y_pred_iso[i], y_pred_pca[i], y_pred_vae[i]]\n",
    "    if i >= seq_length-1:\n",
    "        votes.append(y_pred_lstm_aligned[i])\n",
    "    \n",
    "    # Majority vote\n",
    "    ensemble_pred[i] = 1 if sum(votes) >= len(votes)/2 else 0\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "precision = precision_score(y_test, ensemble_pred)\n",
    "recall = recall_score(y_test, ensemble_pred)\n",
    "f1 = f1_score(y_test, ensemble_pred)\n",
    "\n",
    "print(f\"Ensemble model evaluation:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_test, ensemble_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Anomaly'], yticklabels=['Normal', 'Anomaly'])\n",
    "plt.title('Ensemble Model Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare all models\n",
    "models = ['Isolation Forest', 'PCA', 'LSTM Autoencoder', 'VAE', 'Ensemble']\n",
    "predictions = [y_pred_iso, y_pred_pca, y_pred_lstm_aligned, y_pred_vae, ensemble_pred]\n",
    "\n",
    "# Calculate metrics for all models\n",
    "results = []\n",
    "for model_name, y_pred in zip(models, predictions):\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results.append([model_name, precision, recall, f1])\n",
    "\n",
    "# Create DataFrame for results\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Precision', 'Recall', 'F1 Score'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x - width, results_df['Precision'], width, label='Precision')\n",
    "plt.bar(x, results_df['Recall'], width, label='Recall')\n",
    "plt.bar(x + width, results_df['F1 Score'], width, label='F1 Score')\n",
    "\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(x, models, rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}