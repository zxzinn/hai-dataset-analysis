{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model for HAI Security Dataset Anomaly Detection\n",
    "\n",
    "This notebook implements a Random Forest classifier for anomaly detection on the HAI security dataset. Random Forests are effective for this task due to their ability to handle high-dimensional data and capture complex relationships between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data\n",
    "\n",
    "First, let's load the preprocessed data created in the preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_processed_data(file_path):\n",
    "    \"\"\"\n",
    "    Load processed data from NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the NPZ file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Loaded data\n",
    "    \"\"\"\n",
    "    # Load NPZ file\n",
    "    npz_data = np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(npz_data['data'], columns=npz_data['columns'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load preprocessor\n",
    "preprocessor_path = './models/hai_hai_20_07_standard_preprocessor.joblib'\n",
    "preprocessor_dict = joblib.load(preprocessor_path)\n",
    "\n",
    "# Extract important information\n",
    "feature_columns = preprocessor_dict['feature_columns']\n",
    "attack_columns = preprocessor_dict['attack_columns']\n",
    "timestamp_col = preprocessor_dict['timestamp_col']\n",
    "\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "print(f\"Attack columns: {attack_columns}\")\n",
    "print(f\"Timestamp column: {timestamp_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get list of processed data files\n",
    "train_data_dir = './processed_data/hai-20.07/train'\n",
    "test_data_dir = './processed_data/hai-20.07/test'\n",
    "\n",
    "train_files = sorted(glob.glob(f'{train_data_dir}/*.npz'))\n",
    "test_files = sorted(glob.glob(f'{test_data_dir}/*.npz'))\n",
    "\n",
    "print(f\"Training files: {[os.path.basename(f) for f in train_files]}\")\n",
    "print(f\"Test files: {[os.path.basename(f) for f in test_files]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Random Forest\n",
    "\n",
    "Unlike LSTM, Random Forest doesn't require sequence data. We'll use individual time points as samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_and_prepare_tabular_data(file_paths, feature_cols, target_col=None, max_files=None, sample_fraction=None):\n",
    "    \"\"\"\n",
    "    Load and prepare tabular data from multiple files.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of file paths\n",
    "        feature_cols: List of feature column names\n",
    "        target_col: Target column name (None for unsupervised learning)\n",
    "        max_files: Maximum number of files to load (None for all files)\n",
    "        sample_fraction: Fraction of data to sample (None for all data)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y) - Features and targets\n",
    "    \"\"\"\n",
    "    all_X = []\n",
    "    all_y = [] if target_col is not None else None\n",
    "    \n",
    "    # Limit the number of files if specified\n",
    "    if max_files is not None:\n",
    "        file_paths = file_paths[:max_files]\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        \n",
    "        # Load data\n",
    "        df = load_processed_data(file_path)\n",
    "        \n",
    "        # Sample data if specified\n",
    "        if sample_fraction is not None and sample_fraction < 1.0:\n",
    "            df = df.sample(frac=sample_fraction, random_state=42)\n",
    "        \n",
    "        # Extract features\n",
    "        X = df[feature_cols]\n",
    "        all_X.append(X)\n",
    "        \n",
    "        # Extract target if provided\n",
    "        if target_col is not None and target_col in df.columns:\n",
    "            y = df[target_col]\n",
    "            all_y.append(y)\n",
    "    \n",
    "    # Combine data from all files\n",
    "    combined_X = pd.concat(all_X) if all_X else pd.DataFrame()\n",
    "    combined_y = pd.concat(all_y) if all_y else None\n",
    "    \n",
    "    return combined_X, combined_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set parameters\n",
    "target_col = 'attack' if attack_columns else None  # Target column\n",
    "sample_fraction = 0.1  # Sample 10% of data to reduce memory usage\n",
    "\n",
    "# Load and prepare training data\n",
    "print(\"Loading and preparing training data...\")\n",
    "X_train, _ = load_and_prepare_tabular_data(train_files, feature_columns, target_col=None, \n",
    "                                          max_files=2, sample_fraction=sample_fraction)\n",
    "\n",
    "# Load and prepare test data\n",
    "print(\"\\nLoading and preparing test data...\")\n",
    "X_test, y_test = load_and_prepare_tabular_data(test_files, feature_columns, target_col=target_col, \n",
    "                                              max_files=2, sample_fraction=sample_fraction)\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
    "if y_test is not None:\n",
    "    print(f\"Test data shape: {X_test.shape}, Test labels shape: {y_test.shape}\")\n",
    "else:\n",
    "    print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Selection\n",
    "\n",
    "Let's perform feature selection to reduce dimensionality and improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def select_features(X_train, X_test, n_estimators=100, max_features=20):\n",
    "    \"\"\"\n",
    "    Select important features using a Random Forest.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        X_test: Test features\n",
    "        n_estimators: Number of trees in the forest\n",
    "        max_features: Maximum number of features to select\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X_train_selected, X_test_selected, selected_features) - Selected data and feature names\n",
    "    \"\"\"\n",
    "    print(\"Performing feature selection...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train a Random Forest for feature importance\n",
    "    rf = RandomForestClassifier(n_estimators=n_estimators, random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # Create synthetic target for unsupervised feature selection\n",
    "    # We'll use Isolation Forest to create pseudo-labels\n",
    "    iso_forest = IsolationForest(random_state=42, n_jobs=-1)\n",
    "    pseudo_labels = iso_forest.fit_predict(X_train)\n",
    "    # Convert to binary classification (1 for normal, 0 for anomaly)\n",
    "    pseudo_labels = np.where(pseudo_labels == 1, 1, 0)\n",
    "    \n",
    "    # Fit Random Forest with pseudo-labels\n",
    "    rf.fit(X_train, pseudo_labels)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = rf.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Select top features\n",
    "    selector = SelectFromModel(rf, max_features=max_features, prefit=True)\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    \n",
    "    # Get names of selected features\n",
    "    selected_mask = selector.get_support()\n",
    "    selected_features = X_train.columns[selected_mask].tolist()\n",
    "    \n",
    "    print(f\"Feature selection completed in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Selected {len(selected_features)} features out of {X_train.shape[1]}\")\n",
    "    \n",
    "    return X_train_selected, X_test_selected, selected_features, importances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Perform feature selection\n",
    "X_train_selected, X_test_selected, selected_features, importances, indices = select_features(X_train, X_test, max_features=20)\n",
    "\n",
    "print(f\"Selected features: {selected_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(min(20, len(indices))), importances[indices[:20]], align=\"center\")\n",
    "plt.xticks(range(min(20, len(indices))), [X_train.columns[i] for i in indices[:20]], rotation=90)\n",
    "plt.xlim([-1, min(20, len(indices))])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build and Train Random Forest Model\n",
    "\n",
    "Now we'll build and train a Random Forest model for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_random_forest(X_train, y_train=None, param_grid=None, cv=3, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Train a Random Forest model with hyperparameter tuning.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        y_train: Training labels (None for unsupervised learning)\n",
    "        param_grid: Grid of hyperparameters to search\n",
    "        cv: Number of cross-validation folds\n",
    "        n_jobs: Number of parallel jobs\n",
    "        \n",
    "    Returns:\n",
    "        RandomForestClassifier: Trained model\n",
    "    \"\"\"\n",
    "    print(\"Training Random Forest model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Default hyperparameters\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'bootstrap': [True, False]\n",
    "        }\n",
    "    \n",
    "    # Create synthetic target for unsupervised learning if needed\n",
    "    if y_train is None:\n",
    "        # Use Isolation Forest to create pseudo-labels\n",
    "        iso_forest = IsolationForest(random_state=42, n_jobs=n_jobs)\n",
    "        y_train = iso_forest.fit_predict(X_train)\n",
    "        # Convert to binary classification (1 for normal, 0 for anomaly)\n",
    "        y_train = np.where(y_train == 1, 1, 0)\n",
    "    \n",
    "    # Initialize Random Forest\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # Use RandomizedSearchCV for faster hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(rf, param_distributions=param_grid, n_iter=10, \n",
    "                                      cv=cv, verbose=1, random_state=42, n_jobs=n_jobs)\n",
    "    \n",
    "    # Fit model\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    print(f\"Training completed in {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Best parameters: {random_search.best_params_}\")\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a smaller parameter grid for faster training\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 20],\n",
    "    'min_samples_split': [2, 10],\n",
    "    'min_samples_leaf': [1, 4],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = train_random_forest(X_train_selected, y_train=None, param_grid=param_grid, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model and Detect Anomalies\n",
    "\n",
    "Now we'll evaluate the model's performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Predict on test data\n",
    "y_pred_proba = rf_model.predict_proba(X_test_selected)[:, 1]  # Probability of normal class\n",
    "anomaly_scores = 1 - y_pred_proba  # Convert to anomaly score (higher = more anomalous)\n",
    "\n",
    "print(f\"Anomaly score statistics:\")\n",
    "print(f\"Min: {np.min(anomaly_scores):.6f}\")\n",
    "print(f\"Max: {np.max(anomaly_scores):.6f}\")\n",
    "print(f\"Mean: {np.mean(anomaly_scores):.6f}\")\n",
    "print(f\"Std: {np.std(anomaly_scores):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot anomaly score distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(anomaly_scores, bins=50)\n",
    "plt.title('Anomaly Score Distribution')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(anomaly_scores, bins=50, log=True)\n",
    "plt.title('Anomaly Score Distribution (Log Scale)')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt.ylabel('Frequency (Log Scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Determine Anomaly Threshold\n",
    "\n",
    "We need to determine a threshold for the anomaly score to classify data points as normal or anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def find_optimal_threshold(scores, y_true):\n",
    "    \"\"\"\n",
    "    Find the optimal threshold for anomaly detection using ROC curve.\n",
    "    \n",
    "    Args:\n",
    "        scores: Anomaly scores\n",
    "        y_true: True labels (0 for normal, 1 for anomaly)\n",
    "        \n",
    "    Returns:\n",
    "        float: Optimal threshold value\n",
    "    \"\"\"\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, scores)\n",
    "    \n",
    "    # Calculate the geometric mean of sensitivity and specificity\n",
    "    gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "    \n",
    "    # Find the optimal threshold\n",
    "    ix = np.argmax(gmeans)\n",
    "    optimal_threshold = thresholds[ix]\n",
    "    \n",
    "    print(f\"Optimal threshold: {optimal_threshold:.6f}\")\n",
    "    print(f\"At this threshold - TPR: {tpr[ix]:.4f}, FPR: {fpr[ix]:.4f}, G-mean: {gmeans[ix]:.4f}\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.scatter(fpr[ix], tpr[ix], marker='o', color='red', label=f'Optimal (Threshold = {optimal_threshold:.6f})')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate AUC\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Find optimal threshold if labels are available\n",
    "if y_test is not None:\n",
    "    threshold = find_optimal_threshold(anomaly_scores, y_test)\n",
    "else:\n",
    "    # If no labels, use a statistical approach\n",
    "    threshold = np.mean(anomaly_scores) + 2 * np.std(anomaly_scores)  # Mean + 2 standard deviations\n",
    "    print(f\"Using statistical threshold: {threshold:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Classify as anomaly if anomaly score > threshold\n",
    "y_pred = (anomaly_scores > threshold).astype(int)\n",
    "\n",
    "# Evaluate if labels are available\n",
    "if y_test is not None:\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Importance Analysis\n",
    "\n",
    "Let's analyze which features are most important for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get feature importances from the model\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame for easier visualization\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "plt.title('Feature Importances for Anomaly Detection')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Analyze Anomaly Patterns\n",
    "\n",
    "Let's analyze the patterns of detected anomalies to understand their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get indices of anomalies\n",
    "anomaly_indices = np.where(y_pred == 1)[0]\n",
    "\n",
    "if len(anomaly_indices) > 0:\n",
    "    # Get anomalous samples\n",
    "    anomaly_samples = X_test.iloc[anomaly_indices]\n",
    "    \n",
    "    # Calculate statistics for each feature in anomalous samples\n",
    "    anomaly_stats = anomaly_samples.describe()\n",
    "    \n",
    "    # Compare with overall statistics\n",
    "    overall_stats = X_test.describe()\n",
    "    \n",
    "    # Calculate the difference in means (as a percentage of overall standard deviation)\n",
    "    mean_diff = (anomaly_stats.loc['mean'] - overall_stats.loc['mean']) / overall_stats.loc['std']\n",
    "    \n",
    "    # Sort features by absolute difference\n",
    "    sorted_features = mean_diff.abs().sort_values(ascending=False)\n",
    "    \n",
    "    # Plot top 10 features with largest differences\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x=mean_diff[sorted_features.index[:10]], y=sorted_features.index[:10])\n",
    "    plt.title('Top 10 Features with Largest Differences in Anomalies (Normalized)')\n",
    "    plt.xlabel('Difference in Means (normalized by std)')\n",
    "    plt.axvline(x=0, color='r', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 10 features with largest differences in anomalies:\")\n",
    "    for feature in sorted_features.index[:10]:\n",
    "        print(f\"{feature}: Normal mean = {overall_stats.loc['mean', feature]:.4f}, \"\n",
    "              f\"Anomaly mean = {anomaly_stats.loc['mean', feature]:.4f}, \"\n",
    "              f\"Difference = {mean_diff[feature]:.4f} std\")\n",
    "else:\n",
    "    print(\"No anomalies detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model and Results\n",
    "\n",
    "Finally, let's save the model and results for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save model\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "joblib.dump(rf_model, './models/random_forest_hai_20_07.joblib')\n",
    "\n",
    "# Save metadata\n",
    "model_metadata = {\n",
    "    'threshold': threshold,\n",
    "    'selected_features': selected_features,\n",
    "    'feature_importances': feature_importances.tolist()\n",
    "}\n",
    "\n",
    "joblib.dump(model_metadata, './models/random_forest_metadata_hai_20_07.joblib')\n",
    "print(\"Model and metadata saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we've implemented a Random Forest model for anomaly detection on the HAI security dataset. The model identifies anomalies based on feature patterns learned from normal data. Key steps included:\n",
    "\n",
    "1. Loading and preparing preprocessed data\n",
    "2. Performing feature selection to identify the most important features\n",
    "3. Building and training a Random Forest model\n",
    "4. Detecting anomalies using anomaly scores\n",
    "5. Evaluating the model's performance\n",
    "6. Analyzing feature importance and anomaly patterns\n",
    "\n",
    "The Random Forest approach provides an effective method for detecting anomalies in industrial control system data, with the added benefit of interpretability through feature importance analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}