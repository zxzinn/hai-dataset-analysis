{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning for HAI Security Dataset Anomaly Detection\n",
    "\n",
    "This notebook implements ensemble learning methods to combine the strengths of multiple anomaly detection models (LSTM, Random Forest, and Autoencoder) for the HAI security dataset. Ensemble methods can improve detection performance by leveraging the complementary strengths of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data and Trained Models\n",
    "\n",
    "First, let's load the preprocessed data and the trained models from the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_processed_data(file_path):\n",
    "    \"\"\"\n",
    "    Load processed data from NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the NPZ file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Loaded data\n",
    "    \"\"\"\n",
    "    # Load NPZ file\n",
    "    npz_data = np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(npz_data['data'], columns=npz_data['columns'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load preprocessor\n",
    "preprocessor_path = './models/hai_hai_20_07_standard_preprocessor.joblib'\n",
    "preprocessor_dict = joblib.load(preprocessor_path)\n",
    "\n",
    "# Extract important information\n",
    "feature_columns = preprocessor_dict['feature_columns']\n",
    "attack_columns = preprocessor_dict['attack_columns']\n",
    "timestamp_col = preprocessor_dict['timestamp_col']\n",
    "\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "print(f\"Attack columns: {attack_columns}\")\n",
    "print(f\"Timestamp column: {timestamp_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get list of processed data files\n",
    "train_data_dir = './processed_data/hai-20.07/train'\n",
    "test_data_dir = './processed_data/hai-20.07/test'\n",
    "\n",
    "train_files = sorted(glob.glob(f'{train_data_dir}/*.npz'))\n",
    "test_files = sorted(glob.glob(f'{test_data_dir}/*.npz'))\n",
    "\n",
    "print(f\"Training files: {[os.path.basename(f) for f in train_files]}\")\n",
    "print(f\"Test files: {[os.path.basename(f) for f in test_files]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n",
    "\n",
    "We need to prepare both tabular data (for Random Forest and Autoencoder) and sequence data (for LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_and_prepare_tabular_data(file_paths, feature_cols, target_col=None, max_files=None, sample_fraction=None):\n",
    "    \"\"\"\n",
    "    Load and prepare tabular data from multiple files.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of file paths\n",
    "        feature_cols: List of feature column names\n",
    "        target_col: Target column name (None for unsupervised learning)\n",
    "        max_files: Maximum number of files to load (None for all files)\n",
    "        sample_fraction: Fraction of data to sample (None for all data)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y) - Features and targets\n",
    "    \"\"\"\n",
    "    all_X = []\n",
    "    all_y = [] if target_col is not None else None\n",
    "    \n",
    "    # Limit the number of files if specified\n",
    "    if max_files is not None:\n",
    "        file_paths = file_paths[:max_files]\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        \n",
    "        # Load data\n",
    "        df = load_processed_data(file_path)\n",
    "        \n",
    "        # Sample data if specified\n",
    "        if sample_fraction is not None and sample_fraction < 1.0:\n",
    "            df = df.sample(frac=sample_fraction, random_state=42)\n",
    "        \n",
    "        # Extract features\n",
    "        X = df[feature_cols]\n",
    "        all_X.append(X)\n",
    "        \n",
    "        # Extract target if provided\n",
    "        if target_col is not None and target_col in df.columns:\n",
    "            y = df[target_col]\n",
    "            all_y.append(y)\n",
    "    \n",
    "    # Combine data from all files\n",
    "    combined_X = pd.concat(all_X) if all_X else pd.DataFrame()\n",
    "    combined_y = pd.concat(all_y) if all_y else None\n",
    "    \n",
    "    return combined_X, combined_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_sequences(data, feature_cols, target_col=None, seq_length=100):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM input.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame containing the data\n",
    "        feature_cols: List of feature column names\n",
    "        target_col: Target column name (None for unsupervised learning)\n",
    "        seq_length: Length of each sequence\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y) - Sequences and targets (if target_col is provided)\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = [] if target_col is not None else None\n",
    "    \n",
    "    # Extract features\n",
    "    features = data[feature_cols].values\n",
    "    \n",
    "    # Extract target if provided\n",
    "    targets = data[target_col].values if target_col is not None else None\n",
    "    \n",
    "    # Create sequences\n",
    "    for i in range(len(features) - seq_length):\n",
    "        X.append(features[i:i+seq_length])\n",
    "        if target_col is not None:\n",
    "            # Use the label of the last timestep in the sequence\n",
    "            y.append(targets[i+seq_length])\n",
    "    \n",
    "    return np.array(X), np.array(y) if target_col is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_and_prepare_sequence_data(file_paths, feature_cols, target_col=None, seq_length=100, max_files=None):\n",
    "    \"\"\"\n",
    "    Load and prepare sequence data from multiple files.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of file paths\n",
    "        feature_cols: List of feature column names\n",
    "        target_col: Target column name (None for unsupervised learning)\n",
    "        seq_length: Length of each sequence\n",
    "        max_files: Maximum number of files to load (None for all files)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y) - Combined sequences and targets\n",
    "    \"\"\"\n",
    "    all_X = []\n",
    "    all_y = [] if target_col is not None else None\n",
    "    \n",
    "    # Limit the number of files if specified\n",
    "    if max_files is not None:\n",
    "        file_paths = file_paths[:max_files]\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        \n",
    "        # Load data\n",
    "        df = load_processed_data(file_path)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = create_sequences(df, feature_cols, target_col, seq_length)\n",
    "        \n",
    "        all_X.append(X)\n",
    "        if target_col is not None:\n",
    "            all_y.append(y)\n",
    "    \n",
    "    # Combine data from all files\n",
    "    combined_X = np.vstack(all_X) if all_X else np.array([])\n",
    "    combined_y = np.concatenate(all_y) if all_y else None\n",
    "    \n",
    "    return combined_X, combined_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set parameters\n",
    "target_col = 'attack' if attack_columns else None  # Target column\n",
    "sample_fraction = 0.1  # Sample 10% of data to reduce memory usage\n",
    "seq_length = 100  # Sequence length for LSTM\n",
    "\n",
    "# Load and prepare tabular test data (for Random Forest and Autoencoder)\n",
    "print(\"Loading and preparing tabular test data...\")\n",
    "X_test_tabular, y_test_tabular = load_and_prepare_tabular_data(test_files, feature_columns, \n",
    "                                                              target_col=target_col, \n",
    "                                                              max_files=2, \n",
    "                                                              sample_fraction=sample_fraction)\n",
    "\n",
    "# Load and prepare sequence test data (for LSTM)\n",
    "print(\"\\nLoading and preparing sequence test data...\")\n",
    "X_test_sequence, y_test_sequence = load_and_prepare_sequence_data(test_files, feature_columns, \n",
    "                                                                 target_col=target_col, \n",
    "                                                                 seq_length=seq_length, \n",
    "                                                                 max_files=2)\n",
    "\n",
    "print(f\"\\nTabular test data shape: {X_test_tabular.shape}, Labels shape: {y_test_tabular.shape}\")\n",
    "print(f\"Sequence test data shape: {X_test_sequence.shape}, Labels shape: {y_test_sequence.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Trained Models\n",
    "\n",
    "Now let's load the trained models from the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if models exist\n",
    "model_files = {\n",
    "    'lstm': './models/lstm_autoencoder_hai_20_07.h5',\n",
    "    'random_forest': './models/random_forest_hai_20_07.joblib',\n",
    "    'autoencoder': './models/autoencoder_hai_20_07.h5'\n",
    "}\n",
    "\n",
    "metadata_files = {\n",
    "    'lstm': './models/lstm_model_metadata_hai_20_07.joblib',\n",
    "    'random_forest': './models/random_forest_metadata_hai_20_07.joblib',\n",
    "    'autoencoder': './models/autoencoder_metadata_hai_20_07.joblib'\n",
    "}\n",
    "\n",
    "# Check which models are available\n",
    "available_models = {}\n",
    "available_metadata = {}\n",
    "\n",
    "for model_name, model_path in model_files.items():\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"{model_name.capitalize()} model found at {model_path}\")\n",
    "        available_models[model_name] = model_path\n",
    "    else:\n",
    "        print(f\"{model_name.capitalize()} model not found at {model_path}\")\n",
    "\n",
    "for model_name, metadata_path in metadata_files.items():\n",
    "    if os.path.exists(metadata_path):\n",
    "        print(f\"{model_name.capitalize()} metadata found at {metadata_path}\")\n",
    "        available_metadata[model_name] = metadata_path\n",
    "    else:\n",
    "        print(f\"{model_name.capitalize()} metadata not found at {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load models and metadata\n",
    "models = {}\n",
    "metadata = {}\n",
    "\n",
    "for model_name, model_path in available_models.items():\n",
    "    try:\n",
    "        if model_name == 'random_forest':\n",
    "            # Load scikit-learn model\n",
    "            models[model_name] = joblib.load(model_path)\n",
    "        else:\n",
    "            # Load Keras model\n",
    "            models[model_name] = load_model(model_path)\n",
    "        print(f\"Loaded {model_name} model successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_name} model: {e}\")\n",
    "\n",
    "for model_name, metadata_path in available_metadata.items():\n",
    "    try:\n",
    "        metadata[model_name] = joblib.load(metadata_path)\n",
    "        print(f\"Loaded {model_name} metadata successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_name} metadata: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Predictions from Individual Models\n",
    "\n",
    "Let's generate predictions from each model to use in our ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert data to float32 for neural network models\n",
    "X_test_tabular_array = X_test_tabular.values.astype('float32')\n",
    "X_test_sequence_array = X_test_sequence.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate predictions from each model\n",
    "predictions = {}\n",
    "anomaly_scores = {}\n",
    "binary_predictions = {}\n",
    "\n",
    "# LSTM predictions\n",
    "if 'lstm' in models and 'lstm' in metadata:\n",
    "    print(\"Generating LSTM predictions...\")\n",
    "    lstm_model = models['lstm']\n",
    "    lstm_threshold = metadata['lstm']['threshold']\n",
    "    \n",
    "    # Predict on test data\n",
    "    X_pred_lstm = lstm_model.predict(X_test_sequence_array)\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    lstm_scores = np.mean(np.square(X_test_sequence_array - X_pred_lstm), axis=(1, 2))\n",
    "    \n",
    "    # Binary predictions\n",
    "    lstm_binary = (lstm_scores > lstm_threshold).astype(int)\n",
    "    \n",
    "    # Store predictions\n",
    "    anomaly_scores['lstm'] = lstm_scores\n",
    "    binary_predictions['lstm'] = lstm_binary\n",
    "    \n",
    "    print(f\"LSTM anomaly scores - Min: {np.min(lstm_scores):.4f}, Max: {np.max(lstm_scores):.4f}, Mean: {np.mean(lstm_scores):.4f}\")\n",
    "    print(f\"LSTM anomalies detected: {np.sum(lstm_binary)} ({np.mean(lstm_binary)*100:.2f}%)\")\n",
    "\n",
    "# Random Forest predictions\n",
    "if 'random_forest' in models and 'random_forest' in metadata:\n",
    "    print(\"\\nGenerating Random Forest predictions...\")\n",
    "    rf_model = models['random_forest']\n",
    "    rf_threshold = metadata['random_forest']['threshold']\n",
    "    \n",
    "    # Get selected features\n",
    "    selected_features = metadata['random_forest']['selected_features']\n",
    "    \n",
    "    # Select features\n",
    "    X_test_rf = X_test_tabular[selected_features]\n",
    "    \n",
    "    # Predict probabilities\n",
    "    y_pred_proba = rf_model.predict_proba(X_test_rf)[:, 1]  # Probability of normal class\n",
    "    rf_scores = 1 - y_pred_proba  # Convert to anomaly score (higher = more anomalous)\n",
    "    \n",
    "    # Binary predictions\n",
    "    rf_binary = (rf_scores > rf_threshold).astype(int)\n",
    "    \n",
    "    # Store predictions\n",
    "    anomaly_scores['random_forest'] = rf_scores\n",
    "    binary_predictions['random_forest'] = rf_binary\n",
    "    \n",
    "    print(f\"Random Forest anomaly scores - Min: {np.min(rf_scores):.4f}, Max: {np.max(rf_scores):.4f}, Mean: {np.mean(rf_scores):.4f}\")\n",
    "    print(f\"Random Forest anomalies detected: {np.sum(rf_binary)} ({np.mean(rf_binary)*100:.2f}%)\")\n",
    "\n",
    "# Autoencoder predictions\n",
    "if 'autoencoder' in models and 'autoencoder' in metadata:\n",
    "    print(\"\\nGenerating Autoencoder predictions...\")\n",
    "    ae_model = models['autoencoder']\n",
    "    ae_threshold = metadata['autoencoder']['threshold']\n",
    "    \n",
    "    # Predict on test data\n",
    "    X_pred_ae = ae_model.predict(X_test_tabular_array)\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    ae_scores = np.mean(np.square(X_test_tabular_array - X_pred_ae), axis=1)\n",
    "    \n",
    "    # Binary predictions\n",
    "    ae_binary = (ae_scores > ae_threshold).astype(int)\n",
    "    \n",
    "    # Store predictions\n",
    "    anomaly_scores['autoencoder'] = ae_scores\n",
    "    binary_predictions['autoencoder'] = ae_binary\n",
    "    \n",
    "    print(f\"Autoencoder anomaly scores - Min: {np.min(ae_scores):.4f}, Max: {np.max(ae_scores):.4f}, Mean: {np.mean(ae_scores):.4f}\")\n",
    "    print(f\"Autoencoder anomalies detected: {np.sum(ae_binary)} ({np.mean(ae_binary)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement Ensemble Methods\n",
    "\n",
    "Now let's implement different ensemble methods to combine the predictions from individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def evaluate_binary_predictions(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate binary predictions.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels\n",
    "        y_pred: Predicted labels\n",
    "        model_name: Name of the model\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\n{model_name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{model_name} Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Majority Voting Ensemble\n",
    "\n",
    "The simplest ensemble method is majority voting, where the final prediction is the one that receives the most votes from individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Implement majority voting ensemble\n",
    "if len(binary_predictions) >= 2:\n",
    "    print(\"Implementing majority voting ensemble...\")\n",
    "    \n",
    "    # Stack binary predictions\n",
    "    stacked_predictions = np.column_stack([binary_predictions[model] for model in binary_predictions])\n",
    "    \n",
    "    # Majority voting (sum > half of models)\n",
    "    majority_vote = np.sum(stacked_predictions, axis=1) >= (len(binary_predictions) / 2)\n",
    "    majority_vote = majority_vote.astype(int)\n",
    "    \n",
    "    # Evaluate majority voting\n",
    "    majority_metrics = evaluate_binary_predictions(y_test_tabular, majority_vote, \"Majority Voting Ensemble\")\n",
    "else:\n",
    "    print(\"Need at least 2 models for majority voting ensemble.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Weighted Voting Ensemble\n",
    "\n",
    "In weighted voting, we assign different weights to each model based on their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Implement weighted voting ensemble\n",
    "if len(binary_predictions) >= 2 and len(anomaly_scores) >= 2:\n",
    "    print(\"Implementing weighted voting ensemble...\")\n",
    "    \n",
    "    # Normalize anomaly scores to [0, 1] range\n",
    "    normalized_scores = {}\n",
    "    for model in anomaly_scores:\n",
    "        scores = anomaly_scores[model]\n",
    "        min_score = np.min(scores)\n",
    "        max_score = np.max(scores)\n",
    "        normalized_scores[model] = (scores - min_score) / (max_score - min_score)\n",
    "    \n",
    "    # Assign weights based on F1 score on a validation set\n",
    "    # For simplicity, we'll use predefined weights here\n",
    "    # In practice, these weights should be determined using a validation set\n",
    "    weights = {\n",
    "        'lstm': 0.4,\n",
    "        'random_forest': 0.3,\n",
    "        'autoencoder': 0.3\n",
    "    }\n",
    "    \n",
    "    # Calculate weighted average of normalized scores\n",
    "    weighted_scores = np.zeros(len(y_test_tabular))\n",
    "    for model in normalized_scores:\n",
    "        if model in weights:\n",
    "            weighted_scores += weights[model] * normalized_scores[model]\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    fpr, tpr, thresholds = roc_curve(y_test_tabular, weighted_scores)\n",
    "    gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "    ix = np.argmax(gmeans)\n",
    "    optimal_threshold = thresholds[ix]\n",
    "    \n",
    "    print(f\"Optimal threshold for weighted ensemble: {optimal_threshold:.6f}\")\n",
    "    \n",
    "    # Make binary predictions\n",
    "    weighted_vote = (weighted_scores > optimal_threshold).astype(int)\n",
    "    \n",
    "    # Evaluate weighted voting\n",
    "    weighted_metrics = evaluate_binary_predictions(y_test_tabular, weighted_vote, \"Weighted Voting Ensemble\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.scatter(fpr[ix], tpr[ix], marker='o', color='red', label=f'Optimal (Threshold = {optimal_threshold:.6f})')\n",
    "    plt.title('Weighted Ensemble ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate AUC\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "else:\n",
    "    print(\"Need at least 2 models for weighted voting ensemble.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Stacking Ensemble\n",
    "\n",
    "In stacking, we train a meta-model that takes the predictions of the base models as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Implement stacking ensemble\n",
    "if len(normalized_scores) >= 2:\n",
    "    print(\"Implementing stacking ensemble...\")\n",
    "    \n",
    "    # Create a DataFrame with normalized scores as features\n",
    "    meta_features = pd.DataFrame({\n",
    "        model: normalized_scores[model] for model in normalized_scores\n",
    "    })\n",
    "    \n",
    "    # Split data for training and testing the meta-model\n",
    "    X_meta_train, X_meta_test, y_meta_train, y_meta_test = train_test_split(\n",
    "        meta_features, y_test_tabular, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train a meta-model (Random Forest)\n",
    "    meta_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    meta_model.fit(X_meta_train, y_meta_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    meta_pred = meta_model.predict(X_meta_test)\n",
    "    \n",
    "    # Evaluate stacking ensemble\n",
    "    stacking_metrics = evaluate_binary_predictions(y_meta_test, meta_pred, \"Stacking Ensemble\")\n",
    "    \n",
    "    # Feature importance of the meta-model\n",
    "    feature_importances = meta_model.feature_importances_\n",
    "    \n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(meta_features.columns, feature_importances)\n",
    "    plt.title('Meta-model Feature Importances')\n",
    "    plt.xlabel('Base Model')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nMeta-model Feature Importances:\")\n",
    "    for model, importance in zip(meta_features.columns, feature_importances):\n",
    "        print(f\"{model}: {importance:.4f}\")\n",
    "else:\n",
    "    print(\"Need at least 2 models for stacking ensemble.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Model Performance\n",
    "\n",
    "Let's compare the performance of individual models and ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate individual models\n",
    "individual_metrics = {}\n",
    "\n",
    "for model in binary_predictions:\n",
    "    individual_metrics[model] = evaluate_binary_predictions(y_test_tabular, binary_predictions[model], f\"Individual {model.capitalize()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a comparison table\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "comparison_data = []\n",
    "\n",
    "# Add individual models\n",
    "for model in individual_metrics:\n",
    "    row = {'Model': f\"Individual {model.capitalize()}\"}\n",
    "    for metric in metrics:\n",
    "        row[metric] = individual_metrics[model][metric]\n",
    "    comparison_data.append(row)\n",
    "\n",
    "# Add ensemble methods\n",
    "if 'majority_metrics' in locals():\n",
    "    row = {'Model': 'Majority Voting Ensemble'}\n",
    "    for metric in metrics:\n",
    "        row[metric] = majority_metrics[metric]\n",
    "    comparison_data.append(row)\n",
    "\n",
    "if 'weighted_metrics' in locals():\n",
    "    row = {'Model': 'Weighted Voting Ensemble'}\n",
    "    for metric in metrics:\n",
    "        row[metric] = weighted_metrics[metric]\n",
    "    comparison_data.append(row)\n",
    "\n",
    "if 'stacking_metrics' in locals():\n",
    "    row = {'Model': 'Stacking Ensemble'}\n",
    "    for metric in metrics:\n",
    "        row[metric] = stacking_metrics[metric]\n",
    "    comparison_data.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Display table\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot comparison of key metrics\n",
    "metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "# Prepare data for plotting\n",
    "plot_data = []\n",
    "for _, row in comparison_df.iterrows():\n",
    "    for metric in metrics_to_plot:\n",
    "        plot_data.append({\n",
    "            'Model': row['Model'],\n",
    "            'Metric': metric.capitalize(),\n",
    "            'Value': row[metric]\n",
    "        })\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.barplot(x='Metric', y='Value', hue='Model', data=plot_df)\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.ylim(0, 1.05)  # Metrics are between 0 and 1\n",
    "plt.grid(axis='y')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Best Ensemble Model\n",
    "\n",
    "Let's save the best ensemble model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Determine the best ensemble method based on F1 score\n",
    "ensemble_methods = []\n",
    "if 'majority_metrics' in locals():\n",
    "    ensemble_methods.append(('majority', majority_metrics['f1']))\n",
    "if 'weighted_metrics' in locals():\n",
    "    ensemble_methods.append(('weighted', weighted_metrics['f1']))\n",
    "if 'stacking_metrics' in locals():\n",
    "    ensemble_methods.append(('stacking', stacking_metrics['f1']))\n",
    "\n",
    "if ensemble_methods:\n",
    "    best_ensemble, best_f1 = max(ensemble_methods, key=lambda x: x[1])\n",
    "    print(f\"Best ensemble method: {best_ensemble} (F1 Score: {best_f1:.4f})\")\n",
    "    \n",
    "    # Save best ensemble metadata\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    \n",
    "    if best_ensemble == 'majority':\n",
    "        ensemble_metadata = {\n",
    "            'ensemble_type': 'majority',\n",
    "            'models': list(binary_predictions.keys()),\n",
    "            'metrics': majority_metrics\n",
    "        }\n",
    "    elif best_ensemble == 'weighted':\n",
    "        ensemble_metadata = {\n",
    "            'ensemble_type': 'weighted',\n",
    "            'models': list(normalized_scores.keys()),\n",
    "            'weights': weights,\n",
    "            'threshold': optimal_threshold,\n",
    "            'metrics': weighted_metrics\n",
    "        }\n",
    "    elif best_ensemble == 'stacking':\n",
    "        # Save meta-model\n",
    "        joblib.dump(meta_model, './models/ensemble_meta_model_hai_20_07.joblib')\n",
    "        \n",
    "        ensemble_metadata = {\n",
    "            'ensemble_type': 'stacking',\n",
    "            'models': list(normalized_scores.keys()),\n",
    "            'meta_model_path': './models/ensemble_meta_model_hai_20_07.joblib',\n",
    "            'feature_importances': {model: importance for model, importance in zip(meta_features.columns, feature_importances)},\n",
    "            'metrics': stacking_metrics\n",
    "        }\n",
    "    \n",
    "    joblib.dump(ensemble_metadata, './models/ensemble_metadata_hai_20_07.joblib')\n",
    "    print(\"Ensemble metadata saved successfully.\")\n",
    "else:\n",
    "    print(\"No ensemble methods available to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "In this notebook, we've implemented and compared different ensemble methods for anomaly detection on the HAI security dataset. Key steps included:\n",
    "\n",
    "1. Loading and preparing data for different model types\n",
    "2. Loading trained models (LSTM, Random Forest, Autoencoder)\n",
    "3. Generating predictions from individual models\n",
    "4. Implementing ensemble methods (majority voting, weighted voting, stacking)\n",
    "5. Comparing model performance\n",
    "6. Saving the best ensemble model\n",
    "\n",
    "The ensemble approach leverages the strengths of different models to improve anomaly detection performance. By combining multiple models, we can achieve better precision, recall, and F1 scores compared to individual models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}