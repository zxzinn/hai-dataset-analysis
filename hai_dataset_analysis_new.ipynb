{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI Security Dataset Analysis\n",
    "\n",
    "This notebook analyzes the HIL-based Augmented ICS (HAI) Security Dataset, which contains data collected from a realistic industrial control system (ICS) testbed augmented with a hardware-in-the-loop (HIL) simulator that emulates steam-turbine power generation and pumped-storage hydropower generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview\n",
    "\n",
    "The HAI security dataset includes multiple versions (HAI-20.07, HAI-21.03, HAI-22.04, HAI-23.05, HAIEnd-23.05), each containing normal and abnormal behaviors for ICS anomaly detection research. Let's first explore the available datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# List all available dataset versions\n",
    "dataset_versions = glob('hai-security-dataset/hai-*')\n",
    "dataset_versions.extend(glob('hai-security-dataset/haiend-*'))\n",
    "dataset_versions = [os.path.basename(version) for version in dataset_versions]\n",
    "print(f\"Available dataset versions: {dataset_versions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring HAI-22.04 Dataset\n",
    "\n",
    "Let's start by exploring the HAI-22.04 dataset, which includes training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# List all files in HAI-22.04\n",
    "hai_22_04_files = glob('hai-security-dataset/hai-22.04/*.csv')\n",
    "hai_22_04_files = [os.path.basename(file) for file in hai_22_04_files]\n",
    "print(f\"Files in HAI-22.04: {hai_22_04_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load one training dataset to explore its structure\n",
    "train1_path = 'hai-security-dataset/hai-22.04/train1.csv'\n",
    "train1_df = pd.read_csv(train1_path)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Shape of train1.csv: {train1_df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "train1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check column names\n",
    "print(f\"Number of columns: {len(train1_df.columns)}\")\n",
    "print(\"\\nColumn names:\")\n",
    "train1_df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check data types and missing values\n",
    "train1_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert timestamp to datetime\n",
    "if 'timestamp' in train1_df.columns:\n",
    "    train1_df['timestamp'] = pd.to_datetime(train1_df['timestamp'])\n",
    "    \n",
    "# Check time range\n",
    "print(f\"Start time: {train1_df['timestamp'].min()}\")\n",
    "print(f\"End time: {train1_df['timestamp'].max()}\")\n",
    "print(f\"Duration: {train1_df['timestamp'].max() - train1_df['timestamp'].min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing Attack Labels\n",
    "\n",
    "Let's check if the dataset contains attack labels and analyze their distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if attack labels are present\n",
    "attack_columns = [col for col in train1_df.columns if 'attack' in col.lower()]\n",
    "print(f\"Attack label columns: {attack_columns}\")\n",
    "\n",
    "if attack_columns:\n",
    "    # Count attack instances\n",
    "    for col in attack_columns:\n",
    "        attack_count = train1_df[col].sum()\n",
    "        attack_percentage = (attack_count / len(train1_df)) * 100\n",
    "        print(f\"{col}: {attack_count} attacks ({attack_percentage:.2f}% of data)\")\n",
    "        \n",
    "    # Plot attack distribution over time for the first attack column\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.plot(train1_df['timestamp'], train1_df[attack_columns[0]])\n",
    "    plt.title(f'{attack_columns[0]} Distribution Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Attack (1) / Normal (0)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploring Data Points\n",
    "\n",
    "Based on the technical details, the dataset contains various data points related to different controllers (P1-PC, P1-LC, P1-FC, P1-TC, P2-SC, P3-LC). Let's explore some key data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Group data points by controller\n",
    "controller_prefixes = ['P1_', 'P2_', 'P3_', 'P4_']\n",
    "for prefix in controller_prefixes:\n",
    "    cols = [col for col in train1_df.columns if col.startswith(prefix)]\n",
    "    print(f\"\\n{prefix} data points ({len(cols)}): {cols[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select key data points for visualization\n",
    "key_points = [\n",
    "    'P1_B2016',  # Pressure demand for thermal power output control\n",
    "    'P1_PIT01',  # Heat-exchanger outlet pressure\n",
    "    'P1_B3004',  # Water level setpoint (return water tank)\n",
    "    'P1_LIT01',  # Water level of the return water tank\n",
    "    'P1_B3005',  # Discharge flowrate setpoint (return water tank)\n",
    "    'P1_FT03',   # Measured flowrate of the return water tank\n",
    "    'P1_B4022',  # Temperature demand for thermal power output control\n",
    "    'P1_TIT01'   # Heat-exchanger outlet temperature\n",
    "]\n",
    "\n",
    "# Check if these columns exist in the dataset\n",
    "existing_key_points = [col for col in key_points if col in train1_df.columns]\n",
    "print(f\"Available key points: {existing_key_points}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot time series for key data points\n",
    "if existing_key_points:\n",
    "    # Sample data to reduce plotting time (every 100th point)\n",
    "    sampled_df = train1_df.iloc[::100].copy()\n",
    "    \n",
    "    # Plot each key point\n",
    "    fig, axes = plt.subplots(len(existing_key_points), 1, figsize=(14, 4*len(existing_key_points)))\n",
    "    \n",
    "    for i, point in enumerate(existing_key_points):\n",
    "        axes[i].plot(sampled_df['timestamp'], sampled_df[point])\n",
    "        axes[i].set_title(f'{point} Time Series')\n",
    "        axes[i].set_xlabel('Time')\n",
    "        axes[i].set_ylabel('Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis\n",
    "\n",
    "Let's analyze correlations between different data points to understand their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Select a subset of columns for correlation analysis\n",
    "if existing_key_points:\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = train1_df[existing_key_points].corr()\n",
    "    \n",
    "    # Plot correlation heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.title('Correlation Matrix of Key Data Points')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Multiple Datasets\n",
    "\n",
    "Let's compare key statistics across different training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# List all training datasets\n",
    "train_files = [f for f in hai_22_04_files if f.startswith('train')]\n",
    "print(f\"Training datasets: {train_files}\")\n",
    "\n",
    "# Function to get basic statistics for a dataset\n",
    "def get_dataset_stats(file_name):\n",
    "    file_path = f'hai-security-dataset/hai-22.04/{file_name}'\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Convert timestamp if it exists\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Get attack information if available\n",
    "    attack_cols = [col for col in df.columns if 'attack' in col.lower()]\n",
    "    attack_info = {}\n",
    "    for col in attack_cols:\n",
    "        attack_info[col] = df[col].sum()\n",
    "    \n",
    "    return {\n",
    "        'file_name': file_name,\n",
    "        'rows': len(df),\n",
    "        'columns': len(df.columns),\n",
    "        'start_time': df['timestamp'].min() if 'timestamp' in df.columns else None,\n",
    "        'end_time': df['timestamp'].max() if 'timestamp' in df.columns else None,\n",
    "        'duration': df['timestamp'].max() - df['timestamp'].min() if 'timestamp' in df.columns else None,\n",
    "        'attack_info': attack_info\n",
    "    }\n",
    "\n",
    "# Get statistics for all training datasets\n",
    "train_stats = []\n",
    "for file in train_files:\n",
    "    try:\n",
    "        stats = get_dataset_stats(file)\n",
    "        train_stats.append(stats)\n",
    "        print(f\"Processed {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Display statistics in a table\n",
    "stats_df = pd.DataFrame(train_stats)\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyzing Test Datasets\n",
    "\n",
    "Let's also analyze the test datasets to understand their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# List all test datasets\n",
    "test_files = [f for f in hai_22_04_files if f.startswith('test')]\n",
    "print(f\"Test datasets: {test_files}\")\n",
    "\n",
    "# Get statistics for all test datasets\n",
    "test_stats = []\n",
    "for file in test_files:\n",
    "    try:\n",
    "        stats = get_dataset_stats(file)\n",
    "        test_stats.append(stats)\n",
    "        print(f\"Processed {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Display statistics in a table\n",
    "test_stats_df = pd.DataFrame(test_stats)\n",
    "test_stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualizing Attack Scenarios\n",
    "\n",
    "Let's visualize some attack scenarios in the test datasets to understand their patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load a test dataset with attacks\n",
    "test_file = test_files[0]  # Using the first test file\n",
    "test_path = f'hai-security-dataset/hai-22.04/{test_file}'\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "if 'timestamp' in test_df.columns:\n",
    "    test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n",
    "\n",
    "# Check for attack columns\n",
    "attack_cols = [col for col in test_df.columns if 'attack' in col.lower()]\n",
    "print(f\"Attack columns in {test_file}: {attack_cols}\")\n",
    "\n",
    "if attack_cols:\n",
    "    # Create a combined attack column if multiple attack columns exist\n",
    "    if len(attack_cols) > 1:\n",
    "        test_df['any_attack'] = test_df[attack_cols].max(axis=1)\n",
    "        attack_col = 'any_attack'\n",
    "    else:\n",
    "        attack_col = attack_cols[0]\n",
    "    \n",
    "    # Find attack periods\n",
    "    attack_starts = []\n",
    "    attack_ends = []\n",
    "    in_attack = False\n",
    "    \n",
    "    for i, row in test_df.iterrows():\n",
    "        if row[attack_col] == 1 and not in_attack:\n",
    "            attack_starts.append(i)\n",
    "            in_attack = True\n",
    "        elif row[attack_col] == 0 and in_attack:\n",
    "            attack_ends.append(i-1)\n",
    "            in_attack = False\n",
    "    \n",
    "    if in_attack:  # If dataset ends during an attack\n",
    "        attack_ends.append(len(test_df)-1)\n",
    "    \n",
    "    print(f\"Found {len(attack_starts)} attack periods\")\n",
    "    \n",
    "    # Visualize the first few attack periods with key data points\n",
    "    num_attacks_to_show = min(3, len(attack_starts))\n",
    "    \n",
    "    for i in range(num_attacks_to_show):\n",
    "        start_idx = max(0, attack_starts[i] - 100)  # Include some pre-attack data\n",
    "        end_idx = min(len(test_df)-1, attack_ends[i] + 100)  # Include some post-attack data\n",
    "        \n",
    "        attack_df = test_df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        # Plot key data points during this attack\n",
    "        if existing_key_points:\n",
    "            fig, axes = plt.subplots(len(existing_key_points)+1, 1, figsize=(14, 3*(len(existing_key_points)+1)))\n",
    "            \n",
    "            # Plot attack label\n",
    "            axes[0].plot(attack_df['timestamp'], attack_df[attack_col], 'r-')\n",
    "            axes[0].set_title(f'Attack Period {i+1}')\n",
    "            axes[0].set_ylabel('Attack')\n",
    "            \n",
    "            # Plot each key point\n",
    "            for j, point in enumerate(existing_key_points):\n",
    "                if point in attack_df.columns:\n",
    "                    axes[j+1].plot(attack_df['timestamp'], attack_df[point])\n",
    "                    axes[j+1].set_title(f'{point} During Attack Period {i+1}')\n",
    "                    axes[j+1].set_ylabel('Value')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparing Different HAI Dataset Versions\n",
    "\n",
    "Let's compare the structure and characteristics of different HAI dataset versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to get basic information about a dataset version\n",
    "def get_version_info(version):\n",
    "    version_path = f'hai-security-dataset/{version}'\n",
    "    files = glob(f'{version_path}/*.csv')\n",
    "    file_names = [os.path.basename(file) for file in files]\n",
    "    \n",
    "    train_files = [f for f in file_names if f.startswith('train') or f.startswith('hai-train') or f.startswith('end-train')]\n",
    "    test_files = [f for f in file_names if f.startswith('test') or f.startswith('hai-test') or f.startswith('end-test')]\n",
    "    \n",
    "    # Sample one file to get column count\n",
    "    sample_file = files[0] if files else None\n",
    "    num_columns = 0\n",
    "    if sample_file:\n",
    "        try:\n",
    "            df = pd.read_csv(sample_file)\n",
    "            num_columns = len(df.columns)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {sample_file}: {e}\")\n",
    "    \n",
    "    return {\n",
    "        'version': version,\n",
    "        'total_files': len(files),\n",
    "        'train_files': len(train_files),\n",
    "        'test_files': len(test_files),\n",
    "        'train_file_names': train_files,\n",
    "        'test_file_names': test_files,\n",
    "        'num_columns': num_columns\n",
    "    }\n",
    "\n",
    "# Get information for all dataset versions\n",
    "version_info = []\n",
    "for version in dataset_versions:\n",
    "    try:\n",
    "        info = get_version_info(version)\n",
    "        version_info.append(info)\n",
    "        print(f\"Processed {version}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {version}: {e}\")\n",
    "\n",
    "# Display information in a table\n",
    "version_info_df = pd.DataFrame(version_info)\n",
    "version_info_df[['version', 'total_files', 'train_files', 'test_files', 'num_columns']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Statistical Analysis of Data Points\n",
    "\n",
    "Let's perform statistical analysis on key data points to understand their distributions and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load a training dataset\n",
    "train_file = train_files[0]  # Using the first training file\n",
    "train_path = f'hai-security-dataset/hai-22.04/{train_file}'\n",
    "train_df = pd.read_csv(train_path)\n",
    "\n",
    "# Select numerical columns (excluding timestamp and attack labels)\n",
    "numerical_cols = train_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "numerical_cols = [col for col in numerical_cols if not ('attack' in col.lower() or 'timestamp' in col.lower())]\n",
    "\n",
    "# Calculate basic statistics\n",
    "stats = train_df[numerical_cols].describe()\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize distributions of key data points\n",
    "if existing_key_points:\n",
    "    # Create histograms for each key point\n",
    "    fig, axes = plt.subplots(len(existing_key_points), 1, figsize=(14, 4*len(existing_key_points)))\n",
    "    \n",
    "    for i, point in enumerate(existing_key_points):\n",
    "        if point in train_df.columns:\n",
    "            sns.histplot(train_df[point], kde=True, ax=axes[i])\n",
    "            axes[i].set_title(f'Distribution of {point}')\n",
    "            axes[i].set_xlabel('Value')\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Analyzing Control Loops\n",
    "\n",
    "Based on the technical details, the dataset includes data from various control loops (P1-PC, P1-LC, P1-FC, P1-TC, P2-SC, P3-LC). Let's analyze the relationships between setpoints (SP), process variables (PV), and control variables (CV) in these control loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define control loops with their setpoints, process variables, and control variables\n",
    "control_loops = {\n",
    "    'P1-PC': {\n",
    "        'SP': 'P1_B2016',  # Pressure demand\n",
    "        'PV': 'P1_PIT01',  # Heat-exchanger outlet pressure\n",
    "        'CV': ['P1_PCV01D', 'P1_PCV02D']  # Position command for pressure control valves\n",
    "    },\n",
    "    'P1-LC': {\n",
    "        'SP': 'P1_B3004',  # Water level setpoint\n",
    "        'PV': 'P1_LIT01',  # Water level of the return water tank\n",
    "        'CV': 'P1_LCV01D'  # Position command for level control valve\n",
    "    },\n",
    "    'P1-FC': {\n",
    "        'SP': 'P1_B3005',  # Discharge flowrate setpoint\n",
    "        'PV': 'P1_FT03',   # Measured flowrate of the return water tank\n",
    "        'CV': 'P1_FCV03D'  # Position command for flow control valve\n",
    "    },\n",
    "    'P1-TC': {\n",
    "        'SP': 'P1_B4022',  # Temperature demand\n",
    "        'PV': 'P1_TIT01',  # Heat-exchanger outlet temperature\n",
    "        'CV': ['P1_FCV01D', 'P1_FCV02D']  # Position command for flow control valves\n",
    "    }\n",
    "}\n",
    "\n",
    "# Analyze each control loop\n",
    "for loop_name, loop_vars in control_loops.items():\n",
    "    print(f\"\\nAnalyzing {loop_name} control loop\")\n",
    "    \n",
    "    # Check if all variables exist in the dataset\n",
    "    sp = loop_vars['SP']\n",
    "    pv = loop_vars['PV']\n",
    "    cv = loop_vars['CV'] if isinstance(loop_vars['CV'], list) else [loop_vars['CV']]\n",
    "    \n",
    "    missing_vars = [var for var in [sp, pv] + cv if var not in train_df.columns]\n",
    "    if missing_vars:\n",
    "        print(f\"  Missing variables: {missing_vars}\")\n",
    "        continue\n",
    "    \n",
    "    # Plot setpoint and process variable\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Sample data to reduce plotting time (every 100th point)\n",
    "    sampled_df = train_df.iloc[::100].copy()\n",
    "    \n",
    "    plt.plot(sampled_df['timestamp'], sampled_df[sp], 'r-', label=f'Setpoint ({sp})')\n",
    "    plt.plot(sampled_df['timestamp'], sampled_df[pv], 'b-', label=f'Process Variable ({pv})')\n",
    "    \n",
    "    plt.title(f'{loop_name} Control Loop: Setpoint vs. Process Variable')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot control variables\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    for control_var in cv:\n",
    "        plt.plot(sampled_df['timestamp'], sampled_df[control_var], label=f'Control Variable ({control_var})')\n",
    "    \n",
    "    plt.title(f'{loop_name} Control Loop: Control Variables')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value (%)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate error (difference between setpoint and process variable)\n",
    "    train_df[f'{loop_name}_error'] = train_df[sp] - train_df[pv]\n",
    "    \n",
    "    # Plot error histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(train_df[f'{loop_name}_error'], kde=True)\n",
    "    plt.title(f'{loop_name} Control Loop: Error Distribution')\n",
    "    plt.xlabel('Error (SP - PV)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Analyzing Attack Scenarios\n",
    "\n",
    "Let's analyze the attack scenarios in the test datasets to understand their impact on different control loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load a test dataset\n",
    "test_file = test_files[0]  # Using the first test file\n",
    "test_path = f'hai-security-dataset/hai-22.04/{test_file}'\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "if 'timestamp' in test_df.columns:\n",
    "    test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n",
    "\n",
    "# Check for attack columns\n",
    "attack_cols = [col for col in test_df.columns if 'attack' in col.lower()]\n",
    "print(f\"Attack columns in {test_file}: {attack_cols}\")\n",
    "\n",
    "if attack_cols:\n",
    "    # Analyze attacks for each control loop\n",
    "    for loop_name, loop_vars in control_loops.items():\n",
    "        # Check if all variables exist in the dataset\n",
    "        sp = loop_vars['SP']\n",
    "        pv = loop_vars['PV']\n",
    "        cv = loop_vars['CV'] if isinstance(loop_vars['CV'], list) else [loop_vars['CV']]\n",
    "        \n",
    "        missing_vars = [var for var in [sp, pv] + cv if var not in test_df.columns]\n",
    "        if missing_vars:\n",
    "            print(f\"  Missing variables for {loop_name}: {missing_vars}\")\n",
    "            continue\n",
    "        \n",
    "        # Find attack periods specific to this control loop if possible\n",
    "        loop_attack_col = next((col for col in attack_cols if loop_name.lower() in col.lower()), attack_cols[0])\n",
    "        \n",
    "        # Find attack periods\n",
    "        attack_starts = []\n",
    "        attack_ends = []\n",
    "        in_attack = False\n",
    "        \n",
    "        for i, row in test_df.iterrows():\n",
    "            if row[loop_attack_col] == 1 and not in_attack:\n",
    "                attack_starts.append(i)\n",
    "                in_attack = True\n",
    "            elif row[loop_attack_col] == 0 and in_attack:\n",
    "                attack_ends.append(i-1)\n",
    "                in_attack = False\n",
    "        \n",
    "        if in_attack:  # If dataset ends during an attack\n",
    "            attack_ends.append(len(test_df)-1)\n",
    "        \n",
    "        print(f\"\\nFound {len(attack_starts)} attack periods for {loop_name}\")\n",
    "        \n",
    "        if not attack_starts:\n",
    "            continue\n",
    "        \n",
    "        # Analyze the first attack period\n",
    "        start_idx = max(0, attack_starts[0] - 100)  # Include some pre-attack data\n",
    "        end_idx = min(len(test_df)-1, attack_ends[0] + 100)  # Include some post-attack data\n",
    "        \n",
    "        attack_df = test_df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        # Plot setpoint, process variable, and control variables during the attack\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
    "        \n",
    "        # Plot attack label\n",
    "        axes[0].plot(attack_df['timestamp'], attack_df[loop_attack_col], 'r-')\n",
    "        axes[0].set_title(f'{loop_name} Attack Period')\n",
    "        axes[0].set_ylabel('Attack')\n",
    "        \n",
    "        # Plot setpoint and process variable\n",
    "        axes[1].plot(attack_df['timestamp'], attack_df[sp], 'r-', label=f'Setpoint ({sp})')\n",
    "        axes[1].plot(attack_df['timestamp'], attack_df[pv], 'b-', label=f'Process Variable ({pv})')\n",
    "        axes[1].set_title(f'{loop_name} Control Loop: Setpoint vs. Process Variable During Attack')\n",
    "        axes[1].set_ylabel('Value')\n",
    "        axes[1].legend()\n",
    "        \n",
    "        # Plot control variables\n",
    "        for control_var in cv:\n",
    "            axes[2].plot(attack_df['timestamp'], attack_df[control_var], label=f'Control Variable ({control_var})')\n",
    "        \n",
    "        axes[2].set_title(f'{loop_name} Control Loop: Control Variables During Attack')\n",
    "        axes[2].set_xlabel('Time')\n",
    "        axes[2].set_ylabel('Value (%)')\n",
    "        axes[2].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Conclusion\n",
    "\n",
    "In this notebook, we've analyzed the HAI security dataset, focusing on:\n",
    "\n",
    "1. Dataset structure and characteristics\n",
    "2. Key data points and their relationships\n",
    "3. Control loops and their behavior\n",
    "4. Attack scenarios and their impact\n",
    "\n",
    "The HAI dataset provides valuable insights into industrial control system behavior under normal and attack conditions, making it useful for developing and testing anomaly detection algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}