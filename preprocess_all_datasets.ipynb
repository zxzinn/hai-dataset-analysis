{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI Security Dataset: Preprocessing All Versions\n",
    "\n",
    "This notebook preprocesses all versions of the HIL-based Augmented ICS (HAI) Security Dataset. It handles the different formats and structures of each version (HAI-20.07, HAI-21.03, HAI-22.04, HAI-23.05, HAIEnd-23.05) and creates standardized processed data files for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import datetime\n",
    "import dask.dataframe as dd  # For parallel computing with DataFrames\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import joblib  # For saving preprocessing objects\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm  # For progress tracking\n",
    "\n",
    "# Try to import GPU libraries, but continue if not available\n",
    "try:\n",
    "    import cudf  # GPU-accelerated DataFrame library\n",
    "    import cupy as cp  # GPU-accelerated NumPy-like library\n",
    "    GPU_AVAILABLE = True\n",
    "    print(\"GPU libraries loaded successfully. GPU acceleration is available.\")\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"GPU libraries not available. Falling back to CPU processing.\")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Overview\n",
    "\n",
    "First, let's explore the available dataset versions and their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define the base path to the HAI security dataset\n",
    "base_dataset_path = '../hai-security-dataset/'\n",
    "\n",
    "# List all available dataset versions\n",
    "dataset_versions = [os.path.basename(version) for version in glob.glob(f'{base_dataset_path}hai-*')]\n",
    "dataset_versions.extend([os.path.basename(version) for version in glob.glob(f'{base_dataset_path}haiend-*')])\n",
    "\n",
    "# Filter out any non-directory items (like zip files)\n",
    "dataset_versions = [version for version in dataset_versions if os.path.isdir(os.path.join(base_dataset_path, version))]\n",
    "\n",
    "print(f\"Available dataset versions: {dataset_versions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a function to get dataset information\n",
    "def get_dataset_info(version):\n",
    "    \"\"\"\n",
    "    Get information about a specific dataset version.\n",
    "    \n",
    "    Args:\n",
    "        version: Dataset version (e.g., 'hai-20.07')\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dataset information\n",
    "    \"\"\"\n",
    "    version_path = os.path.join(base_dataset_path, version)\n",
    "    \n",
    "    # Get all CSV files\n",
    "    all_files = glob.glob(f'{version_path}/*.csv')\n",
    "    \n",
    "    # Identify train and test files\n",
    "    train_files = [f for f in all_files if 'train' in os.path.basename(f).lower()]\n",
    "    test_files = [f for f in all_files if 'test' in os.path.basename(f).lower()]\n",
    "    label_files = [f for f in all_files if 'label' in os.path.basename(f).lower()]\n",
    "    \n",
    "    # Determine separator (HAI-20.07 uses semicolon, others use comma)\n",
    "    separator = ';' if version == 'hai-20.07' else ','\n",
    "    \n",
    "    # Determine timestamp column name\n",
    "    timestamp_col = 'time' if version in ['hai-20.07', 'hai-21.03'] else 'timestamp'\n",
    "    \n",
    "    # Try to get column count from first file\n",
    "    column_count = None\n",
    "    sample_columns = None\n",
    "    if all_files:\n",
    "        try:\n",
    "            sample_df = pd.read_csv(all_files[0], sep=separator, nrows=1)\n",
    "            column_count = len(sample_df.columns)\n",
    "            sample_columns = sample_df.columns.tolist()\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading sample from {version}: {e}\")\n",
    "    \n",
    "    return {\n",
    "        'version': version,\n",
    "        'path': version_path,\n",
    "        'train_files': train_files,\n",
    "        'test_files': test_files,\n",
    "        'label_files': label_files,\n",
    "        'total_files': len(all_files),\n",
    "        'separator': separator,\n",
    "        'timestamp_col': timestamp_col,\n",
    "        'column_count': column_count,\n",
    "        'sample_columns': sample_columns\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get information for all dataset versions\n",
    "dataset_info = {}\n",
    "for version in dataset_versions:\n",
    "    dataset_info[version] = get_dataset_info(version)\n",
    "    \n",
    "# Display summary information\n",
    "summary_data = []\n",
    "for version, info in dataset_info.items():\n",
    "    summary_data.append({\n",
    "        'Version': version,\n",
    "        'Train Files': len(info['train_files']),\n",
    "        'Test Files': len(info['test_files']),\n",
    "        'Label Files': len(info['label_files']),\n",
    "        'Total Files': info['total_files'],\n",
    "        'Separator': info['separator'],\n",
    "        'Timestamp Column': info['timestamp_col'],\n",
    "        'Column Count': info['column_count']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Pipeline\n",
    "\n",
    "Now let's create a comprehensive preprocessing pipeline that can handle all HAI dataset versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class HAIDataPreprocessor:\n",
    "    \"\"\"\n",
    "    Preprocessing pipeline for HAI security dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_version, scaler_type='standard', base_path='../hai-security-dataset/'):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor.\n",
    "        \n",
    "        Args:\n",
    "            dataset_version: Dataset version (e.g., 'hai-20.07')\n",
    "            scaler_type: Type of scaler ('standard' or 'minmax')\n",
    "            base_path: Base path to the HAI security dataset\n",
    "        \"\"\"\n",
    "        self.dataset_version = dataset_version\n",
    "        self.dataset_path = os.path.join(base_path, dataset_version)\n",
    "        self.scaler_type = scaler_type\n",
    "        self.scaler = None\n",
    "        \n",
    "        # Set version-specific parameters\n",
    "        self.timestamp_col = 'time' if dataset_version in ['hai-20.07', 'hai-21.03'] else 'timestamp'\n",
    "        self.separator = ';' if dataset_version == 'hai-20.07' else ','\n",
    "        \n",
    "        # Initialize columns\n",
    "        self.feature_columns = None\n",
    "        self.attack_columns = None\n",
    "        self.label_columns = None\n",
    "        \n",
    "    def get_file_list(self):\n",
    "        \"\"\"\n",
    "        Get list of training, test, and label files.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (train_files, test_files, label_files)\n",
    "        \"\"\"\n",
    "        all_files = glob.glob(f'{self.dataset_path}/*.csv')\n",
    "        train_files = [f for f in all_files if 'train' in os.path.basename(f).lower() and 'label' not in os.path.basename(f).lower()]\n",
    "        test_files = [f for f in all_files if 'test' in os.path.basename(f).lower() and 'label' not in os.path.basename(f).lower()]\n",
    "        label_files = [f for f in all_files if 'label' in os.path.basename(f).lower()]\n",
    "        \n",
    "        return train_files, test_files, label_files\n",
    "    \n",
    "    def load_file(self, file_path, sample_rows=None):\n",
    "        \"\"\"\n",
    "        Load a CSV file efficiently.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the CSV file\n",
    "            sample_rows: Number of rows to sample (None for all rows)\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame: Loaded data\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # First, use pandas to read a few rows to get column names and dtypes\n",
    "            sample_df = pd.read_csv(file_path, sep=self.separator, nrows=5)\n",
    "            \n",
    "            # Create a dictionary of dtypes to optimize memory usage\n",
    "            dtypes = {}\n",
    "            for col in sample_df.columns:\n",
    "                if col == self.timestamp_col:\n",
    "                    dtypes[col] = 'object'  # Keep timestamp as object initially\n",
    "                elif 'attack' in col.lower() or 'label' in col.lower():\n",
    "                    dtypes[col] = 'int8'  # Binary labels can be stored as int8\n",
    "                elif sample_df[col].dtype == 'float64':\n",
    "                    dtypes[col] = 'float64'  # Keep as float64 to avoid conversion issues\n",
    "                elif sample_df[col].dtype == 'int64':\n",
    "                    dtypes[col] = 'int32'  # Reduce precision to save memory\n",
    "            \n",
    "            # Use Dask to read the file in parallel chunks\n",
    "            if sample_rows:\n",
    "                # For sampling, use pandas directly\n",
    "                df = pd.read_csv(file_path, sep=self.separator, dtype=dtypes, nrows=sample_rows)\n",
    "            else:\n",
    "                # For full file, use Dask\n",
    "                dask_df = dd.read_csv(file_path, sep=self.separator, dtype=dtypes, blocksize=\"64MB\")\n",
    "                df = dask_df.compute()\n",
    "        except ValueError as e:\n",
    "            # If there's a dtype conversion error, try again without specifying dtypes\n",
    "            print(f\"Warning: Error with dtype conversion: {e}\")\n",
    "            print(\"Attempting to load without dtype specifications...\")\n",
    "            \n",
    "            if sample_rows:\n",
    "                df = pd.read_csv(file_path, sep=self.separator, nrows=sample_rows)\n",
    "            else:\n",
    "                dask_df = dd.read_csv(file_path, sep=self.separator, blocksize=\"64MB\")\n",
    "                df = dask_df.compute()\n",
    "        \n",
    "        # Convert timestamp\n",
    "        if self.timestamp_col in df.columns:\n",
    "            df[self.timestamp_col] = pd.to_datetime(df[self.timestamp_col])\n",
    "        \n",
    "        # Identify feature and attack columns if not already set\n",
    "        if self.feature_columns is None:\n",
    "            # Identify attack/label columns\n",
    "            self.attack_columns = [col for col in df.columns if 'attack' in col.lower()]\n",
    "            self.label_columns = [col for col in df.columns if 'label' in col.lower()]\n",
    "            \n",
    "            # All columns that are not timestamp, attack, or label are features\n",
    "            exclude_cols = [self.timestamp_col] + self.attack_columns + self.label_columns\n",
    "            self.feature_columns = [col for col in df.columns if col not in exclude_cols]\n",
    "        \n",
    "        print(f\"Data loaded in {time.time() - start_time:.2f} seconds\")\n",
    "        return df\n",
    "    \n",
    "    def fit_scaler(self, train_files, sample_size=10000):\n",
    "        \"\"\"\n",
    "        Fit a scaler on training data.\n",
    "        \n",
    "        Args:\n",
    "            train_files: List of training file paths\n",
    "            sample_size: Number of rows to sample from each file for fitting\n",
    "            \n",
    "        Returns:\n",
    "            self: The fitted preprocessor\n",
    "        \"\"\"\n",
    "        print(f\"Fitting {self.scaler_type} scaler on training data...\")\n",
    "        \n",
    "        # Initialize the appropriate scaler\n",
    "        if self.scaler_type == 'standard':\n",
    "            self.scaler = StandardScaler()\n",
    "        elif self.scaler_type == 'minmax':\n",
    "            self.scaler = MinMaxScaler()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scaler type: {self.scaler_type}\")\n",
    "        \n",
    "        # Sample data from each training file\n",
    "        all_samples = []\n",
    "        for file in train_files:\n",
    "            df = self.load_file(file, sample_rows=sample_size)\n",
    "            all_samples.append(df[self.feature_columns])\n",
    "        \n",
    "        # Concatenate samples and fit the scaler\n",
    "        combined_samples = pd.concat(all_samples)\n",
    "        self.scaler.fit(combined_samples)\n",
    "        \n",
    "        print(f\"Scaler fitted on {len(combined_samples)} samples\")\n",
    "        return self\n",
    "    \n",
    "    def transform_data(self, df):\n",
    "        \"\"\"\n",
    "        Apply preprocessing transformations to a DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (X, y, timestamps) - Features, labels, and timestamps\n",
    "        \"\"\"\n",
    "        # Extract features, labels, and timestamps\n",
    "        X = df[self.feature_columns].copy()\n",
    "        timestamps = df[self.timestamp_col].copy() if self.timestamp_col in df.columns else None\n",
    "        \n",
    "        # Extract labels if available\n",
    "        y = None\n",
    "        label_cols = self.attack_columns + self.label_columns\n",
    "        if label_cols and any(col in df.columns for col in label_cols):\n",
    "            # Use the first available label column\n",
    "            for col in label_cols:\n",
    "                if col in df.columns:\n",
    "                    y = df[col].copy()\n",
    "                    break\n",
    "        \n",
    "        # Apply scaling if scaler is fitted\n",
    "        if self.scaler is not None:\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "            X = pd.DataFrame(X_scaled, columns=X.columns, index=X.index)\n",
    "        \n",
    "        return X, y, timestamps\n",
    "    \n",
    "    def save_preprocessor(self, output_dir='./models'):\n",
    "        \"\"\"\n",
    "        Save the preprocessor to disk.\n",
    "        \n",
    "        Args:\n",
    "            output_dir: Directory to save the preprocessor\n",
    "            \n",
    "        Returns:\n",
    "            str: Path to the saved preprocessor\n",
    "        \"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_path = f\"{output_dir}/hai_{self.dataset_version.replace('-', '_')}_{self.scaler_type}_preprocessor.joblib\"\n",
    "        \n",
    "        # Create a dictionary with all necessary attributes\n",
    "        preprocessor_dict = {\n",
    "            'scaler': self.scaler,\n",
    "            'feature_columns': self.feature_columns,\n",
    "            'attack_columns': self.attack_columns,\n",
    "            'label_columns': self.label_columns,\n",
    "            'timestamp_col': self.timestamp_col,\n",
    "            'separator': self.separator,\n",
    "            'dataset_version': self.dataset_version,\n",
    "            'scaler_type': self.scaler_type\n",
    "        }\n",
    "        \n",
    "        # Save to disk\n",
    "        joblib.dump(preprocessor_dict, output_path)\n",
    "        print(f\"Preprocessor saved to {output_path}\")\n",
    "        \n",
    "        return output_path\n",
    "    \n",
    "    @classmethod\n",
    "    def load_preprocessor(cls, input_path, base_path='../hai-security-dataset/'):\n",
    "        \"\"\"\n",
    "        Load a preprocessor from disk.\n",
    "        \n",
    "        Args:\n",
    "            input_path: Path to the saved preprocessor\n",
    "            base_path: Base path to the HAI security dataset\n",
    "            \n",
    "        Returns:\n",
    "            HAIDataPreprocessor: Loaded preprocessor\n",
    "        \"\"\"\n",
    "        # Load from disk\n",
    "        preprocessor_dict = joblib.load(input_path)\n",
    "        \n",
    "        # Create a new instance\n",
    "        preprocessor = cls(dataset_version=preprocessor_dict['dataset_version'],\n",
    "                          scaler_type=preprocessor_dict['scaler_type'],\n",
    "                          base_path=base_path)\n",
    "        \n",
    "        # Restore attributes\n",
    "        preprocessor.scaler = preprocessor_dict['scaler']\n",
    "        preprocessor.feature_columns = preprocessor_dict['feature_columns']\n",
    "        preprocessor.attack_columns = preprocessor_dict['attack_columns']\n",
    "        preprocessor.label_columns = preprocessor_dict.get('label_columns', [])\n",
    "        preprocessor.timestamp_col = preprocessor_dict['timestamp_col']\n",
    "        preprocessor.separator = preprocessor_dict['separator']\n",
    "        \n",
    "        print(f\"Preprocessor loaded from {input_path}\")\n",
    "        return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def process_and_save_data(preprocessor, file_paths, output_dir='./processed_data', chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Process and save data in chunks.\n",
    "    \n",
    "    Args:\n",
    "        preprocessor: HAIDataPreprocessor instance\n",
    "        file_paths: List of file paths to process\n",
    "        output_dir: Directory to save processed data\n",
    "        chunk_size: Size of each chunk\n",
    "        \n",
    "    Returns:\n",
    "        list: Paths to saved files\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    saved_files = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        file_name = os.path.basename(file_path)\n",
    "        print(f\"Processing {file_name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Process in chunks\n",
    "            chunk_reader = pd.read_csv(file_path, sep=preprocessor.separator, chunksize=chunk_size)\n",
    "            \n",
    "            for i, chunk in enumerate(chunk_reader):\n",
    "                # Convert timestamp\n",
    "                if preprocessor.timestamp_col in chunk.columns:\n",
    "                    chunk[preprocessor.timestamp_col] = pd.to_datetime(chunk[preprocessor.timestamp_col])\n",
    "                \n",
    "                # Transform data\n",
    "                X, y, timestamps = preprocessor.transform_data(chunk)\n",
    "                \n",
    "                # Create output DataFrame\n",
    "                output_df = X.copy()\n",
    "                if timestamps is not None:\n",
    "                    output_df[preprocessor.timestamp_col] = timestamps\n",
    "                if y is not None:\n",
    "                    output_df['attack'] = y\n",
    "                \n",
    "                # Save chunk\n",
    "                output_path = f\"{output_dir}/{file_name.replace('.csv', f'_chunk{i}.npz')}\"\n",
    "                np.savez_compressed(output_path, \n",
    "                                   data=output_df.to_numpy(), \n",
    "                                   columns=output_df.columns.tolist())\n",
    "                \n",
    "                saved_files.append(output_path)\n",
    "                print(f\"  Saved chunk {i} to {output_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "    \n",
    "    return saved_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_dataset_version(version, scaler_type='standard', base_path='../hai-security-dataset/'):\n",
    "    \"\"\"\n",
    "    Preprocess a specific HAI dataset version.\n",
    "    \n",
    "    Args:\n",
    "        version: Dataset version (e.g., 'hai-21.03')\n",
    "        scaler_type: Type of scaler ('standard' or 'minmax')\n",
    "        base_path: Base path to the HAI security dataset\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (preprocessor, train_saved_files, test_saved_files)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Preprocessing {version} dataset...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Initialize preprocessor\n",
    "    preprocessor = HAIDataPreprocessor(dataset_version=version, scaler_type=scaler_type, base_path=base_path)\n",
    "    \n",
    "    # Get training, test, and label files\n",
    "    train_files, test_files, label_files = preprocessor.get_file_list()\n",
    "    print(f\"Training files: {[os.path.basename(f) for f in train_files]}\")\n",
    "    print(f\"Test files: {[os.path.basename(f) for f in test_files]}\")\n",
    "    print(f\"Label files: {[os.path.basename(f) for f in label_files]}\")\n",
    "    \n",
    "    # Fit scaler on training data\n",
    "    preprocessor.fit_scaler(train_files)\n",
    "    \n",
    "    # Save preprocessor\n",
    "    preprocessor.save_preprocessor()\n",
    "    \n",
    "    # Process and save training data\n",
    "    train_output_dir = f'./processed_data/{version}/train'\n",
    "    train_saved_files = process_and_save_data(preprocessor, train_files, output_dir=train_output_dir)\n",
    "    \n",
    "    # Process and save test data\n",
    "    test_output_dir = f'./processed_data/{version}/test'\n",
    "    test_saved_files = process_and_save_data(preprocessor, test_files, output_dir=test_output_dir)\n",
    "    \n",
    "    # Process and save label data if available\n",
    "    label_saved_files = []\n",
    "    if label_files:\n",
    "        label_output_dir = f'./processed_data/{version}/label'\n",
    "        label_saved_files = process_and_save_data(preprocessor, label_files, output_dir=label_output_dir)\n",
    "    \n",
    "    return preprocessor, train_saved_files, test_saved_files, label_saved_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess All Dataset Versions\n",
    "\n",
    "Now let's preprocess all available HAI dataset versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a function to load and verify processed data\n",
    "def load_processed_data(file_path):\n",
    "    \"\"\"\n",
    "    Load processed data from NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the NPZ file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Loaded data\n",
    "    \"\"\"\n",
    "    # Load NPZ file\n",
    "    npz_data = np.load(file_path, allow_pickle=True)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(npz_data['data'], columns=npz_data['columns'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preprocess all dataset versions\n",
    "results = {}\n",
    "\n",
    "for version in tqdm(dataset_versions, desc=\"Processing dataset versions\"):\n",
    "    try:\n",
    "        preprocessor, train_files, test_files, label_files = preprocess_dataset_version(version)\n",
    "        results[version] = {\n",
    "            'preprocessor': preprocessor,\n",
    "            'train_files': train_files,\n",
    "            'test_files': test_files,\n",
    "            'label_files': label_files,\n",
    "            'status': 'success'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {version}: {e}\")\n",
    "        results[version] = {\n",
    "            'status': 'error',\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Processed Data\n",
    "\n",
    "Let's verify the processed data for each dataset version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verify processed data for each version\n",
    "for version, result in results.items():\n",
    "    if result['status'] == 'success' and result['train_files']:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Verifying processed data for {version}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Load a sample file\n",
    "        sample_file = result['train_files'][0]\n",
    "        print(f\"Loading {os.path.basename(sample_file)}...\")\n",
    "        \n",
    "        try:\n",
    "            loaded_df = load_processed_data(sample_file)\n",
    "            print(f\"Loaded data shape: {loaded_df.shape}\")\n",
    "            print(\"\\nFirst 5 rows:\")\n",
    "            display(loaded_df.head())\n",
    "            \n",
    "            # Check for attack column\n",
    "            attack_cols = [col for col in loaded_df.columns if 'attack' in col.lower()]\n",
    "            if attack_cols:\n",
    "                print(f\"\\nAttack columns: {attack_cols}\")\n",
    "                for col in attack_cols:\n",
    "                    attack_count = loaded_df[col].sum()\n",
    "                    attack_percentage = (attack_count / len(loaded_df)) * 100\n",
    "                    print(f\"{col}: {attack_count} attacks ({attack_percentage:.2f}% of data)\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error verifying {version}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Data Distribution\n",
    "\n",
    "Let's visualize the data distribution for each dataset version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize data distribution for each version\n",
    "for version, result in results.items():\n",
    "    if result['status'] == 'success' and result['test_files']:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Visualizing data for {version}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Load a test file\n",
    "        sample_file = result['test_files'][0]\n",
    "        print(f\"Loading {os.path.basename(sample_file)}...\")\n",
    "        \n",
    "        try:\n",
    "            loaded_df = load_processed_data(sample_file)\n",
    "            \n",
    "            # Check for attack column\n",
    "            attack_cols = [col for col in loaded_df.columns if 'attack' in col.lower()]\n",
    "            timestamp_col = [col for col in loaded_df.columns if col in ['time', 'timestamp']]\n",
    "            \n",
    "            if attack_cols and timestamp_col:\n",
    "                # Plot attack distribution over time\n",
    "                plt.figure(figsize=(14, 6))\n",
    "                plt.plot(loaded_df[timestamp_col[0]], loaded_df[attack_cols[0]])\n",
    "                plt.title(f'{version}: {attack_cols[0]} Distribution Over Time')\n",
    "                plt.xlabel('Time')\n",
    "                plt.ylabel('Attack (1) / Normal (0)')\n",
    "                plt.show()\n",
    "                \n",
    "                # Select a few features to visualize\n",
    "                feature_cols = [col for col in loaded_df.columns \n",
    "                               if col not in attack_cols and col not in timestamp_col]\n",
    "                selected_features = feature_cols[:5]  # First 5 features\n",
    "                \n",
    "                # Plot features during attack periods\n",
    "                if loaded_df[attack_cols[0]].sum() > 0:\n",
    "                    # Find attack periods\n",
    "                    attack_starts = []\n",
    "                    attack_ends = []\n",
    "                    in_attack = False\n",
    "                    \n",
    "                    for i, val in enumerate(loaded_df[attack_cols[0]]):\n",
    "                        if val == 1 and not in_attack:\n",
    "                            attack_starts.append(i)\n",
    "                            in_attack = True\n",
    "                        elif val == 0 and in_attack:\n",
    "                            attack_ends.append(i-1)\n",
    "                            in_attack = False\n",
    "                    \n",
    "                    if in_attack:  # If dataset ends during an attack\n",
    "                        attack_ends.append(len(loaded_df)-1)\n",
    "                    \n",
    "                    print(f\"Found {len(attack_starts)} attack periods\")\n",
    "                    \n",
    "                    # Visualize the first attack period\n",
    "                    if attack_starts:\n",
    "                        start_idx = max(0, attack_starts[0] - 100)  # Include some pre-attack data\n",
    "                        end_idx = min(len(loaded_df)-1, attack_ends[0] + 100)  # Include some post-attack data\n",
    "                        \n",
    "                        attack_df = loaded_df.iloc[start_idx:end_idx].copy()\n",
    "                        \n",
    "                        # Plot features during attack\n",
    "                        fig, axes = plt.subplots(len(selected_features)+1, 1, figsize=(14, 4*(len(selected_features)+1)))\n",
    "                        \n",
    "                        # Plot attack label\n",
    "                        axes[0].plot(attack_df[timestamp_col[0]], attack_df[attack_cols[0]], 'r-')\n",
    "                        axes[0].set_title(f'{version}: Attack Period')\n",
    "                        axes[0].set_ylabel('Attack')\n",
    "                        \n",
    "                        # Plot each feature\n",
    "                        for i, feature in enumerate(selected_features):\n",
    "                            axes[i+1].plot(attack_df[timestamp_col[0]], attack_df[feature])\n",
    "                            axes[i+1].set_title(f'{feature} During Attack')\n",
    "                            axes[i+1].set_ylabel('Value')\n",
    "                        \n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error visualizing {version}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary of Preprocessing Results\n",
    "\n",
    "Let's summarize the preprocessing results for all dataset versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create a summary of preprocessing results\n",
    "summary_data = []\n",
    "\n",
    "for version, result in results.items():\n",
    "    if result['status'] == 'success':\n",
    "        summary_data.append({\n",
    "            'Version': version,\n",
    "            'Preprocessor': 'Success',\n",
    "            'Train Files': len(result['train_files']),\n",
    "            'Test Files': len(result['test_files']),\n",
    "            'Label Files': len(result['label_files']),\n",
    "            'Status': 'Success'\n",
    "        })\n",
    "    else:\n",
    "        summary_data.append({\n",
    "            'Version': version,\n",
    "            'Preprocessor': 'Failed',\n",
    "            'Train Files': 0,\n",
    "            'Test Files': 0,\n",
    "            'Label Files': 0,\n",
    "            'Status': f\"Error: {result['error']}\"\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we've implemented a comprehensive preprocessing pipeline for all versions of the HAI security dataset. The pipeline includes:\n",
    "\n",
    "1. Efficient data loading using Dask for parallel processing\n",
    "2. Handling different file formats and structures across dataset versions\n",
    "3. Feature scaling and normalization\n",
    "4. Processing and saving data in a standardized format for model training\n",
    "\n",
    "The preprocessed data and fitted preprocessors are saved and ready to be used for training anomaly detection models in separate notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}