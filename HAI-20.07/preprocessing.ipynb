{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI-20.07 Dataset Preprocessing\n",
    "\n",
    "This notebook performs preprocessing on the HAI-20.07 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.data_loader import HAIDataLoader\n",
    "from utils.feature_engineering import FeatureEngineer\n",
    "from utils.visualization import Visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize data loader\n",
    "data_loader = HAIDataLoader(base_path='../hai-security-dataset')\n",
    "\n",
    "# Load training and testing data\n",
    "train_df1 = data_loader.load_dataset('20.07', 'train', 1)\n",
    "train_df2 = data_loader.load_dataset('20.07', 'train', 2)\n",
    "test_df1 = data_loader.load_dataset('20.07', 'test', 1)\n",
    "test_df2 = data_loader.load_dataset('20.07', 'test', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize visualizer\n",
    "visualizer = Visualizer(save_dir='figures')\n",
    "\n",
    "# Plot time series data for key features\n",
    "key_features = ['P1_PIT01', 'P1_LIT01', 'P1_FT01', 'P2_SIT01']\n",
    "fig = visualizer.plot_time_series(train_df1, key_features)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize feature engineer\n",
    "feature_engineer = FeatureEngineer()\n",
    "\n",
    "# Extract time features\n",
    "train_df1 = feature_engineer.extract_time_features(train_df1)\n",
    "train_df2 = feature_engineer.extract_time_features(train_df2)\n",
    "test_df1 = feature_engineer.extract_time_features(test_df1)\n",
    "test_df2 = feature_engineer.extract_time_features(test_df2)\n",
    "\n",
    "# Extract statistical features\n",
    "window_size = 10\n",
    "train_df1 = feature_engineer.extract_statistical_features(train_df1, window_size)\n",
    "train_df2 = feature_engineer.extract_statistical_features(train_df2, window_size)\n",
    "test_df1 = feature_engineer.extract_statistical_features(test_df1, window_size)\n",
    "test_df2 = feature_engineer.extract_statistical_features(test_df2, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot correlation heatmap\n",
    "corr_matrix = feature_engineer.calculate_feature_correlations(train_df1)\n",
    "fig = visualizer.plot_correlation_heatmap(corr_matrix.to_numpy(), train_df1.columns)\n",
    "fig.show()\n",
    "\n",
    "# Plot attack distribution\n",
    "attack_cols = ['attack', 'attack_P1', 'attack_P2', 'attack_P3']\n",
    "fig = visualizer.plot_attack_distribution(test_df1, attack_cols)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Fit scaler on training data\n",
    "exclude_cols = ['time'] + attack_cols\n",
    "feature_engineer.fit_standard_scaler(train_df1, exclude_cols)\n",
    "\n",
    "# Transform all datasets\n",
    "train_df1_scaled = feature_engineer.transform_with_scaler(train_df1)\n",
    "train_df2_scaled = feature_engineer.transform_with_scaler(train_df2)\n",
    "test_df1_scaled = feature_engineer.transform_with_scaler(test_df1)\n",
    "test_df2_scaled = feature_engineer.transform_with_scaler(test_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create processed data directory\n",
    "processed_dir = Path('processed_data')\n",
    "processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save processed datasets\n",
    "train_df1_scaled.collect().write_parquet(processed_dir / 'train1.parquet')\n",
    "train_df2_scaled.collect().write_parquet(processed_dir / 'train2.parquet')\n",
    "test_df1_scaled.collect().write_parquet(processed_dir / 'test1.parquet')\n",
    "test_df2_scaled.collect().write_parquet(processed_dir / 'test2.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
