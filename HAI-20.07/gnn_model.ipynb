{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI-20.07 Graph Neural Network Model\n",
    "\n",
    "Anomaly detection using Graph Neural Networks with NetworkX graph data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
    "import networkx as nx\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from utils.model_utils import ModelManager\n",
    "from utils.evaluation import Evaluator\n",
    "from utils.visualization import Visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load preprocessed data\n",
    "processed_dir = Path('processed_data')\n",
    "\n",
    "train_df1 = pl.read_parquet(processed_dir / 'train1.parquet')\n",
    "train_df2 = pl.read_parquet(processed_dir / 'train2.parquet')\n",
    "test_df1 = pl.read_parquet(processed_dir / 'test1.parquet')\n",
    "test_df2 = pl.read_parquet(processed_dir / 'test2.parquet')\n",
    "\n",
    "# Load NetworkX graph data\n",
    "graph_dir = Path('../hai-security-dataset/graph/boiler')\n",
    "with open(graph_dir / 'phy_boiler.json', 'r') as f:\n",
    "    physical_graph_data = json.load(f)\n",
    "    \n",
    "with open(graph_dir / 'dcs_1001h.json', 'r') as f:\n",
    "    dcs_graph_data = json.load(f)\n",
    "    \n",
    "# Create NetworkX graphs\n",
    "physical_graph = nx.node_link_graph(physical_graph_data)\n",
    "dcs_graph = nx.node_link_graph(dcs_graph_data)\n",
    "\n",
    "# Combine graphs\n",
    "combined_graph = nx.compose(physical_graph, dcs_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Graph Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def extract_graph_features(G):\n",
    "    \"\"\"Extract node features from graph\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Node degree\n",
    "    degrees = dict(G.degree())\n",
    "    \n",
    "    # Betweenness centrality\n",
    "    betweenness = nx.betweenness_centrality(G)\n",
    "    \n",
    "    # Closeness centrality\n",
    "    closeness = nx.closeness_centrality(G)\n",
    "    \n",
    "    # Eigenvector centrality\n",
    "    eigenvector = nx.eigenvector_centrality_numpy(G)\n",
    "    \n",
    "    # Combine features\n",
    "    for node in G.nodes():\n",
    "        features[node] = np.array([\n",
    "            degrees[node],\n",
    "            betweenness[node],\n",
    "            closeness[node],\n",
    "            eigenvector[node]\n",
    "        ])\n",
    "        \n",
    "    return features\n",
    "\n",
    "# Extract graph features\n",
    "node_features = extract_graph_features(combined_graph)\n",
    "\n",
    "# Create PyTorch Geometric data\n",
    "def create_pyg_data(G, node_features):\n",
    "    # Node features\n",
    "    x = torch.FloatTensor([node_features[node] for node in G.nodes()])\n",
    "    \n",
    "    # Edge index\n",
    "    edge_index = torch.LongTensor([[i, j] for i, j in G.edges()]).t()\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "pyg_data = create_pyg_data(combined_graph, node_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GNNModel, self).__init__()\n",
    "        \n",
    "        # Graph convolutional layers\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # Output layer\n",
    "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # Graph convolutions\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "model = GNNModel(\n",
    "    in_channels=4,  # Number of node features\n",
    "    hidden_channels=64,\n",
    "    out_channels=2  # Binary classification\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training parameters\n",
    "n_epochs = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "pyg_data = pyg_data.to(device)\n",
    "\n",
    "# Training history\n",
    "history = {'train_loss': []}\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(pyg_data.x, pyg_data.edge_index)\n",
    "    loss = criterion(out, pyg_data.y)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Record loss\n",
    "    history['train_loss'].append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Attack Detection and Propagation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_attack_propagation(model, data, G):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get node embeddings\n",
    "        node_embeddings = model.conv3(\n",
    "            model.conv2(\n",
    "                model.conv1(data.x, data.edge_index),\n",
    "                data.edge_index\n",
    "            ),\n",
    "            data.edge_index\n",
    "        ).cpu().numpy()\n",
    "        \n",
    "        # Get predictions\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred = out.argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "    # Find attack paths\n",
    "    attack_nodes = [node for i, node in enumerate(G.nodes()) if pred[i] == 1]\n",
    "    attack_subgraph = G.subgraph(attack_nodes)\n",
    "    \n",
    "    # Find connected components in attack subgraph\n",
    "    attack_paths = list(nx.connected_components(attack_subgraph))\n",
    "    \n",
    "    return node_embeddings, pred, attack_paths\n",
    "\n",
    "# Analyze attack propagation\n",
    "node_embeddings, predictions, attack_paths = analyze_attack_propagation(\n",
    "    model, pyg_data, combined_graph\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize visualizer\n",
    "visualizer = Visualizer(save_dir='figures')\n",
    "\n",
    "# Plot training history\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=history['train_loss'], name='Training Loss'))\n",
    "fig.update_layout(title='Training History', xaxis_title='Epoch', yaxis_title='Loss')\n",
    "fig.show()\n",
    "\n",
    "# Plot attack propagation paths\n",
    "for i, path in enumerate(attack_paths):\n",
    "    fig = visualizer.plot_attack_propagation(\n",
    "        combined_graph,\n",
    "        list(path),\n",
    "        title=f'Attack Propagation Path {i+1}'\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize model manager\n",
    "model_manager = ModelManager(base_dir='models')\n",
    "\n",
    "# Prepare metadata\n",
    "metadata = {\n",
    "    'model_type': 'gnn',\n",
    "    'dataset_version': '20.07',\n",
    "    'parameters': {\n",
    "        'in_channels': 4,\n",
    "        'hidden_channels': 64,\n",
    "        'out_channels': 2,\n",
    "        'n_epochs': n_epochs\n",
    "    },\n",
    "    'graph_info': {\n",
    "        'n_nodes': combined_graph.number_of_nodes(),\n",
    "        'n_edges': combined_graph.number_of_edges(),\n",
    "        'n_attack_paths': len(attack_paths)\n",
    "    },\n",
    "    'training_history': history\n",
    "}\n",
    "\n",
    "# Save model\n",
    "model_manager.save_torch_model(\n",
    "    model=model,\n",
    "    model_name='gnn',\n",
    "    version='v1',\n",
    "    metadata=metadata\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
