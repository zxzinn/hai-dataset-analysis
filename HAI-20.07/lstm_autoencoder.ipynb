{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI-20.07 LSTM Autoencoder Model\n",
    "\n",
    "Anomaly detection using LSTM Autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from utils.model_utils import ModelManager\n",
    "from utils.evaluation import Evaluator\n",
    "from utils.visualization import Visualizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load preprocessed data\n",
    "processed_dir = Path('processed_data')\n",
    "\n",
    "train_df1 = pl.read_parquet(processed_dir / 'train1.parquet')\n",
    "train_df2 = pl.read_parquet(processed_dir / 'train2.parquet')\n",
    "test_df1 = pl.read_parquet(processed_dir / 'test1.parquet')\n",
    "test_df2 = pl.read_parquet(processed_dir / 'test2.parquet')\n",
    "\n",
    "# Combine training data\n",
    "train_data = pl.concat([train_df1, train_df2])\n",
    "\n",
    "# Separate features and labels\n",
    "feature_cols = [col for col in train_data.columns if not col.startswith('attack')]\n",
    "X_train = train_data.select(feature_cols).to_numpy()\n",
    "X_test1 = test_df1.select(feature_cols).to_numpy()\n",
    "X_test2 = test_df2.select(feature_cols).to_numpy()\n",
    "\n",
    "y_test1 = test_df1.select('attack').to_numpy().ravel()\n",
    "y_test2 = test_df2.select('attack').to_numpy().ravel()\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        sequences.append(data[i:i+seq_length])\n",
    "    return np.array(sequences)\n",
    "\n",
    "# Parameters\n",
    "sequence_length = 60  # 1 minute of data (assuming 1 Hz sampling)\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "# Create sequences\n",
    "X_train_seq = create_sequences(X_train, sequence_length)\n",
    "X_test1_seq = create_sequences(X_test1, sequence_length)\n",
    "X_test2_seq = create_sequences(X_test2, sequence_length)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_seq)\n",
    "X_test1_tensor = torch.FloatTensor(X_test1_seq)\n",
    "X_test2_tensor = torch.FloatTensor(X_test2_seq)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, X_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test1_dataset = TensorDataset(X_test1_tensor, X_test1_tensor)\n",
    "test1_loader = DataLoader(test1_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test2_dataset = TensorDataset(X_test2_tensor, X_test2_tensor)\n",
    "test2_loader = DataLoader(test2_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=input_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        encoded, _ = self.encoder(x)\n",
    "        \n",
    "        # Decode\n",
    "        decoded, _ = self.decoder(encoded)\n",
    "        \n",
    "        return decoded\n",
    "\n",
    "# Initialize model\n",
    "model = LSTMAutoencoder(\n",
    "    input_size=n_features,\n",
    "    hidden_size=64,\n",
    "    num_layers=2\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Training parameters\n",
    "n_epochs = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Training history\n",
    "history = {'train_loss': []}\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_x, _ in train_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_x)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Average loss for the epoch\n",
    "    train_loss /= len(train_loader)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{n_epochs}], Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def get_reconstruction_errors(model, dataloader, device):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, _ in dataloader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            output = model(batch_x)\n",
    "            \n",
    "            # Calculate reconstruction error for each sequence\n",
    "            errors = torch.mean(torch.pow(batch_x - output, 2), dim=(1,2))\n",
    "            reconstruction_errors.extend(errors.cpu().numpy())\n",
    "    \n",
    "    return np.array(reconstruction_errors)\n",
    "\n",
    "# Get reconstruction errors\n",
    "train_errors = get_reconstruction_errors(model, train_loader, device)\n",
    "test1_errors = get_reconstruction_errors(model, test1_loader, device)\n",
    "test2_errors = get_reconstruction_errors(model, test2_loader, device)\n",
    "\n",
    "# Calculate threshold (e.g., 95th percentile of training errors)\n",
    "threshold = np.percentile(train_errors, 95)\n",
    "\n",
    "# Get predictions\n",
    "y_pred1 = (test1_errors > threshold).astype(int)\n",
    "y_pred2 = (test2_errors > threshold).astype(int)\n",
    "\n",
    "# Adjust labels to match predictions length\n",
    "y_test1_adj = y_test1[sequence_length-1:]\n",
    "y_test2_adj = y_test2[sequence_length-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize evaluator\n",
    "evaluator = Evaluator()\n",
    "\n",
    "# Calculate metrics for test set 1\n",
    "metrics1 = evaluator.calculate_basic_metrics(y_test1_adj, y_pred1)\n",
    "etapr1 = evaluator.calculate_etapr(y_test1_adj, y_pred1)\n",
    "delay1 = evaluator.calculate_detection_delay(\n",
    "    y_test1_adj,\n",
    "    y_pred1,\n",
    "    test_df1.select('time').to_numpy().ravel()[sequence_length-1:]\n",
    ")\n",
    "\n",
    "# Calculate metrics for test set 2\n",
    "metrics2 = evaluator.calculate_basic_metrics(y_test2_adj, y_pred2)\n",
    "etapr2 = evaluator.calculate_etapr(y_test2_adj, y_pred2)\n",
    "delay2 = evaluator.calculate_detection_delay(\n",
    "    y_test2_adj,\n",
    "    y_pred2,\n",
    "    test_df2.select('time').to_numpy().ravel()[sequence_length-1:]\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Test Set 1 Results:\")\n",
    "print(f\"Basic Metrics: {metrics1}\")\n",
    "print(f\"eTaPR Metrics: {etapr1}\")\n",
    "print(f\"Detection Delay: {delay1}\")\n",
    "\n",
    "print(\"\\nTest Set 2 Results:\")\n",
    "print(f\"Basic Metrics: {metrics2}\")\n",
    "print(f\"eTaPR Metrics: {etapr2}\")\n",
    "print(f\"Detection Delay: {delay2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize visualizer\n",
    "visualizer = Visualizer(save_dir='figures')\n",
    "\n",
    "# Plot training history\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=history['train_loss'], name='Training Loss'))\n",
    "fig.update_layout(title='Training History', xaxis_title='Epoch', yaxis_title='Loss')\n",
    "fig.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm1 = evaluator.calculate_confusion_matrix(y_test1_adj, y_pred1)\n",
    "fig = visualizer.plot_confusion_matrix(cm1, ['Normal', 'Attack'], title='Test Set 1 Confusion Matrix')\n",
    "fig.show()\n",
    "\n",
    "# Plot reconstruction errors\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=test1_errors, name='Reconstruction Error'))\n",
    "fig.add_hline(y=threshold, line_dash='dash', line_color='red', name='Threshold')\n",
    "fig.update_layout(title='Reconstruction Errors (Test Set 1)', showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize model manager\n",
    "model_manager = ModelManager(base_dir='models')\n",
    "\n",
    "# Prepare metadata\n",
    "metadata = {\n",
    "    'model_type': 'lstm_autoencoder',\n",
    "    'dataset_version': '20.07',\n",
    "    'parameters': {\n",
    "        'input_size': n_features,\n",
    "        'hidden_size': 64,\n",
    "        'num_layers': 2,\n",
    "        'sequence_length': sequence_length,\n",
    "        'threshold': float(threshold)\n",
    "    },\n",
    "    'performance': {\n",
    "        'test1': {\n",
    "            'basic_metrics': metrics1,\n",
    "            'etapr_metrics': etapr1,\n",
    "            'detection_delay': delay1\n",
    "        },\n",
    "        'test2': {\n",
    "            'basic_metrics': metrics2,\n",
    "            'etapr_metrics': etapr2,\n",
    "            'detection_delay': delay2\n",
    "        }\n",
    "    },\n",
    "    'training_history': history\n",
    "}\n",
    "\n",
    "# Save model\n",
    "model_manager.save_torch_model(\n",
    "    model=model,\n",
    "    model_name='lstm_autoencoder',\n",
    "    version='v1',\n",
    "    metadata=metadata\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
