{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model for HAI Security Dataset Anomaly Detection\n",
    "\n",
    "This notebook implements a Long Short-Term Memory (LSTM) neural network for anomaly detection on the HAI security dataset. LSTM networks are well-suited for time series anomaly detection due to their ability to capture temporal dependencies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, BatchNormalization, Input, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Check for GPU availability\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(\"GPU is available for training\")\n",
    "    # Set memory growth to avoid memory allocation errors\n",
    "    for gpu in tf.config.list_physical_devices('GPU'):\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"No GPU available, using CPU for training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data\n",
    "\n",
    "First, let's load the preprocessed data created in the preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_processed_data(file_path):\n",
    "    \"\"\"\n",
    "    Load processed data from NPZ file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the NPZ file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Loaded data\n",
    "    \"\"\"\n",
    "    # Load NPZ file\n",
    "    npz_data = np.load(file_path)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(npz_data['data'], columns=npz_data['columns'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load preprocessor\n",
    "preprocessor_path = './models/hai_hai_20_07_standard_preprocessor.joblib'\n",
    "preprocessor_dict = joblib.load(preprocessor_path)\n",
    "\n",
    "# Extract important information\n",
    "feature_columns = preprocessor_dict['feature_columns']\n",
    "attack_columns = preprocessor_dict['attack_columns']\n",
    "timestamp_col = preprocessor_dict['timestamp_col']\n",
    "\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "print(f\"Attack columns: {attack_columns}\")\n",
    "print(f\"Timestamp column: {timestamp_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get list of processed data files\n",
    "train_data_dir = './processed_data/hai-20.07/train'\n",
    "test_data_dir = './processed_data/hai-20.07/test'\n",
    "\n",
    "train_files = sorted(glob.glob(f'{train_data_dir}/*.npz'))\n",
    "test_files = sorted(glob.glob(f'{test_data_dir}/*.npz'))\n",
    "\n",
    "print(f\"Training files: {[os.path.basename(f) for f in train_files]}\")\n",
    "print(f\"Test files: {[os.path.basename(f) for f in test_files]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for LSTM\n",
    "\n",
    "LSTM models require sequences of data as input. We'll create sequences of fixed length from our time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_sequences(data, feature_cols, target_col=None, seq_length=100):\n",
    "    \"\"\"\n",
    "    Create sequences for LSTM input.\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame containing the data\n",
    "        feature_cols: List of feature column names\n",
    "        target_col: Target column name (None for unsupervised learning)\n",
    "        seq_length: Length of each sequence\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y) - Sequences and targets (if target_col is provided)\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = [] if target_col is not None else None\n",
    "    \n",
    "    # Extract features\n",
    "    features = data[feature_cols].values\n",
    "    \n",
    "    # Extract target if provided\n",
    "    targets = data[target_col].values if target_col is not None else None\n",
    "    \n",
    "    # Create sequences\n",
    "    for i in range(len(features) - seq_length):\n",
    "        X.append(features[i:i+seq_length])\n",
    "        if target_col is not None:\n",
    "            # Use the label of the last timestep in the sequence\n",
    "            y.append(targets[i+seq_length])\n",
    "    \n",
    "    return np.array(X), np.array(y) if target_col is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_and_prepare_data(file_paths, feature_cols, target_col=None, seq_length=100, max_files=None):\n",
    "    \"\"\"\n",
    "    Load and prepare data from multiple files.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of file paths\n",
    "        feature_cols: List of feature column names\n",
    "        target_col: Target column name (None for unsupervised learning)\n",
    "        seq_length: Length of each sequence\n",
    "        max_files: Maximum number of files to load (None for all files)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (X, y) - Combined sequences and targets\n",
    "    \"\"\"\n",
    "    all_X = []\n",
    "    all_y = [] if target_col is not None else None\n",
    "    \n",
    "    # Limit the number of files if specified\n",
    "    if max_files is not None:\n",
    "        file_paths = file_paths[:max_files]\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        \n",
    "        # Load data\n",
    "        df = load_processed_data(file_path)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = create_sequences(df, feature_cols, target_col, seq_length)\n",
    "        \n",
    "        all_X.append(X)\n",
    "        if target_col is not None:\n",
    "            all_y.append(y)\n",
    "    \n",
    "    # Combine data from all files\n",
    "    combined_X = np.vstack(all_X) if all_X else np.array([])\n",
    "    combined_y = np.concatenate(all_y) if all_y else None\n",
    "    \n",
    "    return combined_X, combined_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set parameters\n",
    "seq_length = 100  # Sequence length (number of time steps)\n",
    "target_col = 'attack' if attack_columns else None  # Target column\n",
    "\n",
    "# Load and prepare training data\n",
    "print(\"Loading and preparing training data...\")\n",
    "X_train, _ = load_and_prepare_data(train_files, feature_columns, target_col=None, seq_length=seq_length, max_files=2)\n",
    "\n",
    "# Load and prepare test data\n",
    "print(\"\\nLoading and preparing test data...\")\n",
    "X_test, y_test = load_and_prepare_data(test_files, feature_columns, target_col=target_col, seq_length=seq_length, max_files=2)\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
    "if y_test is not None:\n",
    "    print(f\"Test data shape: {X_test.shape}, Test labels shape: {y_test.shape}\")\n",
    "else:\n",
    "    print(f\"Test data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build LSTM Autoencoder Model\n",
    "\n",
    "We'll use an LSTM autoencoder for anomaly detection. The autoencoder will learn to reconstruct normal behavior, and anomalies will have higher reconstruction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def build_lstm_autoencoder(input_shape, encoding_dim=32):\n",
    "    \"\"\"\n",
    "    Build an LSTM autoencoder model.\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Shape of input data (seq_length, num_features)\n",
    "        encoding_dim: Dimension of the encoded representation\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (model, encoder, decoder) - Full model, encoder part, and decoder part\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    encoder_inputs = Input(shape=input_shape)\n",
    "    encoder_lstm1 = LSTM(128, return_sequences=True)(encoder_inputs)\n",
    "    encoder_bn1 = BatchNormalization()(encoder_lstm1)\n",
    "    encoder_lstm2 = LSTM(64, return_sequences=False)(encoder_bn1)\n",
    "    encoder_bn2 = BatchNormalization()(encoder_lstm2)\n",
    "    encoder_output = Dense(encoding_dim)(encoder_bn2)\n",
    "    \n",
    "    # Decoder\n",
    "    decoder_inputs = Input(shape=(encoding_dim,))\n",
    "    decoder_dense1 = Dense(64)(decoder_inputs)\n",
    "    decoder_bn1 = BatchNormalization()(decoder_dense1)\n",
    "    decoder_repeat = RepeatVector(input_shape[0])(decoder_bn1)\n",
    "    decoder_lstm1 = LSTM(64, return_sequences=True)(decoder_repeat)\n",
    "    decoder_bn2 = BatchNormalization()(decoder_lstm1)\n",
    "    decoder_lstm2 = LSTM(128, return_sequences=True)(decoder_bn2)\n",
    "    decoder_bn3 = BatchNormalization()(decoder_lstm2)\n",
    "    decoder_output = TimeDistributed(Dense(input_shape[1]))(decoder_bn3)\n",
    "    \n",
    "    # Create models\n",
    "    encoder = Model(encoder_inputs, encoder_output, name='encoder')\n",
    "    decoder = Model(decoder_inputs, decoder_output, name='decoder')\n",
    "    \n",
    "    # Autoencoder (end-to-end model)\n",
    "    autoencoder_output = decoder(encoder(encoder_inputs))\n",
    "    autoencoder = Model(encoder_inputs, autoencoder_output, name='autoencoder')\n",
    "    \n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set model parameters\n",
    "encoding_dim = 32  # Dimension of the encoded representation\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # (seq_length, num_features)\n",
    "\n",
    "# Build model\n",
    "autoencoder, encoder, decoder = build_lstm_autoencoder(input_shape, encoding_dim)\n",
    "\n",
    "# Compile model\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Print model summary\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train LSTM Autoencoder Model\n",
    "\n",
    "Now we'll train the autoencoder on normal data (without attacks) to learn the normal behavior patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set training parameters\n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "validation_split = 0.1\n",
    "\n",
    "# Create model checkpoint callback\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "checkpoint_path = './models/lstm_autoencoder_hai_20_07.h5'\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min')\n",
    "\n",
    "# Create early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                              patience=10, \n",
    "                              verbose=1, \n",
    "                              mode='min', \n",
    "                              restore_best_weights=True)\n",
    "\n",
    "# Create TensorBoard callback\n",
    "log_dir = './logs/lstm_autoencoder_' + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard = TensorBoard(log_dir=log_dir, \n",
    "                         histogram_freq=1, \n",
    "                         write_graph=True, \n",
    "                         write_images=True)\n",
    "\n",
    "# Combine callbacks\n",
    "callbacks = [checkpoint, early_stopping, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Train model\n",
    "start_time = time.time()\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    X_train, X_train,  # Input and output are the same for autoencoder\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "# Plot zoomed-in view of the last epochs\n",
    "plt.subplot(1, 2, 2)\n",
    "last_epochs = min(20, len(history.history['loss']))  # Last 20 epochs or all if less\n",
    "plt.plot(history.history['loss'][-last_epochs:])\n",
    "plt.plot(history.history['val_loss'][-last_epochs:])\n",
    "plt.title(f'Model Loss (Last {last_epochs} Epochs)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model and Detect Anomalies\n",
    "\n",
    "Now we'll use the trained autoencoder to detect anomalies in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the best model\n",
    "best_model = load_model(checkpoint_path)\n",
    "\n",
    "# Predict on test data\n",
    "X_test_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate reconstruction error\n",
    "mse = np.mean(np.square(X_test - X_test_pred), axis=(1, 2))\n",
    "\n",
    "print(f\"Reconstruction error statistics:\")\n",
    "print(f\"Min: {np.min(mse):.6f}\")\n",
    "print(f\"Max: {np.max(mse):.6f}\")\n",
    "print(f\"Mean: {np.mean(mse):.6f}\")\n",
    "print(f\"Std: {np.std(mse):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot reconstruction error distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(mse, bins=50)\n",
    "plt.title('Reconstruction Error Distribution')\n",
    "plt.xlabel('Reconstruction Error (MSE)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(mse, bins=50, log=True)\n",
    "plt.title('Reconstruction Error Distribution (Log Scale)')\n",
    "plt.xlabel('Reconstruction Error (MSE)')\n",
    "plt.ylabel('Frequency (Log Scale)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Determine Anomaly Threshold\n",
    "\n",
    "We need to determine a threshold for the reconstruction error to classify data points as normal or anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def find_optimal_threshold(mse, y_true):\n",
    "    \"\"\"\n",
    "    Find the optimal threshold for anomaly detection using ROC curve.\n",
    "    \n",
    "    Args:\n",
    "        mse: Reconstruction error values\n",
    "        y_true: True labels (0 for normal, 1 for anomaly)\n",
    "        \n",
    "    Returns:\n",
    "        float: Optimal threshold value\n",
    "    \"\"\"\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, mse)\n",
    "    \n",
    "    # Calculate the geometric mean of sensitivity and specificity\n",
    "    gmeans = np.sqrt(tpr * (1 - fpr))\n",
    "    \n",
    "    # Find the optimal threshold\n",
    "    ix = np.argmax(gmeans)\n",
    "    optimal_threshold = thresholds[ix]\n",
    "    \n",
    "    print(f\"Optimal threshold: {optimal_threshold:.6f}\")\n",
    "    print(f\"At this threshold - TPR: {tpr[ix]:.4f}, FPR: {fpr[ix]:.4f}, G-mean: {gmeans[ix]:.4f}\")\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, marker='.')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "    plt.scatter(fpr[ix], tpr[ix], marker='o', color='red', label=f'Optimal (Threshold = {optimal_threshold:.6f})')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Find optimal threshold if labels are available\n",
    "if y_test is not None:\n",
    "    threshold = find_optimal_threshold(mse, y_test)\n",
    "else:\n",
    "    # If no labels, use a statistical approach\n",
    "    threshold = np.mean(mse) + 3 * np.std(mse)  # Mean + 3 standard deviations\n",
    "    print(f\"Using statistical threshold: {threshold:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Classify as anomaly if reconstruction error > threshold\n",
    "y_pred = (mse > threshold).astype(int)\n",
    "\n",
    "# Evaluate if labels are available\n",
    "if y_test is not None:\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Calculate ROC AUC\n",
    "    roc_auc = auc(*roc_curve(y_test, mse)[:2])\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualize Anomalies\n",
    "\n",
    "Let's visualize some examples of normal and anomalous sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_reconstruction_examples(X, X_pred, mse, threshold, y_true=None, num_examples=3):\n",
    "    \"\"\"\n",
    "    Plot examples of normal and anomalous sequences with their reconstructions.\n",
    "    \n",
    "    Args:\n",
    "        X: Original sequences\n",
    "        X_pred: Reconstructed sequences\n",
    "        mse: Reconstruction errors\n",
    "        threshold: Anomaly threshold\n",
    "        y_true: True labels (optional)\n",
    "        num_examples: Number of examples to plot for each category\n",
    "    \"\"\"\n",
    "    # Predicted labels\n",
    "    y_pred = (mse > threshold).astype(int)\n",
    "    \n",
    "    # Find indices of true positives, false positives, true negatives, and false negatives\n",
    "    if y_true is not None:\n",
    "        tp_indices = np.where((y_true == 1) & (y_pred == 1))[0]\n",
    "        fp_indices = np.where((y_true == 0) & (y_pred == 1))[0]\n",
    "        tn_indices = np.where((y_true == 0) & (y_pred == 0))[0]\n",
    "        fn_indices = np.where((y_true == 1) & (y_pred == 0))[0]\n",
    "        \n",
    "        categories = [\n",
    "            ('True Positive (Correctly Detected Anomaly)', tp_indices),\n",
    "            ('False Positive (False Alarm)', fp_indices),\n",
    "            ('True Negative (Correctly Identified Normal)', tn_indices),\n",
    "            ('False Negative (Missed Anomaly)', fn_indices)\n",
    "        ]\n",
    "    else:\n",
    "        # If no true labels, just show predicted anomalies and normal\n",
    "        anomaly_indices = np.where(y_pred == 1)[0]\n",
    "        normal_indices = np.where(y_pred == 0)[0]\n",
    "        \n",
    "        categories = [\n",
    "            ('Predicted Anomaly', anomaly_indices),\n",
    "            ('Predicted Normal', normal_indices)\n",
    "        ]\n",
    "    \n",
    "    # Plot examples for each category\n",
    "    for category_name, indices in categories:\n",
    "        if len(indices) == 0:\n",
    "            print(f\"No examples found for category: {category_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Select random examples\n",
    "        selected_indices = np.random.choice(indices, size=min(num_examples, len(indices)), replace=False)\n",
    "        \n",
    "        for i, idx in enumerate(selected_indices):\n",
    "            # Select a random feature to visualize\n",
    "            feature_idx = np.random.randint(0, X.shape[2])\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.subplot(2, 1, 1)\n",
    "            plt.plot(X[idx, :, feature_idx], label='Original')\n",
    "            plt.plot(X_pred[idx, :, feature_idx], label='Reconstruction')\n",
    "            plt.title(f\"{category_name} - Example {i+1} (Feature {feature_idx}) - MSE: {mse[idx]:.6f}\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.subplot(2, 1, 2)\n",
    "            plt.plot(np.abs(X[idx, :, feature_idx] - X_pred[idx, :, feature_idx]), color='red')\n",
    "            plt.title('Absolute Reconstruction Error')\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot reconstruction examples\n",
    "plot_reconstruction_examples(X_test, X_test_pred, mse, threshold, y_test, num_examples=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Analyze Anomaly Patterns\n",
    "\n",
    "Let's analyze the patterns of detected anomalies to understand their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate feature-wise reconstruction error\n",
    "feature_mse = np.mean(np.square(X_test - X_test_pred), axis=1)  # Average over time steps\n",
    "\n",
    "# Get indices of anomalies\n",
    "anomaly_indices = np.where(y_pred == 1)[0]\n",
    "\n",
    "if len(anomaly_indices) > 0:\n",
    "    # Calculate average feature-wise error for anomalies\n",
    "    anomaly_feature_mse = feature_mse[anomaly_indices].mean(axis=0)\n",
    "    \n",
    "    # Get top 10 features with highest reconstruction error\n",
    "    top_features_idx = np.argsort(anomaly_feature_mse)[-10:]\n",
    "    top_features_error = anomaly_feature_mse[top_features_idx]\n",
    "    \n",
    "    # Plot top features\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(range(len(top_features_idx)), top_features_error[::-1])\n",
    "    plt.yticks(range(len(top_features_idx)), [f\"Feature {idx}\" for idx in top_features_idx[::-1]])\n",
    "    plt.title('Top 10 Features with Highest Reconstruction Error in Anomalies')\n",
    "    plt.xlabel('Mean Squared Error')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 10 features with highest reconstruction error in anomalies:\")\n",
    "    for i, idx in enumerate(top_features_idx[::-1]):\n",
    "        print(f\"{i+1}. Feature {idx}: {anomaly_feature_mse[idx]:.6f}\")\n",
    "else:\n",
    "    print(\"No anomalies detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model and Results\n",
    "\n",
    "Finally, let's save the model and results for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save model components\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "encoder.save('./models/lstm_encoder_hai_20_07.h5')\n",
    "decoder.save('./models/lstm_decoder_hai_20_07.h5')\n",
    "\n",
    "# Save threshold and other metadata\n",
    "model_metadata = {\n",
    "    'threshold': threshold,\n",
    "    'encoding_dim': encoding_dim,\n",
    "    'seq_length': seq_length,\n",
    "    'feature_columns': feature_columns,\n",
    "    'training_history': {\n",
    "        'loss': history.history['loss'],\n",
    "        'val_loss': history.history['val_loss']\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump(model_metadata, './models/lstm_model_metadata_hai_20_07.joblib')\n",
    "print(\"Model and metadata saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we've implemented an LSTM autoencoder for anomaly detection on the HAI security dataset. The model learns to reconstruct normal behavior patterns and identifies anomalies based on reconstruction error. Key steps included:\n",
    "\n",
    "1. Loading and preparing preprocessed data\n",
    "2. Building an LSTM autoencoder model\n",
    "3. Training the model on normal data\n",
    "4. Detecting anomalies using reconstruction error\n",
    "5. Evaluating the model's performance\n",
    "6. Analyzing anomaly patterns\n",
    "\n",
    "The LSTM autoencoder provides an effective approach for detecting anomalies in industrial control system data, which can help identify potential security threats or system malfunctions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}